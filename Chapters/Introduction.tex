\chapter{Introduction}
\label{chap-intro}
The field of machine learning has witnessed significant advancements in recent years, driven by the explosive growth in data availability and computational power. In today's cutting-edge field of artificial intelligence, generative learning has become a highly popular area of research, especially with the rise of some state-of-the-art generative language AI like \href{https://openai.com/chatgpt/}{ChatGPT}. By effectively learning the data distribution in either an explicit or an implicit form, highly realistic samples can be randomly generated \cite{bernardoGenerativeDiscriminativeGetting2007}. However, despite these technological strides, certain challenges still persist, particularly when modeling unbalanced data distributions, which is a common issue in many real-world scenarios \cite{chawlaDataMiningImbalanced2005}.

Unbalanced data, where certain classes or modes are underrepresented, keeps posing a significant challenge to various machine learning or deep learning models. In the context of discriminative learning, the inequality between classes could cause classification models to poorly generalize to minority classes, leading to a poor prediction performance on those underrepresented classes \cite{chawlaDataMiningImbalanced2005,heLearningImbalancedData2009}. Currently, the problem of data imbalance in classification models has been well-studied. Some traditional and effective solutions including resampling and reweighting can be implemented during the training phase to rebalance the data.

In the realm of generative learning, data imbalance could also lead to similar detrimental effects as seen in the domain of classification, where the generative model disproportionately favors overrepresented groups while overlooking minority modes \cite{choiFairGenerativeModeling2020,gerychDebiasingPretrainedGenerative2023}. As a result, the frequency and quality of generated samples from minority modes are generally inferior to those of the majority classes. Simply speaking, unbalanced training data results in biased or unfair generation from the generative model. Unlike in the context of classification, there is a significant scarcity of research focused on addressing data imbalance in the setting of generative learning. Although much effort has been devoted to addressing the mode collapse problem in GANs pursuing a more diverse generation, most of these approaches do not consider the setting of unbalanced data \cite{goodfellowGenerativeAdversarialNets2014,kossaleModeCollapseGenerative2022}. Furthermore, difficulties in correcting data imbalance in generative learning can be concluded in two key points. First, generative models typically involve complex neural network architectures and optimization schemes, making it impractical to directly apply techniques used for addressing data imbalance in classification. Instead, specific modifications according to different models are often required. Second, generative learning is typically performed in an unsupervised manner, where label information is not provided, making it less straightforward to identify and measure data imbalance.

This thesis aims to address the challenges of biased generation or mode missing brought from unbalanced training data within the framework of Gen-RKM, a relatively novel generative model proposed by Pandey et al \cite{pandeyGenerativeRestrictedKernel2021}. New synergies between kernel principal component analysis and deep neural network architectures have been made in Gen-RKM, thereby unlocking potential new research directions in the field of kernel-based learning. However, the capability of Gen-RKM to handle unbalanced data remains unexplored. Specifically, our work centers on two key research questions:
\begin{itemize}[label={--}]
    \item How does the generation of Gen-RKM get affected when unbalanced data is given, and what is the nature of this impact?
    \item How can we extend Gen-RKM to make it generalize well on minority modes in the unbalanced data?
\end{itemize}

\begin{description}[leftmargin=0pt]
    \item[Contribution] The contributions of this thesis can be summarized in three main parts:
    \begin{itemize}[label={--}]
        \item We show that data imbalance leads to biased generation in Gen-RKM, identifying that this issue arises from a distorted latent space, where latent points corresponding to minority modes are blurred or intermixed with those of majority classes.
        \item Different weighted sampling schemes including inverse frequency sampling, RLS sampling and Iforest sampling are incorporated with the Gen-RKM framework in this thesis. A comprehensive set of experiments is conducted and their effectiveness is verified.
        \item We further extend Gen-RKM's capability to generate samples conditioned on a given label. By combining inverse frequency sampling with conditional generation, we show that it is possible to synthesize minority instances even when the training data is extremely unbalanced.  
    \end{itemize}

    \item[Thesis Organization] The rest of the thesis is structured as follows. Chapter \ref{chap-lr} reviews the theoretical foundation of Gen-RKM and discusses some related works in unbalanced data learning. Next, the problem of mode missing or mode collapse in Gen-RKM caused by unbalanced data is formally outlined and analyzed in Chapter \ref{chap-ps}. After that, Chapter \ref{chap-methods} introduces various weighted sampling schemes to tackle data imbalance problems under both supervised and unsupervised settings. In Chapter \ref{chap-expr}, a comprehensive experimental study is carried out to thoroughly assess the performance of different sampling techniques. Finally, the conclusion of this thesis and some suggestions of future work are given in Chapter \ref{chap-conclu}.
\end{description}