@article{pandeyGenerativeRestrictedKernel2021,
  title = {Generative {{Restricted Kernel Machines}}: {{A}} Framework for Multi-View Generation and Disentangled Feature Learning},
  shorttitle = {Generative {{Restricted Kernel Machines}}},
  author = {Pandey, Arun and Schreurs, Joachim and Suykens, Johan A. K.},
  date = {2021-03-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {135},
  pages = {177--191},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2020.12.010},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608020304275},
  urldate = {2024-03-18},
  abstract = {This paper introduces a novel framework for generative models based on Restricted Kernel Machines (RKMs) with joint multi-view generation and uncorrelated feature learning, called Gen-RKM. To enable joint multi-view generation, this mechanism uses a shared representation of data from various views. Furthermore, the model has a primal and dual formulation to incorporate both kernel-based and (deep convolutional) neural network based models within the same setting. When using neural networks as explicit feature-maps, a novel training procedure is proposed, which jointly learns the features and shared subspace representation. The latent variables are given by the eigen-decomposition of the kernel matrix, where the mutual orthogonality of eigenvectors represents the learned uncorrelated features. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of generated samples on various standard datasets.},
  keywords = {Deep learning,Generative models,Latent variable models,Restricted kernel machines},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\647PYIH2\\Pandey 等 - 2021 - Generative Restricted Kernel Machines A framework.pdf;C\:\\Users\\wenji\\Zotero\\storage\\AGGRQNCA\\S0893608020304275.html}
}

@article{suykensDeepRestrictedKernel2017,
  title = {Deep {{Restricted Kernel Machines Using Conjugate Feature Duality}}},
  author = {Suykens, Johan A. K.},
  date = {2017-08},
  journaltitle = {Neural Computation},
  volume = {29},
  number = {8},
  pages = {2123--2163},
  issn = {0899-7667},
  doi = {10.1162/neco_a_00984},
  url = {https://ieeexplore.ieee.org/abstract/document/7983485},
  urldate = {2024-03-18},
  abstract = {The aim of this letter is to propose a theory of deep restricted kernel machines offering new foundations for deep learning with kernel machines. From the viewpoint of deep learning, it is partially related to restricted Boltzmann machines, which are characterized by visible and hidden units in a bipartite graph without hidden-to-hidden connections and deep learning extensions as deep belief networks and deep Boltzmann machines. From the viewpoint of kernel machines, it includes least squares support vector machines for classification and regression, kernel principal component analysis (PCA), matrix singular value decomposition, and Parzen-type models. A key element is to first characterize these kernel machines in terms of so-called conjugate feature duality, yielding a representation with visible and hidden units. It is shown how this is related to the energy form in restricted Boltzmann machines, with continuous variables in a nonprobabilistic setting. In this new framework of so-called restricted kernel machine (RKM) representations, the dual variables correspond to hidden features. Deep RKM are obtained by coupling the RKMs. The method is illustrated for deep RKM, consisting of three levels with a least squares support vector machine regression level and two kernel PCA levels. In its primal form also deep feedforward neural networks can be trained within this framework.},
  eventtitle = {Neural {{Computation}}},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\LIZAPQ4I\\Suykens - 2017 - Deep Restricted Kernel Machines Using Conjugate Fe.pdf;C\:\\Users\\wenji\\Zotero\\storage\\6WWPD9D4\\7983485.html}
}

@inproceedings{goodfellowGenerativeAdversarialNets2014,
  title = {Generative {{Adversarial Nets}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  date = {2014},
  volume = {27},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html},
  urldate = {2024-03-18},
  abstract = {We propose a new framework for estimating generative models via adversarial nets, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitatively evaluation of the generated samples.},
  file = {C:\Users\wenji\Zotero\storage\IY8V28KF\Goodfellow 等 - 2014 - Generative Adversarial Nets.pdf}
}

@online{VideoGenerationModels,
  title = {Video Generation Models as World Simulators},
  url = {https://openai.com/research/video-generation-models-as-world-simulators},
  urldate = {2024-03-18},
  file = {C:\Users\wenji\Zotero\storage\SDLYF3QP\video-generation-models-as-world-simulators.html}
}

@article{brooksVideoGenerationModels2024,
  title = {Video Generation Models as World Simulators},
  author = {Brooks, Tim and Peebles, Bill and Holmes, Connor and DePue, Will and Guo, Yufei and Jing, Li and Schnurr, David and Taylor, Joe and Luhman, Troy and Luhman, Eric and Ng, Clarence and Wang, Ricky and Ramesh, Aditya},
  date = {2024},
  url = {https://openai.com/research/video-generation-models-as-world-simulators}
}

@online{kingmaAutoEncodingVariationalBayes2022,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  date = {2022-12-10},
  eprint = {1312.6114},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1312.6114},
  url = {http://arxiv.org/abs/1312.6114},
  urldate = {2024-03-18},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\Y9QF3XA9\\Kingma 和 Welling - 2022 - Auto-Encoding Variational Bayes.pdf;C\:\\Users\\wenji\\Zotero\\storage\\KCBMKY9L\\1312.html}
}

@inproceedings{schreursLeverageScoreSampling2022,
  title = {Leverage {{Score Sampling}} for {{Complete Mode Coverage}} in {{Generative Adversarial Networks}}},
  booktitle = {Machine {{Learning}}, {{Optimization}}, and {{Data Science}}},
  author = {Schreurs, Joachim and De Meulemeester, Hannes and Fanuel, Michaël and De Moor, Bart and Suykens, Johan A. K.},
  editor = {Nicosia, Giuseppe and Ojha, Varun and La Malfa, Emanuele and La Malfa, Gabriele and Jansen, Giorgio and Pardalos, Panos M. and Giuffrida, Giovanni and Umeton, Renato},
  date = {2022},
  pages = {466--480},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-95470-3_35},
  abstract = {Commonly, machine learning models minimize an empirical expectation. As a result, the trained models typically perform well for the majority of the data but the performance may deteriorate in less dense regions of the dataset. This issue also arises in generative modeling. A generative model may overlook underrepresented modes that are less frequent in the empirical data distribution. This problem is known as complete mode coverage. We propose a sampling procedure based on ridge leverage scores which significantly improves mode coverage when compared to standard methods and can easily be combined with any GAN. Ridge leverage scores are computed by using an explicit feature map, associated with the next-to-last layer of a GAN discriminator or of a pre-trained network, or by using an implicit feature map corresponding to a Gaussian kernel. Multiple evaluations against recent approaches of complete mode coverage show a clear improvement when using the proposed sampling strategy.},
  isbn = {978-3-030-95470-3},
  langid = {english},
  keywords = {Complete mode coverage,GANs,Leverage score sampling},
  file = {C:\Users\wenji\Zotero\storage\TJCZLTC7\Schreurs 等 - 2022 - Leverage Score Sampling for Complete Mode Coverage.pdf}
}

@online{schreursLeverageScoreSampling2021,
  title = {Leverage {{Score Sampling}} for {{Complete Mode Coverage}} in {{Generative Adversarial Networks}}},
  author = {Schreurs, Joachim and De Meulemeester, Hannes and Fanuel, Michaël and De Moor, Bart and Suykens, Johan A. K.},
  date = {2021-07-21},
  eprint = {2104.02373},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2104.02373},
  url = {http://arxiv.org/abs/2104.02373},
  urldate = {2024-03-18},
  abstract = {Commonly, machine learning models minimize an empirical expectation. As a result, the trained models typically perform well for the majority of the data but the performance may deteriorate in less dense regions of the dataset. This issue also arises in generative modeling. A generative model may overlook underrepresented modes that are less frequent in the empirical data distribution. This problem is known as complete mode coverage. We propose a sampling procedure based on ridge leverage scores which significantly improves mode coverage when compared to standard methods and can easily be combined with any GAN. Ridge leverage scores are computed by using an explicit feature map, associated with the next-to-last layer of a GAN discriminator or of a pre-trained network, or by using an implicit feature map corresponding to a Gaussian kernel. Multiple evaluations against recent approaches of complete mode coverage show a clear improvement when using the proposed sampling strategy.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\I64XI57Y\\Schreurs 等 - 2021 - Leverage Score Sampling for Complete Mode Coverage.pdf;C\:\\Users\\wenji\\Zotero\\storage\\LEQTIXV5\\2104.html}
}

@inproceedings{pandeyRobustGenerativeRestricted2020,
  title = {Robust {{Generative Restricted Kernel Machines Using Weighted Conjugate Feature Duality}}},
  booktitle = {Machine {{Learning}}, {{Optimization}}, and {{Data Science}}},
  author = {Pandey, Arun and Schreurs, Joachim and Suykens, Johan A. K.},
  editor = {Nicosia, Giuseppe and Ojha, Varun and La Malfa, Emanuele and Jansen, Giorgio and Sciacca, Vincenzo and Pardalos, Panos and Giuffrida, Giovanni and Umeton, Renato},
  date = {2020},
  pages = {613--624},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-64583-0_54},
  abstract = {Interest in generative models has grown tremendously in the past decade. However, their training performance can be adversely affected by contamination, where outliers are encoded in the representation of the model. This results in the generation of noisy data. In this paper, we introduce weighted conjugate feature duality in the framework of Restricted Kernel Machines (RKMs). The RKM formulation allows for an easy integration of methods from classical robust statistics. This formulation is used to fine-tune the latent space of generative RKMs using a weighting function based on the Minimum Covariance Determinant, which is a highly robust estimator of multivariate location and scatter. Experiments show that the weighted RKM is capable of generating clean images when contamination is present in the training data. We further show that the robust method also preserves uncorrelated feature learning through qualitative and quantitative experiments on standard datasets.},
  isbn = {978-3-030-64583-0},
  langid = {english},
  keywords = {Generative models,Kernel methods,Machine learning,Restricted kernel machines,Robustness},
  file = {C:\Users\wenji\Zotero\storage\XP5X6CSN\Pandey 等 - 2020 - Robust Generative Restricted Kernel Machines Using.pdf}
}

@inproceedings{wangImbalancedAdversarialTraining2022,
  title = {Imbalanced {{Adversarial Training}} with {{Reweighting}}},
  booktitle = {2022 {{IEEE International Conference}} on {{Data Mining}} ({{ICDM}})},
  author = {Wang, Wentao and Xu, Han and Liu, Xiaorui and Li, Yaxin and Thuraisingham, Bhavani and Tang, Jiliang},
  date = {2022-11},
  pages = {1209--1214},
  issn = {2374-8486},
  doi = {10.1109/ICDM54844.2022.00156},
  url = {https://ieeexplore.ieee.org/abstract/document/10027792},
  urldate = {2024-03-18},
  abstract = {Adversarial training has been empirically proven to be one of the most effective and reliable defense methods against adversarial attacks. However, the majority of existing studies are focused on balanced datasets, where each class has a similar amount of training examples. Research on adversarial training with imbalanced training datasets is rather limited. As the initial effort to investigate this problem, we reveal the facts that adversarially trained models present two distinguished behaviors from naturally trained models in imbalanced datasets: (1) Compared to natural training, adversarially trained models can suffer much worse performance on under-represented classes, when the training dataset is extremely imbalanced. (2) Traditional reweighting strategies which assign large weights to underrepresented classes will drastically hurt the model’s performance on well-represented classes. In this paper, to further understand our observations, we theoretically show that the poor data separability is one key reason causing this strong tension between under-represented and well-represented classes. Motivated by this finding, we propose the Separable Reweighted Adversarial Training (SRAT) framework to facilitate adversarial training under imbalanced scenarios, by learning more separable features for different classes. Extensive experiments on various datasets verify the effectiveness of the proposed framework.},
  eventtitle = {2022 {{IEEE International Conference}} on {{Data Mining}} ({{ICDM}})},
  keywords = {adversarial training,Behavioral sciences,Data mining,Data models,imbalanced data,Learning systems,model robustness,Reliability,reweighting,Training},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\YBPAYDQP\\Wang 等 - 2022 - Imbalanced Adversarial Training with Reweighting.pdf;C\:\\Users\\wenji\\Zotero\\storage\\TDHZIPLF\\10027792.html}
}

@article{fanuelDiversitySamplingImplicit2021,
  title = {Diversity {{Sampling}} Is an {{Implicit Regularization}} for {{Kernel Methods}}},
  author = {Fanuel, Michael and Schreurs, Joachim and Suykens, Johan},
  date = {2021-01},
  journaltitle = {SIAM Journal on Mathematics of Data Science},
  volume = {3},
  number = {1},
  pages = {280--297},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/20M1320031},
  url = {https://epubs.siam.org/doi/abs/10.1137/20M1320031},
  urldate = {2024-03-18},
  abstract = {Semiparametric regression models are used in several applications which require comprehensibility without sacrificing accuracy. Typical examples are spline interpolation in geophysics and nonlinear time series problems, where the system includes a linear and nonlinear component. We discuss here the use of a finite determinantal point process (DPP) for approximating semiparametric models. Recently, Barthelmé, Tremblay, Usevich, and Amblard introduced a novel representation of finite DPPs. These authors formulated extended L-ensembles that can conveniently represent partial-projection DPPs and suggest their use for optimal interpolation. With the help of this formalism, we derive a key identity illustrating the implicit regularization effect of determinantal sampling for semiparametric regression and interpolation. Also, a novel projected Nyström approximation is defined and used to derive a bound on the expected in-sample prediction error for the corresponding approximation of semiparametric regression. This work naturally extends similar results obtained for kernel ridge regression.},
  file = {C:\Users\wenji\Zotero\storage\HL347TZA\Fanuel 等 - 2021 - Diversity Sampling is an Implicit Regularization f.pdf}
}

@inproceedings{demeulemeesterBuresMetricGenerative2021,
  title = {The {{Bures Metric}} for {{Generative Adversarial Networks}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}. {{Research Track}}},
  author = {De Meulemeester, Hannes and Schreurs, Joachim and Fanuel, Michaël and De Moor, Bart and Suykens, Johan A. K.},
  editor = {Oliver, Nuria and Pérez-Cruz, Fernando and Kramer, Stefan and Read, Jesse and Lozano, Jose A.},
  date = {2021},
  pages = {52--66},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-86520-7_4},
  abstract = {Generative Adversarial Networks (GANs) are performant generative methods yielding high-quality samples. However, under certain circumstances, the training of GANs can lead to mode collapse or mode dropping. To address this problem, we use the last layer of the discriminator as a feature map to study the distribution of the real and the fake data. During training, we propose to match the real batch diversity to the fake batch diversity by using the Bures distance between covariance matrices in this feature space. The computation of the Bures distance can be conveniently done in either feature space or kernel space in terms of the covariance and kernel matrix respectively. We observe that diversity matching reduces mode collapse substantially and has a positive effect on sample quality. On the practical side, a very simple training procedure is proposed and assessed on several data sets.},
  isbn = {978-3-030-86520-7},
  langid = {english},
  keywords = {Generative Adversarial Networks,Mode collapse,Optimal transport},
  file = {C:\Users\wenji\Zotero\storage\KKB7NSVK\De Meulemeester 等 - 2021 - The Bures Metric for Generative Adversarial Networ.pdf}
}

@article{hajibabaeeKernelMatrixApproximation2021,
  title = {Kernel {{Matrix Approximation}} on {{Class-Imbalanced Data With}} an {{Application}} to {{Scientific Simulation}}},
  author = {Hajibabaee, Parisa and Pourkamali-Anaraki, Farhad and Hariri-Ardebili, Mohammad Amin},
  date = {2021},
  journaltitle = {IEEE Access},
  volume = {9},
  pages = {83579--83591},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3087730},
  url = {https://ieeexplore.ieee.org/abstract/document/9449889},
  urldate = {2024-03-19},
  abstract = {Generating low-rank approximations of kernel matrices that arise in nonlinear machine learning techniques holds the potential to significantly alleviate the memory and computational burdens. A compelling approach centers on finding a concise set of exemplars or landmarks to reduce the number of similarity measure evaluations from quadratic to linear concerning the data size. However, a key challenge is to regulate tradeoffs between the quality of landmarks and resource consumption. Despite the volume of research in this area, current understanding is limited regarding the performance of landmark selection techniques in the presence of class-imbalanced data sets that are becoming increasingly prevalent in many applications. Hence, this paper provides a comprehensive empirical investigation using several real-world imbalanced data sets, including scientific data, by evaluating the quality of approximate low-rank decompositions and examining their influence on the accuracy of downstream tasks. Furthermore, we present a new landmark selection technique called Distance-based Importance Sampling and Clustering (DISC), in which the relative importance scores are computed for improving accuracy-efficiency tradeoffs compared to existing works that range from probabilistic sampling to clustering methods. The proposed landmark selection method follows a coarse-to-fine strategy to capture the intrinsic structure of complex data sets, allowing us to substantially reduce the computational complexity and memory footprint with minimal loss in accuracy.},
  eventtitle = {{{IEEE Access}}},
  keywords = {Classification algorithms,Clustering algorithms,computational complexity,data compression,Eigenvalues and eigenfunctions,Kernel,Machine learning,Matrix decomposition,Memory management,pattern analysis,Task analysis,unsupervised learning},
  annotation = {rate: 1},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\4XU8MBSY\\Hajibabaee 等 - 2021 - Kernel Matrix Approximation on Class-Imbalanced Da.pdf;C\:\\Users\\wenji\\Zotero\\storage\\M7K4L7IV\\9449889.html}
}

@article{hofmannKernelMethodsMachine2008,
  title = {Kernel Methods in Machine Learning},
  author = {Hofmann, Thomas and Schölkopf, Bernhard and Smola, Alexander J.},
  date = {2008-06},
  journaltitle = {The Annals of Statistics},
  volume = {36},
  number = {3},
  pages = {1171--1220},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/009053607000000677},
  url = {https://projecteuclid.org/journals/annals-of-statistics/volume-36/issue-3/Kernel-methods-in-machine-learning/10.1214/009053607000000677.full},
  urldate = {2024-03-19},
  abstract = {We review machine learning methods employing positive definite kernels. These methods formulate learning and estimation problems in a reproducing kernel Hilbert space (RKHS) of functions defined on the data domain, expanded in terms of a kernel. Working in linear spaces of function has the benefit of facilitating the construction and analysis of learning algorithms while at the same time allowing large classes of functions. The latter include nonlinear functions as well as functions defined on nonvectorial data. We cover a wide range of methods, ranging from binary classifiers to sophisticated methods for estimation with structured data.},
  keywords = {30C40,68T05,graphical models,machine learning,reproducing kernels,Support vector machines},
  file = {C:\Users\wenji\Zotero\storage\LFQBF9JU\Hofmann 等 - 2008 - Kernel methods in machine learning.pdf}
}

@article{drineasNystromMethodApproximating2005,
  title = {On the {{Nystrom Method}} for {{Approximating}} a {{Gram Matrix}} for {{Improved Kernel-Based Learning}}},
  author = {Drineas, Petros and Mahoney, Michael W.},
  date = {2005},
  journaltitle = {Journal of Machine Learning Research},
  volume = {6},
  number = {72},
  pages = {2153--2175},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v6/drineas05a.html},
  urldate = {2024-03-19},
  abstract = {A problem for many kernel-based methods is that the amount of computation required to find the solution scales as O(n3), where n is the number of training examples. We develop and analyze an algorithm to compute an easily-interpretable low-rank approximation to an n × n Gram matrix G such that computations of interest may be performed more rapidly. The approximation is of the form \textasciitilde Gk = CWk+CT, where C is a matrix consisting of a small number c of columns of G and Wk is the best rank-k approximation to W, the matrix formed by the intersection between those c columns of G and the corresponding c rows of G. An important aspect of the algorithm is the probability distribution used to randomly sample the columns; we will use a judiciously-chosen and data-dependent nonuniform probability distribution. Let ||·||2 and ||·||F denote the spectral norm and the Frobenius norm, respectively, of a matrix, and let Gk be the best rank-k approximation to G. We prove that by choosing O(k/ε4) columns ||G-CWk+CT||ξ ≤ ||G-Gk||ξ + ε Σi=1n Gii2 , both in expectation and with high probability, for both ξ = 2, F, and for all k: 0 ≤ k ≤ rank(W). This approximation can be computed using O(n) additional space and time, after making two passes over the data from external storage. The relationships between this algorithm, other related matrix decompositions, and the Nyström method from integral equation theory are discussed.},
  file = {C:\Users\wenji\Zotero\storage\W6538GIP\Drineas 和 Mahoney - 2005 - On the Nystrom Method for Approximating a Gram Mat.pdf}
}

@inproceedings{williamsUsingNystromMethod2000,
  title = {Using the {{Nyström Method}} to {{Speed Up Kernel Machines}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Williams, Christopher and Seeger, Matthias},
  date = {2000},
  volume = {13},
  publisher = {MIT Press},
  url = {https://papers.nips.cc/paper_files/paper/2000/hash/19de10adbaa1b2ee13f77f679fa1483a-Abstract.html},
  urldate = {2024-03-19},
  file = {C:\Users\wenji\Zotero\storage\3CMBCEA8\Williams 和 Seeger - 2000 - Using the Nyström Method to Speed Up Kernel Machin.pdf}
}

@inproceedings{zhongRethinkingGenerativeMode2019,
  title = {Rethinking {{Generative Mode Coverage}}: {{A Pointwise Guaranteed Approach}}},
  shorttitle = {Rethinking {{Generative Mode Coverage}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhong, Peilin and Mo, Yuchen and Xiao, Chang and Chen, Pengyu and Zheng, Changxi},
  date = {2019},
  volume = {32},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.neurips.cc/paper_files/paper/2019/hash/68d13cf26c4b4f4f932e3eff990093ba-Abstract.html},
  urldate = {2024-03-20},
  abstract = {Many generative models have to combat missing modes. The conventional wisdom to this end is by reducing through training a statistical distance (such as f -divergence) between the generated distribution and provided data distribution. But this is more of a heuristic than a guarantee. The statistical distance measures a global, but not local, similarity between two distributions. Even if it is small, it does not imply a plausible mode coverage. Rethinking this problem from a game-theoretic perspective, we show that a complete mode coverage is firmly attainable. If a generative model can approximate a data distribution moderately well under a global statistical distance measure, then we will be able to find a mixture of generators that collectively covers every data point and thus every mode, with a lower-bounded generation probability. Constructing the generator mixture has a connection to the multiplicative weights update rule, upon which we propose our algorithm. We prove that our algorithm guarantees complete mode coverage. And our experiments on real and synthetic datasets confirm better mode coverage over recent approaches, ones that also use generator mixtures but rely on global statistical distances.},
  file = {C:\Users\wenji\Zotero\storage\HHEXWFPU\Zhong 等 - 2019 - Rethinking Generative Mode Coverage A Pointwise G.pdf}
}

@article{pandeyDisentangledRepresentationLearning2022,
  title = {Disentangled {{Representation Learning}} and {{Generation With Manifold Optimization}}},
  author = {Pandey, Arun and Fanuel, Michaël and Schreurs, Joachim and Suykens, Johan A. K.},
  date = {2022-09-12},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  volume = {34},
  number = {10},
  pages = {2009--2036},
  issn = {0899-7667},
  doi = {10.1162/neco_a_01528},
  url = {https://doi.org/10.1162/neco_a_01528},
  urldate = {2024-03-21},
  abstract = {Disentanglement is a useful property in representation learning, which increases the interpretability of generative models such as variational autoencoders (VAE), generative adversarial models, and their many variants. Typically in such models, an increase in disentanglement performance is traded off with generation quality. In the context of latent space models, this work presents a representation learning framework that explicitly promotes disentanglement by encouraging orthogonal directions of variations. The proposed objective is the sum of an autoencoder error term along with a principal component analysis reconstruction error in the feature space. This has an interpretation of a restricted kernel machine with the eigenvector matrix valued on the Stiefel manifold. Our analysis shows that such a construction promotes disentanglement by matching the principal directions in the latent space with the directions of orthogonal variation in data space. In an alternating minimization scheme, we use the Cayley ADAM algorithm, a stochastic optimization method on the Stiefel manifold along with the Adam optimizer. Our theoretical discussion and various experiments show that the proposed model is an improvement over many VAE variants in terms of both generation quality and disentangled representation learning.},
  file = {C:\Users\wenji\Zotero\storage\3VLHP4A3\Pandey 等 - 2022 - Disentangled Representation Learning and Generatio.pdf}
}

@article{toninDeepKernelPrincipal2024,
  title = {Deep {{Kernel Principal Component Analysis}} for Multi-Level Feature Learning},
  author = {Tonin, Francesco and Tao, Qinghua and Patrinos, Panagiotis and Suykens, Johan A. K.},
  date = {2024-02-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {170},
  pages = {578--595},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2023.11.045},
  url = {https://www.sciencedirect.com/science/article/pii/S089360802300669X},
  urldate = {2024-03-21},
  abstract = {Principal Component Analysis (PCA) and its nonlinear extension Kernel PCA (KPCA) are widely used across science and industry for data analysis and dimensionality reduction. Modern deep learning tools have achieved great empirical success, but a framework for deep principal component analysis is still lacking. Here we develop a deep kernel PCA methodology (DKPCA) to extract multiple levels of the most informative components of the data. Our scheme can effectively identify new hierarchical variables, called deep principal components, capturing the main characteristics of high-dimensional data through a simple and interpretable numerical optimization. We couple the principal components of multiple KPCA levels, theoretically showing that DKPCA creates both forward and backward dependency across levels, which has not been explored in kernel methods and yet is crucial to extract more informative features. Various experimental evaluations on multiple data types show that DKPCA finds more efficient and disentangled representations with higher explained variance in fewer principal components, compared to the shallow KPCA. We demonstrate that our method allows for effective hierarchical data exploration, with the ability to separate the key generative factors of the input data both for large datasets and when few training samples are available. Overall, DKPCA can facilitate the extraction of useful patterns from high-dimensional data by learning more informative features organized in different levels, giving diversified aspects to explore the variation factors in the data, while maintaining a simple mathematical formulation.},
  keywords = {Deep learning,Generative models,Kernel Principal Component Analysis,Manifold optimization},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\KHPQ7JYK\\Tonin 等 - 2024 - Deep Kernel Principal Component Analysis for multi.pdf;C\:\\Users\\wenji\\Zotero\\storage\\NJN3TUAJ\\S089360802300669X.html}
}

@inproceedings{toninUnsupervisedEnergybasedOutofdistribution2021,
  title = {Unsupervised {{Energy-based Out-of-distribution Detection}} Using {{Stiefel-Restricted Kernel Machine}}},
  booktitle = {2021 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Tonin, Francesco and Pandey, Arun and Patrinos, Panagiotis and Suykens, Johan A. K.},
  date = {2021-07},
  pages = {1--8},
  issn = {2161-4407},
  doi = {10.1109/IJCNN52387.2021.9533706},
  url = {https://ieeexplore.ieee.org/abstract/document/9533706},
  urldate = {2024-03-21},
  abstract = {Detecting out-of-distribution (OOD) samples is an essential requirement for the deployment of machine learning systems in the real world. Until now, research on energy-based OOD detectors has focused on the softmax confidence score from a pre-trained neural network classifier with access to class labels. In contrast, we propose an unsupervised energy-based OOD detector leveraging the Stiefel-Restricted Kernel Machine (St-RKM). Training requires minimizing an objective function with an autoencoder loss term and the RKM energy where the interconnection matrix lies on the Stiefel manifold. Further, we outline multiple energy function definitions based on the RKM framework and discuss their utility. In the experiments on standard datasets, the proposed method improves over the existing energy-based OOD detectors and deep generative models. Through several ablation studies, we further illustrate the merit of each proposed energy function on the OOD detection performance.},
  eventtitle = {2021 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  keywords = {Detectors,Kernel,Linear programming,Machine learning,Manifolds,Neural networks,Training},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\ZH7ZAR4L\\Tonin 等 - 2021 - Unsupervised Energy-based Out-of-distribution Dete.pdf;C\:\\Users\\wenji\\Zotero\\storage\\AU5VS7YX\\9533706.html}
}

@article{liImprovedGenerativeAdversarial2019,
  title = {Improved Generative Adversarial Networks with Reconstruction Loss},
  author = {Li, Yanchun and Xiao, Nanfeng and Ouyang, Wanli},
  date = {2019-01-05},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {323},
  pages = {363--372},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2018.10.014},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231218311901},
  urldate = {2024-03-22},
  abstract = {We propose a simple regularization scheme to handle the problem of mode missing and unstable training in the generative adversarial networks (GAN). The key idea is to utilize the visual features learned by the discriminator. We reconstruct the real data by feeding the generator with the real data features extracted by the discriminator. A reconstruction loss is added to the GAN’s objective function to enforce the generator can reconstruct from the features of the discriminator, which helps to explicitly guide the generator towards to near the probable configurations of real data. The proposed reconstruction loss improves the performance of GAN, produces higher quality images on different dataset, and can be easily combined with other regularization loss functions such as gradient penalty to improve the performance of various GANs. We conducted experiments on the widespread adopted architecture DCGAN and the complicated ResNet architecture across different datasets, the results of which show the effectiveness and robustness of our proposed method.},
  keywords = {Deep generative model,Generative adversarial networks (GAN),Image generation,Reconstruction loss},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\VNFCF4C6\\Li 等 - 2019 - Improved generative adversarial networks with reco.pdf;C\:\\Users\\wenji\\Zotero\\storage\\PKNSGSB8\\S0925231218311901.html}
}

@inproceedings{chenInfoGANInterpretableRepresentation2016,
  title = {{{InfoGAN}}: {{Interpretable Representation Learning}} by {{Information Maximizing Generative Adversarial Nets}}},
  shorttitle = {{{InfoGAN}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
  date = {2016},
  volume = {29},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2016/hash/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Abstract.html},
  urldate = {2024-03-23},
  abstract = {This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods.},
  file = {C:\Users\wenji\Zotero\storage\26M5VD3V\Chen 等 - 2016 - InfoGAN Interpretable Representation Learning by .pdf}
}

@inproceedings{scholkopfKernelPrincipalComponent1997,
  title = {Kernel Principal Component Analysis},
  booktitle = {Artificial {{Neural Networks}} — {{ICANN}}'97},
  author = {Schölkopf, Bernhard and Smola, Alexander and Müller, Klaus-Robert},
  editor = {Gerstner, Wulfram and Germond, Alain and Hasler, Martin and Nicoud, Jean-Daniel},
  date = {1997},
  pages = {583--588},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/BFb0020217},
  abstract = {A new method for performing a nonlinear form of Principal Component Analysis is proposed. By the use of integral operator kernel functions, one can efficiently compute principal components in highdimensional feature spaces, related to input space by some nonlinear map; for instance the space of all possible d-pixel products in images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition.},
  isbn = {978-3-540-69620-9},
  langid = {english},
  keywords = {Feature Space,Independent Component Analysis,Kernel Principal Component Analysis,Standard Principal Component Analysis,Support Vector Machine},
  file = {C:\Users\wenji\Zotero\storage\SFIU6M9E\Schölkopf 等 - 1997 - Kernel principal component analysis.pdf}
}

@inproceedings{mikaKernelPCADeNoising1998,
  title = {Kernel {{PCA}} and {{De-Noising}} in {{Feature Spaces}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Mika, Sebastian and Schölkopf, Bernhard and Smola, Alex and Müller, Klaus-Robert and Scholz, Matthias and Rätsch, Gunnar},
  date = {1998},
  volume = {11},
  publisher = {MIT Press},
  url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/226d1f15ecd35f784d2a20c3ecf56d7f-Abstract.html},
  urldate = {2024-03-23},
  abstract = {Kernel  PCA  as  a  nonlinear feature  extractor has  proven powerful  as  a  preprocessing step for classification algorithms.  But it can also be con(cid:173) sidered  as  a  natural  generalization of linear principal  component anal(cid:173) ysis.  This  gives  rise  to  the  question  how  to  use  nonlinear features  for  data compression, reconstruction, and de-noising, applications common  in  linear PCA.  This is  a nontrivial  task,  as the results provided by  ker(cid:173) nel PCA live in some high dimensional feature space and need not have  pre-images in  input space.  This work presents ideas for finding approxi(cid:173) mate pre-images, focusing on Gaussian kernels, and shows experimental  results  using  these pre-images in  data reconstruction and de-noising on  toy examples as well as on real world data.},
  file = {C:\Users\wenji\Zotero\storage\QLP6AZBD\Mika 等 - 1998 - Kernel PCA and De-Noising in Feature Spaces.pdf}
}

@online{zhaoEnergybasedGenerativeAdversarial2017,
  title = {Energy-Based {{Generative Adversarial Network}}},
  author = {Zhao, Junbo and Mathieu, Michael and LeCun, Yann},
  date = {2017-03-06},
  eprint = {1609.03126},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1609.03126},
  url = {http://arxiv.org/abs/1609.03126},
  urldate = {2024-03-23},
  abstract = {We introduce the "Energy-based Generative Adversarial Network" model (EBGAN) which views the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions. Similar to the probabilistic GANs, a generator is seen as being trained to produce contrastive samples with minimal energies, while the discriminator is trained to assign high energies to these generated samples. Viewing the discriminator as an energy function allows to use a wide variety of architectures and loss functionals in addition to the usual binary classifier with logistic output. Among them, we show one instantiation of EBGAN framework as using an auto-encoder architecture, with the energy being the reconstruction error, in place of the discriminator. We show that this form of EBGAN exhibits more stable behavior than regular GANs during training. We also show that a single-scale architecture can be trained to generate high-resolution images.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\K7LCYACZ\\Zhao 等 - 2017 - Energy-based Generative Adversarial Network.pdf;C\:\\Users\\wenji\\Zotero\\storage\\9HJT2HGM\\1609.html}
}

@article{bekkarImbalancedDataLearning2013,
  title = {Imbalanced {{Data Learning Approaches Review}}},
  author = {Bekkar, Mohamed and Alitouche, Taklit},
  date = {2013-07-01},
  journaltitle = {International Journal of Data Mining \& Knowledge Management Process},
  shortjournal = {International Journal of Data Mining \& Knowledge Management Process},
  volume = {3},
  doi = {10.5121/ijdkp.2013.340},
  abstract = {The present work deals with a well-known problem inmachine learning, that classes have generallyskewed prior probabilities distribution. This situation of imbalanced data is a handicap when tryingtoidentify the minority classes , usually more interesting one In real world applications. This paper isanattempt to list the different approachs proposed inscientific research to deal with the imbalanced datalearning, as well a comparison between various applications cases performed on this subject.},
  file = {C:\Users\wenji\Zotero\storage\C6U9CUIW\Bekkar 和 Alitouche - 2013 - Imbalanced Data Learning Approaches Review.pdf}
}

@article{heLearningImbalancedData2009,
  title = {Learning from {{Imbalanced Data}}},
  author = {He, Haibo and Garcia, Edwardo A.},
  date = {2009-09},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {21},
  number = {9},
  pages = {1263--1284},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2008.239},
  url = {https://ieeexplore.ieee.org/abstract/document/5128907},
  urldate = {2024-03-24},
  abstract = {With the continuous expansion of data availability in many large-scale, complex, and networked systems, such as surveillance, security, Internet, and finance, it becomes critical to advance the fundamental understanding of knowledge discovery and analysis from raw data to support decision-making processes. Although existing knowledge discovery and data engineering techniques have shown great success in many real-world applications, the problem of learning from imbalanced data (the imbalanced learning problem) is a relatively new challenge that has attracted growing attention from both academia and industry. The imbalanced learning problem is concerned with the performance of learning algorithms in the presence of underrepresented data and severe class distribution skews. Due to the inherent complex characteristics of imbalanced data sets, learning from such data requires new understandings, principles, algorithms, and tools to transform vast amounts of raw data efficiently into information and knowledge representation. In this paper, we provide a comprehensive review of the development of research in learning from imbalanced data. Our focus is to provide a critical review of the nature of the problem, the state-of-the-art technologies, and the current assessment metrics used to evaluate learning performance under the imbalanced learning scenario. Furthermore, in order to stimulate future research in this field, we also highlight the major opportunities and challenges, as well as potential important research directions for learning from imbalanced data.},
  eventtitle = {{{IEEE Transactions}} on {{Knowledge}} and {{Data Engineering}}},
  keywords = {active learning,assessment metrics.,Availability,classification,cost-sensitive learning,Data analysis,Data engineering,Data security,Decision making,Finance,Imbalanced learning,IP networks,kernel-based learning,Knowledge representation,Large-scale systems,sampling methods,Surveillance},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\Y28IEGLM\\He 和 Garcia - 2009 - Learning from Imbalanced Data.pdf;C\:\\Users\\wenji\\Zotero\\storage\\9I5Q6M4X\\5128907.html}
}

@inproceedings{winantLatentSpaceExploration2020,
  title = {Latent {{Space Exploration Using Generative Kernel PCA}}},
  booktitle = {Artificial {{Intelligence}} and {{Machine Learning}}},
  author = {Winant, David and Schreurs, Joachim and Suykens, Johan A. K.},
  editor = {Bogaerts, Bart and Bontempi, Gianluca and Geurts, Pierre and Harley, Nick and Lebichot, Bertrand and Lenaerts, Tom and Louppe, Gilles},
  date = {2020},
  pages = {70--82},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-65154-1_5},
  abstract = {Kernel PCA is a powerful feature extractor which recently has seen a reformulation in the context of Restricted Kernel Machines (RKMs). These RKMs allow for a representation of kernel PCA in terms of hidden and visible units similar to Restricted Boltzmann Machines. This connection has led to insights on how to use kernel PCA in a generative procedure, called generative kernel PCA. In this paper, the use of generative kernel PCA for exploring latent spaces of datasets is investigated. New points can be generated by gradually moving in the latent space, which allows for an interpretation of the components. Firstly, examples of this feature space exploration on three datasets are shown with one of them leading to an interpretable representation of ECG signals. Afterwards, the use of the tool in combination with novelty detection is shown, where the latent space around novel patterns in the data is explored. This helps in the interpretation of why certain points are considered as novel.},
  isbn = {978-3-030-65154-1},
  langid = {english},
  keywords = {Kernel PCA,Latent space exploration,Restricted Kernel Machines},
  file = {C:\Users\wenji\Zotero\storage\E2ETXR7Z\Winant 等 - 2020 - Latent Space Exploration Using Generative Kernel P.pdf}
}

@inproceedings{renLearningReweightExamples2018,
  title = {Learning to {{Reweight Examples}} for {{Robust Deep Learning}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Ren, Mengye and Zeng, Wenyuan and Yang, Bin and Urtasun, Raquel},
  date = {2018-07-03},
  pages = {4334--4343},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v80/ren18a.html},
  urldate = {2024-03-24},
  abstract = {Deep neural networks have been shown to be very powerful modeling tools for many supervised learning tasks involving complex input patterns. However, they can also easily overfit to training set biases and label noises. In addition to various regularizers, example reweighting algorithms are popular solutions to these problems, but they require careful tuning of additional hyperparameters, such as example mining schedules and regularization hyperparameters. In contrast to past reweighting methods, which typically consist of functions of the cost value of each example, in this work we propose a novel meta-learning algorithm that learns to assign weights to training examples based on their gradient directions. To determine the example weights, our method performs a meta gradient descent step on the current mini-batch example weights (which are initialized from zero) to minimize the loss on a clean unbiased validation set. Our proposed method can be easily implemented on any type of deep network, does not require any additional hyperparameter tuning, and achieves impressive performance on class imbalance and corrupted label problems where only a small amount of clean validation data is available.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {C:\Users\wenji\Zotero\storage\4YHR6DRD\Ren 等 - 2018 - Learning to Reweight Examples for Robust Deep Lear.pdf}
}

@inproceedings{diesendruckImportanceWeightedGenerative2020,
  title = {Importance {{Weighted Generative Networks}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Diesendruck, Maurice and Elenberg, Ethan R. and Sen, Rajat and Cole, Guy W. and Shakkottai, Sanjay and Williamson, Sinead A.},
  editor = {Brefeld, Ulf and Fromont, Elisa and Hotho, Andreas and Knobbe, Arno and Maathuis, Marloes and Robardet, Céline},
  date = {2020},
  pages = {249--265},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-46147-8_15},
  abstract = {While deep generative networks can simulate from complex data distributions, their utility can be hindered by limitations on the data available for training. Specifically, the training data distribution may differ from the target sampling distribution due to sample selection bias, or because the training data comes from a different but related distribution. We present methods to accommodate this difference via importance weighting, which allow us to estimate a loss function with respect to a target distribution even if we cannot access that distribution directly. These estimators, which differentially weight the contribution of data to the loss function, offer theoretical guarantees that heuristic approaches lack, while giving impressive empirical performance in a variety of settings.},
  isbn = {978-3-030-46147-8},
  langid = {english},
  keywords = {Bias correction,Generative networks,Importance weights},
  file = {C:\Users\wenji\Zotero\storage\3GR2LCW8\Diesendruck 等 - 2020 - Importance Weighted Generative Networks.pdf}
}

@article{murpheyNeuralLearningUnbalanced2004,
  title = {Neural {{Learning}} from {{Unbalanced Data}}},
  author = {Murphey, Yi L. and Guo, Hong and Feldkamp, Lee A.},
  date = {2004-09-01},
  journaltitle = {Applied Intelligence},
  shortjournal = {Applied Intelligence},
  volume = {21},
  number = {2},
  pages = {117--128},
  issn = {1573-7497},
  doi = {10.1023/B:APIN.0000033632.42843.17},
  url = {https://doi.org/10.1023/B:APIN.0000033632.42843.17},
  urldate = {2024-04-01},
  abstract = {This paper describes the result of our study on neural learning to solve the classification problems in which data is unbalanced and noisy. We conducted the study on three different neural network architectures, multi-layered Back Propagation, Radial Basis Function, and Fuzzy ARTMAP using three different training methods, duplicating minority class examples, Snowball technique and multidimensional Gaussian modeling of data noise. Three major issues are addressed: neural learning from unbalanced data examples, neural learning from noisy data, and making intentional biased decisions. We argue that by properly generated extra training data examples around the noise densities, we can train a neural network that has a stronger capability of generalization and better control of the classification error of the trained neural network. In particular, we focus on problems that require a neural network to make favorable classification to a particular class such as classifying normal(pass)/abnormal(fail) vehicles in an assembly plant. In addition, we present three methods that quantitatively measure the noise level of a given data set. All experiments were conducted using data examples downloaded directly from test sites of an automobile assembly plant. The experimental results showed that the proposed multidimensional Gaussian noise modeling algorithm was very effective in generating extra data examples that can be used to train a neural network to make favorable decisions for the minority class and to have increased generalization capability.},
  langid = {english},
  keywords = {data noise,machine learning,neural networks,unbalanced data},
  file = {C:\Users\wenji\Zotero\storage\2IP62SEG\Murphey 等 - 2004 - Neural Learning from Unbalanced Data.pdf}
}

@inproceedings{avronSubspaceEmbeddingsPolynomial2014,
  title = {Subspace {{Embeddings}} for the {{Polynomial Kernel}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Avron, Haim and Nguyen, Huy and Woodruff, David},
  date = {2014},
  volume = {27},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper_files/paper/2014/hash/b571ecea16a9824023ee1af16897a582-Abstract.html},
  urldate = {2024-04-01},
  abstract = {Sketching is a powerful dimensionality reduction tool for accelerating statistical learning algorithms. However, its applicability has been limited to a certain extent since the crucial ingredient, the so-called oblivious subspace embedding, can only be applied to data spaces with an explicit representation as the column span or row span of a matrix, while in many settings learning is done in a high-dimensional space implicitly defined by the data matrix via a kernel transformation. We propose the first \{\textbackslash em fast\} oblivious subspace embeddings that are able to embed a space induced by a non-linear kernel \{\textbackslash em without\} explicitly mapping the data to the high-dimensional space. In particular, we propose an embedding for mappings induced by the polynomial kernel. Using the subspace embeddings, we obtain the fastest known algorithms for computing an implicit low rank approximation of the higher-dimension mapping of the data matrix, and for computing an approximate kernel PCA of the data, as well as doing approximate kernel principal component regression.},
  file = {C:\Users\wenji\Zotero\storage\PD6YBEDL\Avron 等 - 2014 - Subspace Embeddings for the Polynomial Kernel.pdf}
}

@online{ghojoghEigenvalueGeneralizedEigenvalue2023,
  title = {Eigenvalue and {{Generalized Eigenvalue Problems}}: {{Tutorial}}},
  shorttitle = {Eigenvalue and {{Generalized Eigenvalue Problems}}},
  author = {Ghojogh, Benyamin and Karray, Fakhri and Crowley, Mark},
  date = {2023-05-20},
  eprint = {1903.11240},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1903.11240},
  url = {http://arxiv.org/abs/1903.11240},
  urldate = {2024-04-02},
  abstract = {This paper is a tutorial for eigenvalue and generalized eigenvalue problems. We first introduce eigenvalue problem, eigen-decomposition (spectral decomposition), and generalized eigenvalue problem. Then, we mention the optimization problems which yield to the eigenvalue and generalized eigenvalue problems. We also provide examples from machine learning, including principal component analysis, kernel supervised principal component analysis, and Fisher discriminant analysis, which result in eigenvalue and generalized eigenvalue problems. Finally, we introduce the solutions to both eigenvalue and generalized eigenvalue problems.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\QE9HMVGG\\Ghojogh 等 - 2023 - Eigenvalue and Generalized Eigenvalue Problems Tu.pdf;C\:\\Users\\wenji\\Zotero\\storage\\7M6LLS4U\\1903.html}
}

@book{boydConvexOptimization2004,
  title = {Convex Optimization},
  author = {Boyd, Stephen P and Vandenberghe, Lieven},
  date = {2004},
  publisher = {Cambridge university press},
  isbn = {0-521-83378-7},
  file = {C:\Users\wenji\Zotero\storage\JMEE5R3K\Boyd_Vandenberghe_2004_Convex optimization.pdf}
}

@article{toninUnsupervisedLearningDisentangled2021,
  title = {Unsupervised Learning of Disentangled Representations in Deep Restricted Kernel Machines with Orthogonality Constraints},
  author = {Tonin, Francesco and Patrinos, Panagiotis and Suykens, Johan A. K.},
  date = {2021-10-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {142},
  pages = {661--679},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2021.07.023},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608021002860},
  urldate = {2024-04-03},
  abstract = {We introduce Constr-DRKM, a deep kernel method for the unsupervised learning of disentangled data representations. We propose augmenting the original deep restricted kernel machine formulation for kernel PCA by orthogonality constraints on the latent variables to promote disentanglement and to make it possible to carry out optimization without first defining a stabilized objective. After discussing a number of algorithms for end-to-end training, we quantitatively evaluate the proposed method’s effectiveness in disentangled feature learning. We demonstrate on four benchmark datasets that this approach performs similarly overall to β-VAE on several disentanglement metrics when few training points are available while being less sensitive to randomness and hyperparameter selection than β-VAE. We also present a deterministic initialization of Constr-DRKM’s training algorithm that significantly improves the reproducibility of the results. Finally, we empirically evaluate and discuss the role of the number of layers in the proposed methodology, examining the influence of each principal component in every layer and showing that components in lower layers act as local feature detectors capturing the broad trends of the data distribution, while components in deeper layers use the representation learned by previous layers and more accurately reproduce higher-level features.},
  keywords = {Kernel methods,Learning disentangled representations,Manifold learning,Unsupervised learning},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\GF4NW9AR\\Tonin 等 - 2021 - Unsupervised learning of disentangled representati.pdf;C\:\\Users\\wenji\\Zotero\\storage\\39KJBDWL\\S0893608021002860.html}
}

@online{liEfficientRiemannianOptimization2020,
  title = {Efficient {{Riemannian Optimization}} on the {{Stiefel Manifold}} via the {{Cayley Transform}}},
  author = {Li, Jun and Fuxin, Li and Todorovic, Sinisa},
  date = {2020-02-03},
  eprint = {2002.01113},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2002.01113},
  url = {http://arxiv.org/abs/2002.01113},
  urldate = {2024-04-03},
  abstract = {Strictly enforcing orthonormality constraints on parameter matrices has been shown advantageous in deep learning. This amounts to Riemannian optimization on the Stiefel manifold, which, however, is computationally expensive. To address this challenge, we present two main contributions: (1) A new efficient retraction map based on an iterative Cayley transform for optimization updates, and (2) An implicit vector transport mechanism based on the combination of a projection of the momentum and the Cayley transform on the Stiefel manifold. We specify two new optimization algorithms: Cayley SGD with momentum, and Cayley ADAM on the Stiefel manifold. Convergence of Cayley SGD is theoretically analyzed. Our experiments for CNN training demonstrate that both algorithms: (a) Use less running time per iteration relative to existing approaches that enforce orthonormality of CNN parameters; and (b) Achieve faster convergence rates than the baseline SGD and ADAM algorithms without compromising the performance of the CNN. Cayley SGD and Cayley ADAM are also shown to reduce the training time for optimizing the unitary transition matrices in RNNs.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\FNS4AWEJ\\Li 等 - 2020 - Efficient Riemannian Optimization on the Stiefel M.pdf;C\:\\Users\\wenji\\Zotero\\storage\\ZP63JHH8\\2002.html}
}

@online{wangDisentangledRepresentationLearning2023,
  title = {Disentangled {{Representation Learning}}},
  author = {Wang, Xin and Chen, Hong and Tang, Si'ao and Wu, Zihao and Zhu, Wenwu},
  date = {2023-08-16},
  eprint = {2211.11695},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2211.11695},
  url = {http://arxiv.org/abs/2211.11695},
  urldate = {2024-04-03},
  abstract = {Disentangled Representation Learning (DRL) aims to learn a model capable of identifying and disentangling the underlying factors hidden in the observable data in representation form. The process of separating underlying factors of variation into variables with semantic meaning benefits in learning explainable representations of data, which imitates the meaningful understanding process of humans when observing an object or relation. As a general learning strategy, DRL has demonstrated its power in improving the model explainability, controlability, robustness, as well as generalization capacity in a wide range of scenarios such as computer vision, natural language processing, data mining etc. In this article, we comprehensively review DRL from various aspects including motivations, definitions, methodologies, evaluations, applications and model designs. We discuss works on DRL based on two well-recognized definitions, i.e., Intuitive Definition and Group Theory Definition. We further categorize the methodologies for DRL into four groups, i.e., Traditional Statistical Approaches, Variational Auto-encoder Based Approaches, Generative Adversarial Networks Based Approaches, Hierarchical Approaches and Other Approaches. We also analyze principles to design different DRL models that may benefit different tasks in practical applications. Finally, we point out challenges in DRL as well as potential research directions deserving future investigations. We believe this work may provide insights for promoting the DRL research in the community.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\D658PHSA\\Wang 等 - 2023 - Disentangled Representation Learning.pdf;C\:\\Users\\wenji\\Zotero\\storage\\6FV6PNY2\\2211.html}
}

@inproceedings{ojhaElasticInfoGANUnsupervisedDisentangled2020,
  title = {Elastic-{{InfoGAN}}: {{Unsupervised Disentangled Representation Learning}} in {{Class-Imbalanced Data}}},
  shorttitle = {Elastic-{{InfoGAN}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ojha, Utkarsh and Singh, Krishna Kumar and Hsieh, Cho-Jui and Lee, Yong Jae},
  date = {2020},
  volume = {33},
  pages = {18063--18075},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2020/hash/d1e39c9bda5c80ac3d8ea9d658163967-Abstract.html},
  urldate = {2024-04-04},
  abstract = {We propose a novel unsupervised generative model that learns to disentangle object identity from other low-level aspects in class-imbalanced data. We first investigate the issues surrounding the assumptions about uniformity made by InfoGAN, and demonstrate its ineffectiveness to properly disentangle object identity in imbalanced data. Our key idea is to make the discovery of the discrete latent factor of variation invariant to identity-preserving transformations in real images, and use that as a signal to learn the appropriate latent distribution representing object identity. Experiments on both artificial (MNIST, 3D cars, 3D chairs, ShapeNet) and real-world (YouTube-Faces) imbalanced datasets demonstrate the effectiveness of our method in disentangling object identity as a latent factor of variation.},
  file = {C:\Users\wenji\Zotero\storage\H424UR7H\Ojha 等 - 2020 - Elastic-InfoGAN Unsupervised Disentangled Represe.pdf}
}

@inproceedings{dupontLearningDisentangledJoint2018,
  title = {Learning {{Disentangled Joint Continuous}} and {{Discrete Representations}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Dupont, Emilien},
  date = {2018},
  volume = {31},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2018/hash/b9228e0962a78b84f3d5d92f4faa000b-Abstract.html},
  urldate = {2024-04-04},
  abstract = {We present a framework for learning disentangled and interpretable jointly continuous and discrete representations in an unsupervised manner. By augmenting the continuous latent distribution of variational autoencoders with a relaxed discrete distribution and controlling the amount of information encoded in each latent unit, we show how continuous and categorical factors of variation can be discovered automatically from data. Experiments show that the framework disentangles continuous and discrete generative factors on various datasets and outperforms current disentangling methods when a discrete generative factor is prominent.},
  file = {C:\Users\wenji\Zotero\storage\843FNRS8\Dupont - 2018 - Learning Disentangled Joint Continuous and Discret.pdf}
}

@online{achtenDualityMultiViewRestricted2023,
  title = {Duality in {{Multi-View Restricted Kernel Machines}}},
  author = {Achten, Sonny and Pandey, Arun and De Meulemeester, Hannes and De Moor, Bart and Suykens, Johan A. K.},
  date = {2023-07-06},
  eprint = {2305.17251},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.17251},
  url = {http://arxiv.org/abs/2305.17251},
  urldate = {2024-04-04},
  abstract = {We propose a unifying setting that combines existing restricted kernel machine methods into a single primal-dual multi-view framework for kernel principal component analysis in both supervised and unsupervised settings. We derive the primal and dual representations of the framework and relate different training and inference algorithms from a theoretical perspective. We show how to achieve full equivalence in primal and dual formulations by rescaling primal variables. Finally, we experimentally validate the equivalence and provide insight into the relationships between different methods on a number of time series data sets by recursively forecasting unseen test data and visualizing the learned features.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\MWT5CAK6\\Achten 等 - 2023 - Duality in Multi-View Restricted Kernel Machines.pdf;C\:\\Users\\wenji\\Zotero\\storage\\2G732CDU\\2305.html}
}

@inproceedings{salakhutdinovDeepBoltzmannMachines2009,
  title = {Deep {{Boltzmann Machines}}},
  booktitle = {Proceedings of the {{Twelth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Salakhutdinov, Ruslan and Hinton, Geoffrey},
  date = {2009-04-15},
  pages = {448--455},
  publisher = {PMLR},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v5/salakhutdinov09a.html},
  urldate = {2024-04-04},
  abstract = {We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and data-independent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer “pre-training” phase that allows variational inference to be initialized by a single bottom-up pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks.},
  eventtitle = {Artificial {{Intelligence}} and {{Statistics}}},
  langid = {english},
  file = {C:\Users\wenji\Zotero\storage\BF4SSXZ7\Salakhutdinov 和 Hinton - 2009 - Deep Boltzmann Machines.pdf}
}

@inproceedings{salakhutdinovEfficientLearningDeep2010,
  title = {Efficient {{Learning}} of {{Deep Boltzmann Machines}}},
  booktitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Salakhutdinov, Ruslan and Larochelle, Hugo},
  date = {2010-03-31},
  pages = {693--700},
  publisher = {{JMLR Workshop and Conference Proceedings}},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v9/salakhutdinov10a.html},
  urldate = {2024-04-04},
  abstract = {We present a new approximate inference algorithm for Deep Boltzmann Machines (DBM’s), a generative model with many layers of hidden variables. The algorithm learns a separate “recognition” model that is used to quickly initialize, in a single bottom-up pass, the values of the latent variables in all hidden layers. We show that using such a recognition model, followed by a combined top-down and bottom-up pass, it is possible to efficiently learn a good generative model of high-dimensional highly-structured sensory input. We show that the additional computations required by incorporating a top-down feedback plays a critical role in the performance of a DBM, both as a generative and discriminative model. Moreover, inference is only at most three times slower compared to the approximate inference in a Deep Belief Network (DBN), making large-scale learning of DBM’s practical. Finally, we demonstrate that the DBM’s trained using the proposed approximate inference algorithm perform well compared to DBN’s and SVM’s on the MNIST handwritten digit, OCR English letters, and NORB visual object recognition tasks.},
  eventtitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  langid = {english},
  file = {C:\Users\wenji\Zotero\storage\2RNTAA24\Salakhutdinov 和 Larochelle - 2010 - Efficient Learning of Deep Boltzmann Machines.pdf}
}

@article{suykensPrimalDualModel2010,
  title = {Primal and Dual Model Representations in Kernel-Based Learning},
  author = {Suykens, Johan A. K. and Alzate, Carlos and Pelckmans, Kristiaan},
  date = {2010-01},
  journaltitle = {Statistics Surveys},
  volume = {4},
  pages = {148--183},
  publisher = {{Amer. Statist. Assoc., the Bernoulli Soc., the Inst. Math. Statist., and the Statist. Soc. Canada}},
  issn = {1935-7516},
  doi = {10.1214/09-SS052},
  url = {https://projecteuclid.org/journals/statistics-surveys/volume-4/issue-none/Primal-and-dual-model-representations-in-kernel-based-learning/10.1214/09-SS052.full},
  urldate = {2024-04-04},
  abstract = {This paper discusses the role of primal and (Lagrange) dual model representations in problems of supervised and unsupervised learning. The specification of the estimation problem is conceived at the primal level as a constrained optimization problem. The constraints relate to the model which is expressed in terms of the feature map. From the conditions for optimality one jointly finds the optimal model representation and the model estimate. At the dual level the model is expressed in terms of a positive definite kernel function, which is characteristic for a support vector machine methodology. It is discussed how least squares support vector machines are playing a central role as core models across problems of regression, classification, principal component analysis, spectral clustering, canonical correlation analysis, dimensionality reduction and data visualization.},
  issue = {none},
  keywords = {canonical correlation analysis,‎classification‎,constrained optimization,dimensionality reduction and data visualization,feature map,independence,kernel methods,primal and dual problem,Principal Component Analysis,regression,robustness,sparseness,spectral clustering,Support vector machines},
  file = {C:\Users\wenji\Zotero\storage\FNQJGEBF\Suykens 等 - 2010 - Primal and dual model representations in kernel-ba.pdf}
}

@article{suykensWeightedLeastSquares2002,
  title = {Weighted Least Squares Support Vector Machines: Robustness and Sparse Approximation},
  shorttitle = {Weighted Least Squares Support Vector Machines},
  author = {Suykens, J. A. K. and De Brabanter, J. and Lukas, L. and Vandewalle, J.},
  date = {2002-10-01},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {48},
  number = {1},
  pages = {85--105},
  issn = {0925-2312},
  doi = {10.1016/S0925-2312(01)00644-0},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231201006440},
  urldate = {2024-04-04},
  abstract = {Least squares support vector machines (LS-SVM) is an SVM version which involves equality instead of inequality constraints and works with a least squares cost function. In this way, the solution follows from a linear Karush–Kuhn–Tucker system instead of a quadratic programming problem. However, sparseness is lost in the LS-SVM case and the estimation of the support values is only optimal in the case of a Gaussian distribution of the error variables. In this paper, we discuss a method which can overcome these two drawbacks. We show how to obtain robust estimates for regression by applying a weighted version of LS-SVM. We also discuss a sparse approximation procedure for weighted and unweighted LS-SVM. It is basically a pruning method which is able to do pruning based upon the physical meaning of the sorted support values, while pruning procedures for classical multilayer perceptrons require the computation of a Hessian matrix or its inverse. The methods of this paper are illustrated for RBF kernels and demonstrate how to obtain robust estimates with selection of an appropriate number of hidden units, in the case of outliers or non-Gaussian error distributions with heavy tails.},
  keywords = {(Weighted) least squares,Ridge regression,Robust estimation,Sparse approximation,Support vector machines},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\SRRPXDKA\\Suykens 等 - 2002 - Weighted least squares support vector machines ro.pdf;C\:\\Users\\wenji\\Zotero\\storage\\8J5LZP6M\\S0925231201006440.html}
}

@inproceedings{alzateWeightedKernelPCA2006,
  title = {A {{Weighted Kernel PCA Formulation}} with {{Out-of-Sample Extensions}} for {{Spectral Clustering Methods}}},
  booktitle = {The 2006 {{IEEE International Joint Conference}} on {{Neural Network Proceedings}}},
  author = {Alzate, C. and Suykens, J.A.K.},
  date = {2006-07},
  pages = {138--144},
  issn = {2161-4407},
  doi = {10.1109/IJCNN.2006.246671},
  url = {https://ieeexplore.ieee.org/document/1716082},
  urldate = {2024-04-04},
  abstract = {A new formulation to spectral clustering methods based on the weighted kernel principal component analysis is presented. This formulation fits in the Least Squares Support Vector Machines (LS-SVM) framework as a primal-dual interpretation in the context of constrained optimization problems. Starting from the LS-SVM formulation to kernel PCA, a weighted approach is derived. An advantage of this method is the possibility to apply the trained clustering model to out-of-sample (test) data points without using approximation techniques such as the Nystrom method. Links with some existing spectral clustering techniques are given, showing that these techniques are particular cases of weighted kernel PCA. Simulation results with toy and real-life data show improvements in terms of generalization to new samples.},
  eventtitle = {The 2006 {{IEEE International Joint Conference}} on {{Neural Network Proceedings}}},
  keywords = {Clustering methods,Constraint optimization,Eigenvalues and eigenfunctions,Graph theory,Kernel,Least squares approximation,Least squares methods,Principal component analysis,Testing,Unsupervised learning},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\DA2Y95MD\\Alzate 和 Suykens - 2006 - A Weighted Kernel PCA Formulation with Out-of-Samp.pdf;C\:\\Users\\wenji\\Zotero\\storage\\H84LQELZ\\1716082.html}
}

@article{alzateKernelComponentAnalysis2008,
  title = {Kernel {{Component Analysis Using}} an {{Epsilon-Insensitive Robust Loss Function}}},
  author = {Alzate, Carlos and Suykens, Johan A. K.},
  date = {2008-09},
  journaltitle = {IEEE Transactions on Neural Networks},
  volume = {19},
  number = {9},
  pages = {1583--1598},
  issn = {1941-0093},
  doi = {10.1109/TNN.2008.2000443},
  url = {https://ieeexplore.ieee.org/abstract/document/4570256?casa_token=ciGf3Shu7kwAAAAA:8b05HsAMlXUjEf0n_7ajZ2CffFLHIGLzoQ90lumzBMTXT3zYHHViSrpRGWmUxzfLHdKc7X_xyLw},
  urldate = {2024-04-05},
  abstract = {Kernel principal component analysis (PCA) is a technique to perform feature extraction in a high-dimensional feature space, which is nonlinearly related to the original input space. The kernel PCA formulation corresponds to an eigendecomposition of the kernel matrix: eigenvectors with large eigenvalues correspond to the principal components in the feature space. Starting from the least squares support vector machine (LS-SVM) formulation to kernel PCA, we extend it to a generalized form of kernel component analysis (KCA) with a general underlying loss function made explicit. For classical kernel PCA, the underlying loss function is L 2 . In this generalized form, one can plug in also other loss functions. In the context of robust statistics, it is known that the L 2 loss function is not robust because its influence function is not bounded. Therefore, outliers can skew the solution from the desired one. Another issue with kernel PCA is the lack of sparseness: the principal components are dense expansions in terms of kernel functions. In this paper, we introduce robustness and sparseness into kernel component analysis by using an epsilon-insensitive robust loss function. We propose two different algorithms. The first method solves a set of nonlinear equations with kernel PCA as starting points. The second method uses a simplified iterative weighting procedure that leads to solving a sequence of generalized eigenvalue problems. Simulations with toy and real-life data show improvements in terms of robustness together with a sparse representation.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}}},
  keywords = {Eigenvalues and eigenfunctions,Epsilon-insensitive loss function,Feature extraction,Iterative algorithms,Kernel,kernel principal component analysis (PCA),Least squares methods,least squares support vector machines (LS-SVM),loss function,Plugs,Principal component analysis,robustness,Robustness,sparseness,Statistics,Support vector machines},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\TRH2QGFB\\Alzate 和 Suykens - 2008 - Kernel Component Analysis Using an Epsilon-Insensi.pdf;C\:\\Users\\wenji\\Zotero\\storage\\8TZIV7WH\\4570256.html}
}

@article{suykensSupportVectorMachine2003,
  title = {A Support Vector Machine Formulation to {{PCA}} Analysis and Its Kernel Version},
  author = {Suykens, J.A.K. and Van Gestel, T. and Vandewalle, J. and De Moor, B.},
  date = {2003-03},
  journaltitle = {IEEE Transactions on Neural Networks},
  volume = {14},
  number = {2},
  pages = {447--450},
  issn = {1941-0093},
  doi = {10.1109/TNN.2003.809414},
  url = {https://ieeexplore.ieee.org/abstract/document/1189643?casa_token=L3qC_J_o23cAAAAA:VbGrr7cz2m1R8H4e6zku3FGq-lthAjauzBskxSaPKRBh_VppT-phdJ7ok2pg52jCHPXYMKAvAc0},
  urldate = {2024-04-05},
  abstract = {In this paper, we present a simple and straightforward primal-dual support vector machine formulation to the problem of principal component analysis (PCA) in dual variables. By considering a mapping to a high-dimensional feature space and application of the kernel trick (Mercer theorem), kernel PCA is obtained as introduced by Scholkopf et al. (2002). While least squares support vector machine classifiers have a natural link with the kernel Fisher discriminant analysis (minimizing the within class scatter around targets +1 and -1), for PCA analysis one can take the interpretation of a one-class modeling problem with zero target value around which one maximizes the variance. The score variables are interpreted as error variables within the problem formulation. In this way primal-dual constrained optimization problem interpretations to the linear and kernel PCA analysis are obtained in a similar style as for least square-support vector machine classifiers.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}}},
  keywords = {Analysis of variance,Constraint optimization,Kernel,Knowledge management,Least squares methods,Predictive models,Principal component analysis,Scattering,Support vector machine classification,Support vector machines},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\ZAJMIGZ7\\Suykens 等 - 2003 - A support vector machine formulation to PCA analys.pdf;C\:\\Users\\wenji\\Zotero\\storage\\MZTW5PYY\\1189643.html}
}

@inproceedings{alzateImageSegmentationUsing2007,
  title = {Image {{Segmentation}} Using a {{Weighted Kernel PCA Approach}} to {{Spectral Clustering}}},
  booktitle = {2007 {{IEEE Symposium}} on {{Computational Intelligence}} in {{Image}} and {{Signal Processing}}},
  author = {Alzate, Carlos and Suykens, Johan A. K.},
  date = {2007-04},
  pages = {208--213},
  doi = {10.1109/CIISP.2007.369319},
  url = {https://ieeexplore.ieee.org/abstract/document/4221420?casa_token=DgSBHPeVwYwAAAAA:Ll88cy4Gs-RR-g6xGizTD13CMOnN7n4fBYJ8usv92qFGLXLbH_gqGRnhuTkcxmFv6fRqpyAqcrg},
  urldate = {2024-04-05},
  abstract = {In classical graph-based image segmentation, a data-driven matrix is constructed representing similarities between every pair of pixels. The eigenvectors of such matrices contain relevant information about the clusters present on the image. An approach to image segmentation using spectral clustering with out-of-sample extensions is presented. This approach is based on the weighted kernel PCA framework. An advantage of the proposed method is the possibility to train and validate the clustering model on subsampled parts of the image to be segmented. The cluster indicators for the remaining pixels can then be inferred using the out-of-sample extension. This subsampling scheme can be used to reduce the computation time of the segmentation. Simulation results with grayscale and color images show improvements in terms of computation times together with visually appealing clusters},
  eventtitle = {2007 {{IEEE Symposium}} on {{Computational Intelligence}} in {{Image}} and {{Signal Processing}}},
  keywords = {Clustering algorithms,Image segmentation,Iterative algorithms,Kernel,Matrix converters,Matrix decomposition,Optimization methods,Partitioning algorithms,Pixel,Principal component analysis},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\E42WBHMU\\Alzate 和 Suykens - 2007 - Image Segmentation using a Weighted Kernel PCA App.pdf;C\:\\Users\\wenji\\Zotero\\storage\\I44QZLHU\\4221420.html}
}

@article{anandApproachClassificationHighly2010,
  title = {An Approach for Classification of Highly Imbalanced Data Using Weighting and Undersampling},
  author = {Anand, Ashish and Pugalenthi, Ganesan and Fogel, Gary B. and Suganthan, P. N.},
  date = {2010-11-01},
  journaltitle = {Amino Acids},
  shortjournal = {Amino Acids},
  volume = {39},
  number = {5},
  pages = {1385--1391},
  issn = {1438-2199},
  doi = {10.1007/s00726-010-0595-2},
  url = {https://doi.org/10.1007/s00726-010-0595-2},
  urldate = {2024-04-05},
  abstract = {Real-world datasets commonly have issues with data imbalance. There are several approaches such as weighting, sub-sampling, and data modeling for handling these data. Learning in the presence of data imbalances presents a great challenge to machine learning. Techniques such as support-vector machines have excellent performance for balanced data, but may fail when applied to imbalanced datasets. In this paper, we propose a new undersampling technique for selecting instances from the majority class. The performance of this approach was evaluated in the context of several real biological imbalanced data. The ratios of negative to positive samples vary from \textasciitilde 9:1 to \textasciitilde 100:1. Useful classifiers have high sensitivity and specificity. Our results demonstrate that the proposed selection technique improves the sensitivity compared to weighted support-vector machine and available results in the literature for the same datasets.},
  langid = {english},
  keywords = {Imbalanced datasets,SVM,Undersampling technique},
  file = {C:\Users\wenji\Zotero\storage\ICZ7MXZS\Anand 等 - 2010 - An approach for classification of highly imbalance.pdf}
}

@article{coussementKernelDensityWeighted2012,
  title = {Kernel Density Weighted Principal Component Analysis of Combustion Processes},
  author = {Coussement, Axel and Gicquel, Olivier and Parente, Alessandro},
  date = {2012-09-01},
  journaltitle = {Combustion and Flame},
  shortjournal = {Combustion and Flame},
  volume = {159},
  number = {9},
  pages = {2844--2855},
  issn = {0010-2180},
  doi = {10.1016/j.combustflame.2012.04.004},
  url = {https://www.sciencedirect.com/science/article/pii/S001021801200123X},
  urldate = {2024-04-05},
  abstract = {Principal component analysis (PCA) has been successfully applied to the analysis of combustion data-sets. However using PCA on a raw direct numerical simulation or an experimental data-set is not straightforward. Indeed, those data-sets usually show non-homogenous data density, hot and cold zones being generally over represented. This can introduce bias in the PCA reconstruction, especially when strong non-linear relationships characterize the data sample. To tackle this problem, a combination of the kernel density method and PCA is introduced here. This new PCA algorithm, called Temperature BAsed KErnel Density weighted PCA (T-BAKED PCA) allows to enhance the PCA accuracy especially in the flame front zone, which is the principal zone of interest. The performance of this new approach is benchmarked against classical PCA. Moreover, a new method called Hybrid T-BAKED PCA or HT-BAKED PCA, combining both classical and T-BAKED PCA, is proposed to provide an optimal representation of all flame regions.},
  keywords = {Combustion,Principal component analysis,Tabulated chemistry},
  file = {D\:\\Ddisk_backup\\desktop_backup\\stat note\\final thesis\\useful papers\\1-s2.0-S001021801200123X-main.pdf;C\:\\Users\\wenji\\Zotero\\storage\\LM5I2QX3\\S001021801200123X.html}
}

@article{schslkopfKernelPrincipalComponent,
  title = {Kernel Principal Component Analysis},
  author = {SchSlkopf, Bernhard and Mfiller, Klaus-Robert},
  abstract = {A new method for performing a nonlinear form of Principal Component Analysis is proposed. By the use of integral operator kernel functions, one can efficiently compute principal components in highdimensional feature spaces, related to input space by some nonlinear map; for instance the space of all possible d-pixel products in images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition.},
  langid = {english},
  file = {C:\Users\wenji\Zotero\storage\XKKL46HT\SchSlkopf 和 Mfiller - Kernel principal component analysis.pdf}
}

@inproceedings{nguyenRobustKernelPrincipal2008,
  title = {Robust {{Kernel Principal Component Analysis}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Nguyen, Minh and Torre, Fernando},
  date = {2008},
  volume = {21},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/8f53295a73878494e9bc8dd6c3c7104f-Abstract.html},
  urldate = {2024-04-08},
  abstract = {Kernel Principal Component Analysis (KPCA) is a popular generalization of linear PCA that allows non-linear feature extraction. In KPCA, data in the input space is mapped to higher (usually) dimensional feature space where the data can be linearly modeled. The feature space is typically induced implicitly by a kernel function, and linear PCA in the feature space is performed via the kernel trick. However, due to the implicitness of the feature space, some extensions of PCA such as robust PCA cannot be directly generalized to KPCA. This paper presents a technique to overcome this problem, and extends it to a unified framework for treating noise, missing data, and outliers in KPCA. Our method is based on a novel cost function to perform inference in KPCA. Extensive experiments, in both synthetic and real data, show that our algorithm outperforms existing methods.},
  file = {C:\Users\wenji\Zotero\storage\CDVEWR3A\Nguyen 和 Torre - 2008 - Robust Kernel Principal Component Analysis.pdf}
}

@article{delchambreWeightedPrincipalComponent2015,
  title = {Weighted Principal Component Analysis: A Weighted Covariance Eigendecomposition Approach},
  shorttitle = {Weighted Principal Component Analysis},
  author = {Delchambre, L.},
  date = {2015-02-01},
  journaltitle = {Monthly Notices of the Royal Astronomical Society},
  shortjournal = {Monthly Notices of the Royal Astronomical Society},
  volume = {446},
  number = {4},
  pages = {3545--3555},
  issn = {0035-8711},
  doi = {10.1093/mnras/stu2219},
  url = {https://doi.org/10.1093/mnras/stu2219},
  urldate = {2024-04-09},
  abstract = {We present a new straightforward principal component analysis (PCA) method based on the diagonalization of the weighted variance–covariance matrix through two spectral decomposition methods: power iteration and Rayleigh quotient iteration. This method allows one to retrieve a given number of orthogonal principal components amongst the most meaningful ones for the case of problems with weighted and/or missing data. Principal coefficients are then retrieved by fitting principal components to the data while providing the final decomposition. Tests performed on real and simulated cases show that our method is optimal in the identification of the most significant patterns within data sets. We illustrate the usefulness of this method by assessing its quality on the extrapolation of Sloan Digital Sky Survey quasar spectra from measured wavelengths to shorter and longer wavelengths. Our new algorithm also benefits from a fast and flexible implementation.},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\RUU85GQH\\Delchambre - 2015 - Weighted principal component analysis a weighted .pdf;C\:\\Users\\wenji\\Zotero\\storage\\6585AD4U\\2891891.html}
}

@inproceedings{rudiFastLeverageScore2018,
  title = {On {{Fast Leverage Score Sampling}} and {{Optimal Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Rudi, Alessandro and Calandriello, Daniele and Carratino, Luigi and Rosasco, Lorenzo},
  date = {2018},
  volume = {31},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2018/hash/56584778d5a8ab88d6393cc4cd11e090-Abstract.html},
  urldate = {2024-04-09},
  file = {C:\Users\wenji\Zotero\storage\NYLGBIU8\Rudi 等 - 2018 - On Fast Leverage Score Sampling and Optimal Learni.pdf}
}

@article{jangCategoricalReparameterizationGumbelSoftmax2016,
  title = {Categorical {{Reparameterization}} with {{Gumbel-Softmax}}},
  author = {Jang, Eric and Gu, Shixiang and Poole, Ben},
  date = {2016-11-03},
  abstract = {Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.},
  file = {C:\Users\wenji\Zotero\storage\ZK5KXUJ4\Jang 等 - 2016 - Categorical Reparameterization with Gumbel-Softmax.pdf}
}

@inproceedings{jangCategoricalReparameterizationGumbelSoftmax2022,
  title = {Categorical {{Reparameterization}} with {{Gumbel-Softmax}}},
  author = {Jang, Eric and Gu, Shixiang and Poole, Ben},
  date = {2022-07-21},
  url = {https://openreview.net/forum?id=rkE3y85ee},
  urldate = {2024-04-14},
  abstract = {Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {C:\Users\wenji\Zotero\storage\F9TWLVVJ\Jang 等 - 2022 - Categorical Reparameterization with Gumbel-Softmax.pdf}
}

@inproceedings{huangLearningDeepRepresentation2016,
  title = {Learning {{Deep Representation}} for {{Imbalanced Classification}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Huang, Chen and Li, Yining and Loy, Chen Change and Tang, Xiaoou},
  date = {2016-06},
  pages = {5375--5384},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2016.580},
  url = {https://ieeexplore.ieee.org/document/7780949},
  urldate = {2024-04-14},
  abstract = {Data in vision domain often exhibit highly-skewed class distribution, i.e., most data belong to a few majority classes, while the minority classes only contain a scarce amount of instances. To mitigate this issue, contemporary classification methods based on deep convolutional neural network (CNN) typically follow classic strategies such as class re-sampling or cost-sensitive training. In this paper, we conduct extensive and systematic experiments to validate the effectiveness of these classic schemes for representation learning on class-imbalanced data. We further demonstrate that more discriminative deep representation can be learned by enforcing a deep network to maintain both intercluster and inter-class margins. This tighter constraint effectively reduces the class imbalance inherent in the local data neighborhood. We show that the margins can be easily deployed in standard deep learning framework through quintuplet instance sampling and the associated triple-header hinge loss. The representation learned by our approach, when combined with a simple k-nearest neighbor (kNN) algorithm, shows significant improvements over existing methods on both high-and low-level vision classification tasks that exhibit imbalanced class distribution.},
  eventtitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  keywords = {Face,Fasteners,Feature extraction,Image edge detection,Machine learning,Neural networks,Training},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\KRQIEK7I\\Huang 等 - 2016 - Learning Deep Representation for Imbalanced Classi.pdf;C\:\\Users\\wenji\\Zotero\\storage\\PP685SFX\\7780949.html}
}

@inproceedings{seiffertComparativeStudyData2008,
  title = {A {{Comparative Study}} of {{Data Sampling}} and {{Cost Sensitive Learning}}},
  booktitle = {2008 {{IEEE International Conference}} on {{Data Mining Workshops}}},
  author = {Seiffert, Chris and Khoshgoftaar, Taghi M. and Hulse, Jason Van and Napolitano, Amri},
  date = {2008-12},
  pages = {46--52},
  issn = {2375-9259},
  doi = {10.1109/ICDMW.2008.119},
  url = {https://ieeexplore.ieee.org/abstract/document/4733920?casa_token=0g0VZkYSxg4AAAAA:wKiekYnr-6FLuUhO3ZmbJFoyNjx64Iv5nmJqnlN2R2bHDfR9NfBk0Lqd4SYywa_VV1-9A9h1BDs},
  urldate = {2024-04-14},
  abstract = {Two common challenges data mining and machine learning practitioners face in many application domains are unequal classification costs and class imbalance. Most traditional data mining techniques attempt to maximize overall accuracy rather than minimize cost. When data is imbalanced, such techniques result in models that highly favor the over represented class, the class which typically carries a lower cost of misclassification. Two techniques that have been used to address both of these issues are cost sensitive learning and data sampling. In this work, we investigate the performance of two cost sensitive learning techniques and four data sampling techniques for minimizing classification costs when data is imbalanced. We present a comprehensive suite of experiments, utilizing 15 datasets with 10 cost ratios, which have been carefully designed to ensure conclusive, significant and reliable results.},
  eventtitle = {2008 {{IEEE International Conference}} on {{Data Mining Workshops}}},
  keywords = {class imbalance,Conferences,Cost function,cost sensitive learning,Data mining,data sampling,Machine learning,Machine learning algorithms,Sampling methods,Stability,Statistical analysis,Training data,USA Councils},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\PDVUGYA3\\Seiffert 等 - 2008 - A Comparative Study of Data Sampling and Cost Sens.pdf;C\:\\Users\\wenji\\Zotero\\storage\\ZZ24LKEH\\4733920.html}
}

@inproceedings{dongClassRectificationHard2017,
  title = {Class {{Rectification Hard Mining}} for {{Imbalanced Deep Learning}}},
  author = {Dong, Qi and Gong, Shaogang and Zhu, Xiatian},
  date = {2017-10-01},
  pages = {1869--1878},
  publisher = {IEEE Computer Society},
  issn = {2380-7504},
  doi = {10.1109/ICCV.2017.205},
  url = {https://www.computer.org/csdl/proceedings-article/iccv/2017/1032b869/12OmNBSSVj7},
  urldate = {2024-04-14},
  abstract = {Recognising detailed facial or clothing attributes in images of people is a challenging task for computer vision, especially when the training data are both in very large scale and extremely imbalanced among different attribute classes. To address this problem, we formulate a novel scheme for batch incremental hard sample mining of minority attribute classes from imbalanced large scale training data. We develop an end-to-end deep learning framework capable of avoiding the dominant effect of majority classes by discovering sparsely sampled boundaries of minority classes. This is made possible by introducing a Class Rectification Loss (CRL) regularising algorithm. We demonstrate the advantages and scalability of CRL over existing state-of-the-art attribute recognition and imbalanced data learning models on two large scale imbalanced benchmark datasets, the CelebA facial attribute dataset and the X-Domain clothing attribute dataset.},
  eventtitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-5386-1032-9},
  langid = {english},
  file = {C:\Users\wenji\Zotero\storage\RRGE6XZZ\Dong 等 - 2017 - Class Rectification Hard Mining for Imbalanced Dee.pdf}
}

@inproceedings{kimDisentanglingFactorising2018,
  title = {Disentangling by {{Factorising}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Kim, Hyunjik and Mnih, Andriy},
  date = {2018-07-03},
  pages = {2649--2658},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v80/kim18b.html},
  urldate = {2024-04-15},
  abstract = {We define and address the problem of unsupervised learning of disentangled representations on data generated from independent factors of variation. We propose FactorVAE, a method that disentangles by encouraging the distribution of representations to be factorial and hence independent across the dimensions. We show that it improves upon beta-VAE by providing a better trade-off between disentanglement and reconstruction quality and being more robust to the number of training iterations. Moreover, we highlight the problems of a commonly used disentanglement metric and introduce a new metric that does not suffer from them.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\AWWU6EVV\\Kim 和 Mnih - 2018 - Disentangling by Factorising.pdf;C\:\\Users\\wenji\\Zotero\\storage\\LLQKFXW6\\Kim 和 Mnih - 2018 - Disentangling by Factorising.pdf}
}

@inproceedings{groverBiasCorrectionLearned2019,
  title = {Bias {{Correction}} of {{Learned Generative Models}} Using {{Likelihood-Free Importance Weighting}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Grover, Aditya and Song, Jiaming and Kapoor, Ashish and Tran, Kenneth and Agarwal, Alekh and Horvitz, Eric J and Ermon, Stefano},
  date = {2019},
  volume = {32},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2019/hash/d76d8deea9c19cc9aaf2237d2bf2f785-Abstract.html},
  urldate = {2024-04-17},
  abstract = {A learned generative model often produces biased statistics relative to the underlying data distribution. A standard technique to correct this bias is importance sampling, where samples from the model are weighted  by the likelihood ratio under model and true distributions. When the likelihood ratio is unknown, it can be estimated by training a probabilistic classifier to distinguish samples from the two distributions. We employ this likelihood-free importance weighting method to correct for the bias in generative models. We find that this technique consistently improves standard goodness-of-fit metrics for evaluating the sample quality of state-of-the-art deep generative models, suggesting reduced bias. Finally, we demonstrate its utility on representative applications in a) data augmentation for classification using generative adversarial networks, and b) model-based policy evaluation using off-policy data.},
  file = {C:\Users\wenji\Zotero\storage\PFA5H2I5\Grover 等 - 2019 - Bias Correction of Learned Generative Models using.pdf}
}

@inproceedings{byrdWhatEffectImportance2019,
  title = {What Is the {{Effect}} of {{Importance Weighting}} in {{Deep Learning}}?},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Byrd, Jonathon and Lipton, Zachary},
  date = {2019-05-24},
  pages = {872--881},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v97/byrd19a.html},
  urldate = {2024-04-17},
  abstract = {Importance-weighted risk minimization is a key ingredient in many machine learning algorithms for causal inference, domain adaptation, class imbalance, and off-policy reinforcement learning. While the effect of importance weighting is well-characterized for low-capacity misspecified models, little is known about how it impacts over-parameterized, deep neural networks. This work is inspired by recent theoretical results showing that on (linearly) separable data, deep linear networks optimized by SGD learn weight-agnostic solutions, prompting us to ask, for realistic deep networks, for which many practical datasets are separable, what is the effect of importance weighting? We present the surprising finding that while importance weighting impacts models early in training, its effect diminishes over successive epochs. Moreover, while L2 regularization and batch normalization (but not dropout), restore some of the impact of importance weighting, they express the effect via (seemingly) the wrong abstraction: why should practitioners tweak the L2 regularization, and by how much, to produce the correct weighting effect? Our experiments confirm these findings across a range of architectures and datasets.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\9AUYHUZI\\Byrd 和 Lipton - 2019 - What is the Effect of Importance Weighting in Deep.pdf;C\:\\Users\\wenji\\Zotero\\storage\\AZN8IGT9\\Byrd 和 Lipton - 2019 - What is the Effect of Importance Weighting in Deep.pdf}
}

@article{demange-chrystVariationalAutoencoderWeighted2023,
  title = {Variational Autoencoder with Weighted Samples for High-Dimensional Non-Parametric Adaptive Importance Sampling},
  author = {Demange-Chryst, Julien and Bachoc, Francois and Morio, Jérôme and Krauth, Timothé},
  date = {2023-10-20},
  journaltitle = {Transactions on Machine Learning Research},
  issn = {2835-8856},
  url = {https://openreview.net/forum?id=nzG9KGssSe},
  urldate = {2024-04-17},
  abstract = {Adaptive importance sampling is a well-known family of algorithms for density approximation, generation and Monte Carlo integration including rare event estimation. The main common denominator of this family of algorithms is to perform density estimation with weighted samples at each iteration. However, the classical existing methods to do so, such as kernel smoothing or approximation by a Gaussian distribution, suffer from the curse of dimensionality and/or a lack of flexibility. Both are limitations in high dimension and when we do not have any prior knowledge on the form of the target distribution, such as its number of modes. Variational autoencoders are probabilistic tools able to represent with fidelity high-dimensional data in a lower dimensional space. They constitute a parametric family of distributions robust faced to the dimension and since they are based on deep neural networks, they are flexible enough to be considered as non-parametric models. In this paper, we propose to use a variational autoencoder as the auxiliary importance sampling distribution by extending the existing framework to weighted samples. We integrate the proposed procedure in existing adaptive importance sampling algorithms and we illustrate its practical interest on diverse examples.},
  langid = {english},
  file = {C:\Users\wenji\Zotero\storage\WKLG2L3R\Demange-Chryst 等 - 2023 - Variational autoencoder with weighted samples for .pdf}
}

@online{burdaImportanceWeightedAutoencoders2016,
  title = {Importance {{Weighted Autoencoders}}},
  author = {Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan},
  date = {2016-11-07},
  eprint = {1509.00519},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1509.00519},
  url = {http://arxiv.org/abs/1509.00519},
  urldate = {2024-04-17},
  abstract = {The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\7JVJLX63\\Burda 等 - 2016 - Importance Weighted Autoencoders.pdf;C\:\\Users\\wenji\\Zotero\\storage\\AZIQSRXM\\1509.html}
}

@article{grettonKernelTwoSampleTest2012,
  title = {A {{Kernel Two-Sample Test}}},
  author = {Gretton, Arthur and Borgwardt, Karsten M. and Rasch, Malte J. and Schölkopf, Bernhard and Smola, Alexander},
  date = {2012},
  journaltitle = {Journal of Machine Learning Research},
  volume = {13},
  number = {25},
  pages = {723--773},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v13/gretton12a.html},
  urldate = {2024-04-18},
  abstract = {We propose a framework for analyzing and comparing distributions, which we use to construct statistical tests to determine if two samples are drawn from different distributions. Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS), and is called the maximum mean discrepancy (MMD). We present two distribution-free tests based on large deviation bounds for the MMD, and a third test based on the asymptotic distribution of this statistic. The MMD can be computed in quadratic time, although efficient linear time approximations are available. Our statistic is an instance of an integral probability metric, and various classical metrics on distributions are obtained when alternative function classes are used in place of an RKHS. We apply our two-sample tests to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly. Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests.},
  file = {C:\Users\wenji\Zotero\storage\VWKSQFB5\Gretton 等 - 2012 - A Kernel Two-Sample Test.pdf}
}

@article{farajianMinorityManifoldRegularization2021,
  title = {Minority Manifold Regularization by Stacked Auto-Encoder for Imbalanced Learning},
  author = {Farajian, Nima and Adibi, Peyman},
  date = {2021-05-01},
  journaltitle = {Expert Systems with Applications},
  shortjournal = {Expert Systems with Applications},
  volume = {169},
  pages = {114317},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2020.114317},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417420310125},
  urldate = {2024-04-19},
  abstract = {Imbalanced learning is considered one of the challenging problems in machine learning. This problem arises when a learning algorithm is biased toward the majority class due to the large proportion of the majority class data while detecting the minority class is of greater importance. In the present study, a novel method (MMRAE) is presented for imbalanced learning encompassing feature learning and classification steps. In the feature learning step, meaningful features are extracted from the minority data and their underlying manifold are captured by taking advantage of one-class learning approach through stacking two regularized auto-encoders. The existence of novel and different regularizers in each auto-encoder leads to a new representation with proper data discrimination which improves the between-class and within-class imbalanced problems. Then, in the classification step, the classification between the minority and majority class is performed by constructing a multilayer neural network using features learned throughout pre-training. The proposed method is extensively studied on six artificial and twenty real datasets in order to have a precise evaluation. Based on different criteria such as F-measure, G-mean, and AUC, the results represent considerable performance of the proposed method compared to several other existing methods.},
  keywords = {Feature learning,Imbalanced data classification,Regularized auto-encoder},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\IBFE2Y42\\Farajian 和 Adibi - 2021 - Minority manifold regularization by stacked auto-e.pdf;C\:\\Users\\wenji\\Zotero\\storage\\TLFQMEFS\\S0957417420310125.html}
}

@inproceedings{yanMindClassWeight2017,
  title = {Mind the {{Class Weight Bias}}: {{Weighted Maximum Mean Discrepancy}} for {{Unsupervised Domain Adaptation}}},
  shorttitle = {Mind the {{Class Weight Bias}}},
  author = {Yan, Hongliang and Ding, Yukang and Li, Peihua and Wang, Qilong and Xu, Yong and Zuo, Wangmeng},
  date = {2017-07-01},
  pages = {945--954},
  publisher = {IEEE Computer Society},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2017.107},
  url = {https://www.computer.org/csdl/proceedings-article/cvpr/2017/0457a945/12OmNynJMVa},
  urldate = {2024-04-23},
  abstract = {In domain adaptation, maximum mean discrepancy (MMD) has been widely adopted as a discrepancy metric between the distributions of source and target domains. However, existing MMD-based domain adaptation methods generally ignore the changes of class prior distributions, i.e., class weight bias across domains. This remains an open problem but ubiquitous for domain adaptation, which can be caused by changes in sample selection criteria and application scenarios. We show that MMD cannot account for class weight bias and results in degraded domain adaptation performance. To address this issue, a weighted MMD model is proposed in this paper. Specifically, we introduce class-specific auxiliary weights into the original MMD for exploiting the class prior probability on source and target domains, whose challenge lies in the fact that the class label in target domain is unavailable. To account for it, our proposed weighted MMD model is defined by introducing an auxiliary weight for each class in the source domain, and a classification EM algorithm is suggested by alternating between assigning the pseudo-labels, estimating auxiliary weights and updating model parameters. Extensive experiments demonstrate the superiority of our weighted MMD over conventional MMD for domain adaptation.},
  eventtitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-5386-0457-1},
  langid = {english},
  file = {C:\Users\wenji\Zotero\storage\6GHU3IPF\Yan 等 - 2017 - Mind the Class Weight Bias Weighted Maximum Mean .pdf}
}

@inproceedings{hsuUnsupervisedDomainAdaptation2015,
  title = {Unsupervised {{Domain Adaptation With Imbalanced Cross-Domain Data}}},
  author = {Hsu, Tzu Ming Harry and Chen, Wei Yu and Hou, Cheng-An and Tsai, Yao-Hung Hubert and Yeh, Yi-Ren and Wang, Yu-Chiang Frank},
  date = {2015},
  pages = {4121--4129},
  url = {https://openaccess.thecvf.com/content_iccv_2015/html/Hsu_Unsupervised_Domain_Adaptation_ICCV_2015_paper.html},
  urldate = {2024-04-23},
  eventtitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}}},
  file = {C:\Users\wenji\Zotero\storage\3HHD9A3M\Hsu 等 - 2015 - Unsupervised Domain Adaptation With Imbalanced Cro.pdf}
}

@article{zhaoInfoVAEBalancingLearning2019,
  title = {{{InfoVAE}}: {{Balancing Learning}} and {{Inference}} in {{Variational Autoencoders}}},
  shorttitle = {{{InfoVAE}}},
  author = {Zhao, Shengjia and Song, Jiaming and Ermon, Stefano},
  date = {2019-07-17},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {33},
  number = {01},
  pages = {5885--5892},
  issn = {2374-3468},
  doi = {10.1609/aaai.v33i01.33015885},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/4538},
  urldate = {2024-04-23},
  abstract = {A key advance in learning generative models is the use of amortized inference distributions that are jointly trained with the models. We find that existing training objectives for variational autoencoders can lead to inaccurate amortized inference distributions and, in some cases, improving the objective provably degrades the inference quality. In addition, it has been observed that variational autoencoders tend to ignore the latent variables when combined with a decoding distribution that is too flexible. We again identify the cause in existing training criteria and propose a new class of objectives (Info-VAE) that mitigate these problems. We show that our model can significantly improve the quality of the variational posterior and can make effective use of the latent features regardless of the flexibility of the decoding distribution. Through extensive qualitative and quantitative analyses, we demonstrate that our models outperform competing approaches on multiple performance metrics},
  issue = {01},
  langid = {english},
  file = {C:\Users\wenji\Zotero\storage\XLR7EWFZ\Zhao 等 - 2019 - InfoVAE Balancing Learning and Inference in Varia.pdf}
}

@online{zhaoInfoVAEInformationMaximizing2018,
  title = {{{InfoVAE}}: {{Information Maximizing Variational Autoencoders}}},
  shorttitle = {{{InfoVAE}}},
  author = {Zhao, Shengjia and Song, Jiaming and Ermon, Stefano},
  date = {2018-05-30},
  eprint = {1706.02262},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1706.02262},
  url = {http://arxiv.org/abs/1706.02262},
  urldate = {2024-04-23},
  abstract = {A key advance in learning generative models is the use of amortized inference distributions that are jointly trained with the models. We find that existing training objectives for variational autoencoders can lead to inaccurate amortized inference distributions and, in some cases, improving the objective provably degrades the inference quality. In addition, it has been observed that variational autoencoders tend to ignore the latent variables when combined with a decoding distribution that is too flexible. We again identify the cause in existing training criteria and propose a new class of objectives (InfoVAE) that mitigate these problems. We show that our model can significantly improve the quality of the variational posterior and can make effective use of the latent features regardless of the flexibility of the decoding distribution. Through extensive qualitative and quantitative analyses, we demonstrate that our models outperform competing approaches on multiple performance metrics.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\KWUHT57Y\\Zhao 等 - 2018 - InfoVAE Information Maximizing Variational Autoen.pdf;C\:\\Users\\wenji\\Zotero\\storage\\89UDH2LP\\1706.html}
}

@article{phamPCAAEPrincipalComponent2022,
  title = {{{PCA-AE}}: {{Principal Component Analysis Autoencoder}} for {{Organising}} the {{Latent Space}} of {{Generative Networks}}},
  shorttitle = {{{PCA-AE}}},
  author = {Pham, Chi-Hieu and Ladjal, Saïd and Newson, Alasdair},
  date = {2022-06-01},
  journaltitle = {Journal of Mathematical Imaging and Vision},
  shortjournal = {J Math Imaging Vis},
  volume = {64},
  number = {5},
  pages = {569--585},
  issn = {1573-7683},
  doi = {10.1007/s10851-022-01077-z},
  url = {https://doi.org/10.1007/s10851-022-01077-z},
  urldate = {2024-04-28},
  abstract = {Autoencoders and generative models produce some of the most spectacular deep learning results to date. However, understanding and controlling the latent space of these models presents a considerable challenge. Drawing inspiration from principal component analysis and autoencoders, we propose the principal component analysis autoencoder (PCA-AE). This is a novel autoencoder whose latent space verifies two properties. Firstly, the dimensions are organised in decreasing importance with respect to the data at hand. Secondly, the components of the latent space are statistically independent. We achieve this by progressively increasing the latent space during training, and with a covariance loss applied to the latent codes. The resulting autoencoder produces a latent space which separates the intrinsic attributes of the data into different components of the latent space, in a completely unsupervised manner. We also describe an extension of our approach to the case of powerful, pre-trained GANs. We show results on both synthetic examples of shapes and on a state-of-the-art GAN. For example, we are able to separate the colour shade scale of hair, pose of faces and gender, without accessing any labels. We compare the PCA-AE with other state-of-the-art approaches, in particular with respect to the ability to disentangle attributes in the latent space. We hope that this approach will contribute to better understanding of the intrinsic latent spaces of powerful deep generative models.},
  langid = {english},
  keywords = {Autoencoders,Generative networks,Image generation},
  file = {C:\Users\wenji\Zotero\storage\YM2PY9EJ\Pham 等 - 2022 - PCA-AE Principal Component Analysis Autoencoder f.pdf}
}

@online{marianiBAGANDataAugmentation2018,
  title = {{{BAGAN}}: {{Data Augmentation}} with {{Balancing GAN}}},
  shorttitle = {{{BAGAN}}},
  author = {Mariani, Giovanni and Scheidegger, Florian and Istrate, Roxana and Bekas, Costas and Malossi, Cristiano},
  date = {2018-06-05},
  eprint = {1803.09655},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1803.09655},
  url = {http://arxiv.org/abs/1803.09655},
  urldate = {2024-04-30},
  abstract = {Image classification datasets are often imbalanced, characteristic that negatively affects the accuracy of deep-learning classifiers. In this work we propose balancing GAN (BAGAN) as an augmentation tool to restore balance in imbalanced datasets. This is challenging because the few minority-class images may not be enough to train a GAN. We overcome this issue by including during the adversarial training all available images of majority and minority classes. The generative model learns useful features from majority classes and uses these to generate images for minority classes. We apply class conditioning in the latent space to drive the generation process towards a target class. The generator in the GAN is initialized with the encoder module of an autoencoder that enables us to learn an accurate class-conditioning in the latent space. We compare the proposed methodology with state-of-the-art GANs and demonstrate that BAGAN generates images of superior quality when trained with an imbalanced dataset.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\KJSVFYDP\\Mariani 等 - 2018 - BAGAN Data Augmentation with Balancing GAN.pdf;C\:\\Users\\wenji\\Zotero\\storage\\Z4WXFR44\\1803.html}
}

@online{marianiBAGANDataAugmentation2018a,
  title = {{{BAGAN}}: {{Data Augmentation}} with {{Balancing GAN}}},
  shorttitle = {{{BAGAN}}},
  author = {Mariani, Giovanni and Scheidegger, Florian and Istrate, Roxana and Bekas, Costas and Malossi, Cristiano},
  date = {2018-06-05},
  eprint = {1803.09655},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1803.09655},
  url = {http://arxiv.org/abs/1803.09655},
  urldate = {2024-04-30},
  abstract = {Image classification datasets are often imbalanced, characteristic that negatively affects the accuracy of deep-learning classifiers. In this work we propose balancing GAN (BAGAN) as an augmentation tool to restore balance in imbalanced datasets. This is challenging because the few minority-class images may not be enough to train a GAN. We overcome this issue by including during the adversarial training all available images of majority and minority classes. The generative model learns useful features from majority classes and uses these to generate images for minority classes. We apply class conditioning in the latent space to drive the generation process towards a target class. The generator in the GAN is initialized with the encoder module of an autoencoder that enables us to learn an accurate class-conditioning in the latent space. We compare the proposed methodology with state-of-the-art GANs and demonstrate that BAGAN generates images of superior quality when trained with an imbalanced dataset.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\8VDMZNWL\\Mariani 等 - 2018 - BAGAN Data Augmentation with Balancing GAN.pdf;C\:\\Users\\wenji\\Zotero\\storage\\BBXVQV6M\\1803.html}
}

@inproceedings{gurumurthyDeLiGANGenerativeAdversarial2017,
  title = {{{DeLiGAN}} : {{Generative Adversarial Networks}} for {{Diverse}} and {{Limited Data}}},
  shorttitle = {{{DeLiGAN}}},
  author = {Gurumurthy, Swaminathan and Kiran Sarvadevabhatla, Ravi and Venkatesh Babu, R.},
  date = {2017},
  pages = {166--174},
  url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper.html},
  urldate = {2024-04-30},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {C:\Users\wenji\Zotero\storage\CAJJL6EJ\Gurumurthy 等 - 2017 - DeLiGAN  Generative Adversarial Networks for Dive.pdf}
}

@inproceedings{odenaConditionalImageSynthesis2017,
  title = {Conditional {{Image Synthesis}} with {{Auxiliary Classifier GANs}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Odena, Augustus and Olah, Christopher and Shlens, Jonathon},
  date = {2017-07-17},
  pages = {2642--2651},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v70/odena17a.html},
  urldate = {2024-04-30},
  abstract = {In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128×128128×128128\textbackslash times 128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128×128128×128128\textbackslash times 128 samples are more than twice as discriminable as artificially resized 32×3232×3232\textbackslash times 32 samples. In addition, 84.7\textbackslash\% of the classes have samples exhibiting diversity comparable to real ImageNet data.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\4PHX3MEC\\Odena 等 - 2017 - Conditional Image Synthesis with Auxiliary Classif.pdf;C\:\\Users\\wenji\\Zotero\\storage\\SAL9T9RR\\Odena 等 - 2017 - Conditional Image Synthesis with Auxiliary Classif.pdf}
}

@inproceedings{sohnLearningStructuredOutput2015,
  title = {Learning {{Structured Output Representation}} Using {{Deep Conditional Generative Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Sohn, Kihyuk and Lee, Honglak and Yan, Xinchen},
  date = {2015},
  volume = {28},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper_files/paper/2015/hash/8d55a249e6baa5c06772297520da2051-Abstract.html},
  urldate = {2024-04-30},
  abstract = {Supervised deep learning has been successfully applied for many recognition problems in machine learning and computer vision. Although it can approximate a complex many-to-one function very well when large number of training data is provided, the lack of probabilistic inference of the current supervised deep learning methods makes it difficult to model a complex structured output representations. In this work, we develop a scalable deep conditional generative model for structured output variables using Gaussian latent variables. The model is trained efficiently in the framework of stochastic gradient variational Bayes, and allows a fast prediction using stochastic feed-forward inference. In addition, we provide novel strategies to build a robust structured prediction algorithms, such as recurrent prediction network architecture, input noise-injection and multi-scale prediction training methods. In experiments, we demonstrate the effectiveness of our proposed algorithm in comparison to the deterministic deep neural network counterparts in generating diverse but realistic output representations using stochastic inference. Furthermore, the proposed schemes in training methods and architecture design were complimentary, which leads to achieve strong pixel-level object segmentation and semantic labeling performance on Caltech-UCSD Birds 200 and the subset of Labeled Faces in the Wild dataset.},
  file = {C:\Users\wenji\Zotero\storage\3QXQ6AK8\Sohn 等 - 2015 - Learning Structured Output Representation using De.pdf}
}

@article{huangEnhancedBalancingGAN2023,
  title = {Enhanced Balancing {{GAN}}: Minority-Class Image Generation},
  shorttitle = {Enhanced Balancing {{GAN}}},
  author = {Huang, Gaofeng and Jafari, Amir Hossein},
  date = {2023-03-01},
  journaltitle = {Neural Computing and Applications},
  shortjournal = {Neural Comput \& Applic},
  volume = {35},
  number = {7},
  pages = {5145--5154},
  issn = {1433-3058},
  doi = {10.1007/s00521-021-06163-8},
  url = {https://doi.org/10.1007/s00521-021-06163-8},
  urldate = {2024-05-01},
  abstract = {Generative adversarial networks (GANs) are one of the most powerful generative models, but always require a large and balanced dataset to train. Traditional GANs are not applicable to generate minority-class images in a highly imbalanced dataset. Balancing GAN (BAGAN) is proposed to mitigate this problem, but it is unstable when images in different classes look similar, e.g., flowers and cells. In this work, we propose a supervised autoencoder with an intermediate embedding model to disperse the labeled latent vectors. With the enhanced autoencoder initialization, we also build an architecture of BAGAN with gradient penalty (BAGAN-GP). Our proposed model overcomes the unstable issue in original BAGAN and converges faster to high-quality generations. Our model achieves high performance on the imbalanced scale-down version of MNIST Fashion, CIFAR-10, and one small-scale medical image dataset. https://github.com/GH920/improved-bagan-gp.},
  langid = {english},
  keywords = {Data augmentation,GAN,Image generation,Imbalanced data,Medical image},
  file = {C:\Users\wenji\Zotero\storage\AW7EE3E5\Huang 和 Jafari - 2023 - Enhanced balancing GAN minority-class image gener.pdf}
}

@inproceedings{liuFaceRecognitionWeighted2006,
  title = {Face {{Recognition}} with {{Weighted Kernel Principal Component Analysis}}},
  booktitle = {Robotics and {{Vision}} 2006 9th {{International Conference}} on {{Control}}, {{Automation}}},
  author = {Liu, Nan and Wang, Han and Yau, Wei-Yun},
  date = {2006-12},
  pages = {1--5},
  doi = {10.1109/ICARCV.2006.345161},
  url = {https://ieeexplore.ieee.org/abstract/document/4150071?casa_token=PV_8JwYdcpUAAAAA:d8vwOZyxd8slVNnw4YJH6ctQe8uoWqU-pecixnm4Qr2POrWZ0HwtZEeIfSY0dG1iVc4JbSkq7wA},
  urldate = {2024-05-02},
  abstract = {Principal component analysis (PCA) is one of the most traditional linear dimensionality reduction algorithms. Kernel principal component analysis (kernel PCA), generalization of PCA, is a nonlinear feature extraction method. However, both PCA and kernel PCA are lack of class information in their feature subspace. In this paper, weighted kernel principal component analysis (WKPCA) is proposed for feature extraction with the application of face recognition. Weights that represent inter-class relationships are incorporated into kernel matrix. Images in training and testing set are projected onto the subspace obtained from weighted kernel matrix. The feature extraction procedure is in a framework of genetic algorithms (GAs) with the fitness as classification accuracy on cross-validation data from training set. The experimental results of WKPCA are compared with PCA and kernel PCA on a combo database (ORL, Yale, UMIST databases), and show that proposed WKPCA algorithm performs best in face recognition},
  eventtitle = {Robotics and {{Vision}} 2006 9th {{International Conference}} on {{Control}}, {{Automation}}},
  keywords = {Algorithm design and analysis,Covariance matrix,Eigenvalues and eigenfunctions,face recognition,Face recognition,Feature extraction,genetic algorithms,Genetic algorithms,Kernel,Light scattering,Principal component analysis,Spatial databases,weighted kernel principal component analysis},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\42BJ65LE\\Liu 等 - 2006 - Face Recognition with Weighted Kernel Principal Co.pdf;C\:\\Users\\wenji\\Zotero\\storage\\33BSSFVF\\4150071.html}
}

@article{alzateKernelComponentAnalysis2008a,
  title = {Kernel {{Component Analysis Using}} an {{Epsilon-Insensitive Robust Loss Function}}},
  author = {Alzate, Carlos and Suykens, Johan A. K.},
  date = {2008-09},
  journaltitle = {IEEE Transactions on Neural Networks},
  volume = {19},
  number = {9},
  pages = {1583--1598},
  issn = {1941-0093},
  doi = {10.1109/TNN.2008.2000443},
  url = {https://ieeexplore.ieee.org/abstract/document/4570256?casa_token=q2DSJoZXC5wAAAAA:Wv5ZODulcbsC3X_BGn844p4TFjov8mMeF8O-O4Xa68S_YcIRYwfEcs6hBWHvh46TFN9Ykpcf86Y},
  urldate = {2024-05-02},
  abstract = {Kernel principal component analysis (PCA) is a technique to perform feature extraction in a high-dimensional feature space, which is nonlinearly related to the original input space. The kernel PCA formulation corresponds to an eigendecomposition of the kernel matrix: eigenvectors with large eigenvalues correspond to the principal components in the feature space. Starting from the least squares support vector machine (LS-SVM) formulation to kernel PCA, we extend it to a generalized form of kernel component analysis (KCA) with a general underlying loss function made explicit. For classical kernel PCA, the underlying loss function is L 2 . In this generalized form, one can plug in also other loss functions. In the context of robust statistics, it is known that the L 2 loss function is not robust because its influence function is not bounded. Therefore, outliers can skew the solution from the desired one. Another issue with kernel PCA is the lack of sparseness: the principal components are dense expansions in terms of kernel functions. In this paper, we introduce robustness and sparseness into kernel component analysis by using an epsilon-insensitive robust loss function. We propose two different algorithms. The first method solves a set of nonlinear equations with kernel PCA as starting points. The second method uses a simplified iterative weighting procedure that leads to solving a sequence of generalized eigenvalue problems. Simulations with toy and real-life data show improvements in terms of robustness together with a sparse representation.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}}},
  keywords = {Eigenvalues and eigenfunctions,Epsilon-insensitive loss function,Feature extraction,Iterative algorithms,Kernel,kernel principal component analysis (PCA),Least squares methods,least squares support vector machines (LS-SVM),loss function,Plugs,Principal component analysis,robustness,Robustness,sparseness,Statistics,Support vector machines},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\MXLC26YE\\Alzate 和 Suykens - 2008 - Kernel Component Analysis Using an Epsilon-Insensi.pdf;C\:\\Users\\wenji\\Zotero\\storage\\RQLS3LUS\\4570256.html}
}

@article{pandeyRecurrentRestrictedKernel2022,
  title = {Recurrent Restricted Kernel Machines for Time-Series Forecasting},
  author = {Pandey, Arun and De Meulemeester, Hannes and De Plaen, Henri and De Moor, Bart and Suykens, Johan},
  date = {2022},
  journaltitle = {Proceedings of ESANN 2022},
  shortjournal = {Proceedings of ESANN 2022},
  publisher = {i6doc},
  file = {C:\Users\wenji\Zotero\storage\J8GP7QKY\Pandey 等 - 2022 - Recurrent restricted kernel machines for time-seri.pdf}
}

@inproceedings{johnsonDeepLearningData2019,
  title = {Deep {{Learning}} and {{Data Sampling}} with {{Imbalanced Big Data}}},
  booktitle = {2019 {{IEEE}} 20th {{International Conference}} on {{Information Reuse}} and {{Integration}} for {{Data Science}} ({{IRI}})},
  author = {Johnson, Justin M. and Khoshgoftaar, Taghi M.},
  date = {2019-07},
  pages = {175--183},
  doi = {10.1109/IRI.2019.00038},
  url = {https://ieeexplore.ieee.org/document/8843477},
  urldate = {2024-05-06},
  abstract = {This study evaluates the use of deep learning and data sampling on a class-imbalanced Big Data problem, i.e. Medicare fraud detection. Medicare offers affordable health insurance to the elderly population and serves more than 15\% of the United States population. To increase transparency and help reduce fraud, the Centers for Medicare and Medicaid Services (CMS) have made several data sets publicly available for analysis. Our research group has conducted several studies using CMS data and traditional machine learning algorithms (non-deep learning), but challenges associated with severe class imbalance leave room for improvement. These previous studies serve as baselines as we employ deep neural networks with various data-sampling techniques to determine the efficacy of deep learning in addressing class imbalance. Random over-sampling (ROS), random under-sampling (RUS), and combinations of the two (ROS-RUS) are applied to study how varying levels of class imbalance impact model training and performance. Classwise performance is maximized by identifying optimal decision thresholds, and a strong linear relationship between minority class size and optimal threshold is observed. Results show that ROS significantly outperforms RUS, combining RUS and ROS both maximizes performance and efficiency with a 4× speedup in training time, and the default threshold of 0.5 is never optimal when training data is imbalanced. To the best of our knowledge, this is the first study to provide statistical results comparing ROS, RUS, and ROS-RUS deep learning methods across a range of class distributions. Additional contributions include a unique analysis of thresholding as it relates to the minority class size and state-of-the-art performance on the given fraud detection task.},
  eventtitle = {2019 {{IEEE}} 20th {{International Conference}} on {{Information Reuse}} and {{Integration}} for {{Data Science}} ({{IRI}})},
  keywords = {Artificial Neural Networks,Big Data,Biomedical imaging,Class Imbalance,Data models,Data Sampling,Deep learning,Deep Learning,Fraud Detection,Medical services,Neural networks,Training},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\Q2I7MCGZ\\Johnson 和 Khoshgoftaar - 2019 - Deep Learning and Data Sampling with Imbalanced Bi.pdf;C\:\\Users\\wenji\\Zotero\\storage\\HCWBRZ69\\8843477.html}
}

@article{huangADAINCVAEImprovedData2022,
  title = {{{ADA-INCVAE}}: {{Improved}} Data Generation Using Variational Autoencoder for Imbalanced Classification},
  shorttitle = {{{ADA-INCVAE}}},
  author = {Huang, Kai and Wang, Xiaoguo},
  date = {2022-02-01},
  journaltitle = {Applied Intelligence},
  shortjournal = {Appl Intell},
  volume = {52},
  number = {3},
  pages = {2838--2853},
  issn = {1573-7497},
  doi = {10.1007/s10489-021-02566-1},
  url = {https://doi.org/10.1007/s10489-021-02566-1},
  urldate = {2024-05-07},
  abstract = {Increasing the number of minority samples by data generation can effectively improve the performance of mining minority samples using a classifier in imbalanced problems. In this paper, we proposed an effective data generation algorithm for minority samples called the Adaptive Increase dimension of Variational AutoEncoder (ADA-INCVAE). Complementary to prior studies, a theoretical study is conducted from the perspective of multi-task learning to solve the posterior collapse for VAE. Afterward, by using the theoretical support, it proposed a novel training method by increasing the dimension of data to avoid the occurrence of posterior collapse. Aiming at restricting the range of synthetic data for different minority samples, an adaptive reconstruction loss weight is proposed according to the distance distribution of majority samples around the minority class samples. In the data generation stage, the generation proportion of different sample points is determined by the local information of the minority class. The experimental results based on 12 imbalanced datasets indicated that the algorithm could help the classifier to effectively improve F1-measure and G-mean, which verifies the effectiveness of synthetic data generated by ADA-INCVAE.},
  langid = {english},
  keywords = {Data generation,Imbalanced learning,Machine learning,Variational AutoEncoder},
  file = {C:\Users\wenji\Zotero\storage\DURMC3UD\Huang 和 Wang - 2022 - ADA-INCVAE Improved data generation using variati.pdf}
}

@inproceedings{tangKernelADASYNKernelBased2015,
  title = {{{KernelADASYN}}: {{Kernel}} Based Adaptive Synthetic Data Generation for Imbalanced Learning},
  shorttitle = {{{KernelADASYN}}},
  booktitle = {2015 {{IEEE Congress}} on {{Evolutionary Computation}} ({{CEC}})},
  author = {Tang, Bo and He, Haibo},
  date = {2015-05},
  pages = {664--671},
  issn = {1941-0026},
  doi = {10.1109/CEC.2015.7256954},
  url = {https://ieeexplore.ieee.org/document/7256954},
  urldate = {2024-05-07},
  abstract = {In imbalanced learning, most standard classification algorithms usually fail to properly represent data distribution and provide unfavorable classification performance. More specifically, the decision rule of minority class is usually weaker than majority class, leading to many misclassification of expensive minority class data. Motivated by our previous work ADASYN [1], this paper presents a novel kernel based adaptive synthetic over-sampling approach, named KernelADASYN, for imbalanced data classification problems. The idea is to construct an adaptive over-sampling distribution to generate synthetic minority class data. The adaptive over-sampling distribution is first estimated with kernel density estimation methods and is further weighted by the difficulty level for different minority class data. The classification performance of our proposed adaptive over-sampling approach is evaluated on several real-life benchmarks, specifically on medical and healthcare applications. The experimental results show the competitive classification performance for many real-life imbalanced data classification problems.},
  eventtitle = {2015 {{IEEE Congress}} on {{Evolutionary Computation}} ({{CEC}})},
  keywords = {Accuracy,adaptive over-sampling,Estimation,Imbalanced learning,Kernel,kernel density estimation,Measurement,medical and healthcare data learning,pattern recognition,Sampling methods,Standards,Training data},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\WGLHN2CW\\Tang 和 He - 2015 - KernelADASYN Kernel based adaptive synthetic data.pdf;C\:\\Users\\wenji\\Zotero\\storage\\YHSQ46KD\\7256954.html}
}

@article{lucasUnderstandingPosteriorCollapse2019,
  title = {Understanding {{Posterior Collapse}} in {{Generative Latent Variable Models}}},
  author = {Lucas, James and Tucker, George and Grosse, Roger and Norouzi, Mohammad},
  date = {2019-04-19},
  url = {https://openreview.net/forum?id=r1xaVLUYuE},
  urldate = {2024-05-07},
  abstract = {Posterior collapse in Variational Autoencoders (VAEs) arises when the variational distribution closely matches the uninformative prior for a subset of latent variables. This paper presents a simple and intuitive explanation for posterior collapse through the analysis of linear VAEs and their direct correspondence with Probabilistic PCA (pPCA). We identify how local maxima can emerge from the marginal log-likelihood of pPCA, which yields similar local maxima for the evidence lower bound (ELBO). We show that training a linear VAE with variational inference recovers a uniquely identifiable global maximum corresponding to the principal component directions. We provide empirical evidence that the presence of local maxima causes posterior collapse in deep non-linear VAEs. Our findings help to explain a wide range of heuristic approaches in the literature that attempt to diminish the effect of the KL term in the ELBO to reduce posterior collapse.},
  langid = {english},
  file = {C:\Users\wenji\Zotero\storage\LE2F48TK\Lucas 等 - 2019 - Understanding Posterior Collapse in Generative Lat.pdf}
}

@inproceedings{wangPosteriorCollapseLatent2021,
  title = {Posterior {{Collapse}} and {{Latent Variable Non-identifiability}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Wang, Yixin and Blei, David and Cunningham, John P},
  date = {2021},
  volume = {34},
  pages = {5443--5455},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2021/hash/2b6921f2c64dee16ba21ebf17f3c2c92-Abstract.html},
  urldate = {2024-05-07},
  abstract = {Variational autoencoders model high-dimensional data by positinglow-dimensional latent variables that are mapped through a flexibledistribution parametrized by a neural network. Unfortunately,variational autoencoders often suffer from posterior collapse: theposterior of the latent variables is equal to its prior, rendering thevariational autoencoder useless as a means to produce meaningfulrepresentations. Existing approaches to posterior collapse oftenattribute it to the use of neural networks or optimization issues dueto variational approximation. In this paper, we consider posteriorcollapse as a problem of latent variable non-identifiability. We provethat the posterior collapses if and only if the latent variables arenon-identifiable in the generative model. This fact implies thatposterior collapse is not a phenomenon specific to the use of flexibledistributions or approximate inference. Rather, it can occur inclassical probabilistic models even with exact inference, which wealso demonstrate. Based on these results, we propose a class oflatent-identifiable variational autoencoders, deep generative modelswhich enforce identifiability without sacrificing flexibility. Thismodel class resolves the problem of latent variablenon-identifiability by leveraging bijective Brenier maps andparameterizing them with input convex neural networks, without specialvariational inference objectives or optimization tricks. Acrosssynthetic and real datasets, latent-identifiable variationalautoencoders outperform existing methods in mitigating posteriorcollapse and providing meaningful representations of the data.},
  file = {C:\Users\wenji\Zotero\storage\3FH33CY2\Wang 等 - 2021 - Posterior Collapse and Latent Variable Non-identif.pdf}
}

@inproceedings{xuUnderstandingRoleImportance2020,
  title = {Understanding the Role of Importance Weighting for Deep Learning},
  author = {Xu, Da and Ye, Yuting and Ruan, Chuanwei},
  date = {2020-10-02},
  url = {https://openreview.net/forum?id=_WnwtieRHxM},
  urldate = {2024-05-08},
  abstract = {The recent paper by Byrd \& Lipton (2019), based on empirical observations, raises a major concern on the impact of importance weighting for the over-parameterized deep learning models. They observe that as long as the model can separate the training data, the impact of importance weighting diminishes as the training proceeds. Nevertheless, there lacks a rigorous characterization of this phenomenon. In this paper, we provide formal characterizations and theoretical justifications on the role of importance weighting with respect to the implicit bias of gradient descent and margin-based learning theory. We reveal both the optimization dynamics and generalization performance under deep learning models. Our work not only explains the various novel phenomenons observed for importance weighting in deep learning, but also extends to the studies where the weights are being optimized as part of the model, which applies to a number of topics under active research.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {C:\Users\wenji\Zotero\storage\6ATWIMKH\Xu 等 - 2020 - Understanding the role of importance weighting for.pdf}
}

@inproceedings{xu2021understanding,
  title = {Understanding the Role of Importance Weighting for Deep Learning},
  booktitle = {International Conference on Learning Representations},
  author = {Xu, Da and Ye, Yuting and Ruan, Chuanwei},
  date = {2021},
  url = {https://openreview.net/forum?id=_WnwtieRHxM},
  file = {C:\Users\wenji\Zotero\storage\VB5GU7K3\Xu 等 - 2021 - Understanding the role of importance weighting for.pdf}
}

@inproceedings{cuiClassBalancedLossBased2019,
  title = {Class-{{Balanced Loss Based}} on {{Effective Number}} of {{Samples}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Cui, Yin and Jia, Menglin and Lin, Tsung-Yi and Song, Yang and Belongie, Serge},
  date = {2019-06},
  pages = {9260--9269},
  issn = {2575-7075},
  doi = {10.1109/CVPR.2019.00949},
  url = {https://ieeexplore.ieee.org/document/8953804},
  urldate = {2024-05-08},
  abstract = {With the rapid increase of large-scale, real-world datasets, it becomes critical to address the problem of long-tailed data distribution (i.e., a few classes account for most of the data, while most classes are under-represented). Existing solutions typically adopt class re-balancing strategies such as re-sampling and re-weighting based on the number of observations for each class. In this work, we argue that as the number of samples increases, the additional benefit of a newly added data point will diminish. We introduce a novel theoretical framework to measure data overlap by associating with each sample a small neighboring region rather than a single point. The effective number of samples is defined as the volume of samples and can be calculated by a simple-formula (1-βn)/(1-β), where n is the number of samples and β ∈ [0, 1) is a hyperparameter. We design a re-weighting scheme that uses the effective number of samples for each class to re-balance the loss, thereby yielding a class-balanced loss. Comprehensive experiments are conducted on artificially induced long-tailed CIFAR datasets and large-scale datasets including ImageNet and iNaturalist. Our results show that when trained with the proposed class-balanced loss, the network is able to achieve significant performance gains on long-tailed datasets.},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  keywords = {Categorization,Computer Vision Theory,Deep Learning,Recognition: Detection,Retrieval},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\H45JGEZ3\\Cui 等 - 2019 - Class-Balanced Loss Based on Effective Number of S.pdf;C\:\\Users\\wenji\\Zotero\\storage\\RS96WZYU\\8953804.html}
}

@article{tokdarImportanceSamplingReview2010,
  title = {Importance Sampling: A Review},
  shorttitle = {Importance Sampling},
  author = {Tokdar, Surya T. and Kass, Robert E.},
  date = {2010},
  journaltitle = {WIREs Computational Statistics},
  volume = {2},
  number = {1},
  pages = {54--60},
  issn = {1939-0068},
  doi = {10.1002/wics.56},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wics.56},
  urldate = {2024-05-08},
  abstract = {We provide a short overview of importance sampling—a popular sampling tool used for Monte Carlo computing. We discuss its mathematical foundation and properties that determine its accuracy in Monte Carlo approximations. We review the fundamental developments in designing efficient importance sampling (IS) for practical use. This includes parametric approximation with optimization-based adaptation, sequential sampling with dynamic adaptation through resampling and population-based approaches that make use of Markov chain sampling. Copyright © 2009 John Wiley \& Sons, Inc. This article is categorized under: Statistical and Graphical Methods of Data Analysis {$>$} Sampling},
  langid = {english},
  keywords = {importance sampling,Markov chain sampling,Monte Carlo approximation,resampling,sequential sampling},
  file = {C:\Users\wenji\Zotero\storage\E3XFRPB6\wics.html}
}

@article{tokdarImportanceSamplingReview2010a,
  title = {Importance Sampling: A Review},
  shorttitle = {Importance Sampling},
  author = {Tokdar, Surya T. and Kass, Robert E.},
  date = {2010},
  journaltitle = {WIREs Computational Statistics},
  volume = {2},
  number = {1},
  pages = {54--60},
  issn = {1939-0068},
  doi = {10.1002/wics.56},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wics.56},
  urldate = {2024-05-08},
  abstract = {We provide a short overview of importance sampling—a popular sampling tool used for Monte Carlo computing. We discuss its mathematical foundation and properties that determine its accuracy in Monte Carlo approximations. We review the fundamental developments in designing efficient importance sampling (IS) for practical use. This includes parametric approximation with optimization-based adaptation, sequential sampling with dynamic adaptation through resampling and population-based approaches that make use of Markov chain sampling. Copyright © 2009 John Wiley \& Sons, Inc. This article is categorized under: Statistical and Graphical Methods of Data Analysis {$>$} Sampling},
  langid = {english},
  keywords = {importance sampling,Markov chain sampling,Monte Carlo approximation,resampling,sequential sampling},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\VJ78FBYQ\\Tokdar 和 Kass - 2010 - Importance sampling a review.pdf;C\:\\Users\\wenji\\Zotero\\storage\\8JH67AJM\\wics.html}
}

@inproceedings{wangLearningModelTail2017,
  title = {Learning to {{Model}} the {{Tail}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Wang, Yu-Xiong and Ramanan, Deva and Hebert, Martial},
  date = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper_files/paper/2017/hash/147ebe637038ca50a1265abac8dea181-Abstract.html},
  urldate = {2024-05-08},
  abstract = {We describe an approach to learning from long-tailed, imbalanced datasets that are prevalent in real-world settings.  Here, the challenge is to learn accurate "few-shot'' models for classes in the tail of the class distribution, for which little data is available. We cast this problem as transfer learning, where knowledge from the data-rich classes in the head of the distribution is transferred to the data-poor classes in the tail. Our key insights are as follows. First, we propose to transfer meta-knowledge about learning-to-learn from the head classes. This knowledge is encoded with a meta-network that operates on the space of model parameters, that is trained to predict many-shot model parameters from few-shot model parameters.  Second, we transfer this meta-knowledge in a progressive manner, from classes in the head to the "body'', and from the "body'' to the tail. That is, we transfer knowledge in a gradual fashion, regularizing meta-networks for few-shot regression with those trained with more training data. This allows our final network to capture a notion of model dynamics, that predicts how model parameters are likely to change as more training data is gradually added. We demonstrate results on image classification datasets (SUN, Places, and ImageNet) tuned for the long-tailed setting, that significantly outperform common heuristics, such as data resampling or reweighting.},
  file = {C:\Users\wenji\Zotero\storage\HM3KGA3V\Wang 等 - 2017 - Learning to Model the Tail.pdf}
}

@inproceedings{dongClassRectificationHard2017a,
  title = {Class {{Rectification Hard Mining}} for {{Imbalanced Deep Learning}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Dong, Qi and Gong, Shaogang and Zhu, Xiatian},
  date = {2017-10},
  pages = {1869--1878},
  issn = {2380-7504},
  doi = {10.1109/ICCV.2017.205},
  url = {https://ieeexplore.ieee.org/document/8237467},
  urldate = {2024-05-08},
  abstract = {Recognising detailed facial or clothing attributes in images of people is a challenging task for computer vision, especially when the training data are both in very large scale and extremely imbalanced among different attribute classes. To address this problem, we formulate a novel scheme for batch incremental hard sample mining of minority attribute classes from imbalanced large scale training data. We develop an end-to-end deep learning framework capable of avoiding the dominant effect of majority classes by discovering sparsely sampled boundaries of minority classes. This is made possible by introducing a Class Rectification Loss (CRL) regularising algorithm. We demonstrate the advantages and scalability of CRL over existing state-of-the-art attribute recognition and imbalanced data learning models on two large scale imbalanced benchmark datasets, the CelebA facial attribute dataset and the X-Domain clothing attribute dataset.},
  eventtitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  keywords = {Clothing,Data mining,Data models,Face recognition,Machine learning,Training,Training data},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\Y6V2KVA4\\Dong 等 - 2017 - Class Rectification Hard Mining for Imbalanced Dee.pdf;C\:\\Users\\wenji\\Zotero\\storage\\SVR98YAL\\8237467.html}
}

@inproceedings{heAdaptivelyWeightedMultitask2017,
  title = {Adaptively {{Weighted Multi-task Deep Network}} for {{Person Attribute Classification}}},
  author = {He, Keke and Wang, Zhanxiong and Fu, Yanwei and Feng, Rui and Jiang, Yu-Gang and Xue, Xiangyang},
  date = {2017-10-23},
  pages = {1636--1644},
  doi = {10.1145/3123266.3123424},
  abstract = {Multi-task learning aims to boost the performance of multiple prediction tasks by appropriately sharing relevant information among them. However, it always suffers from the negative transfer problem. And due to the diverse learning difficulties and convergence rates of different tasks, jointly optimizing multiple tasks is very challenging. To solve these problems, we present a weighted multi-task deep convolutional neural network for person attribute analysis. A novel validation loss trend algorithm is, for the first time proposed to dynamically and adaptively update the weight for learning each task in the training process. Extensive experiments on CelebA, Market-1501 attribute and Duke attribute datasets clearly show that state-of-the-art performance is obtained; and this validates the effectiveness of our proposed framework.}
}

@inproceedings{heAdaptivelyWeightedMultitask2017a,
  title = {Adaptively {{Weighted Multi-task Deep Network}} for {{Person Attribute Classification}}},
  booktitle = {Proceedings of the 25th {{ACM}} International Conference on {{Multimedia}}},
  author = {He, Keke and Wang, Zhanxiong and Fu, Yanwei and Feng, Rui and Jiang, Yu-Gang and Xue, Xiangyang},
  date = {2017-10-23},
  series = {{{MM}} '17},
  pages = {1636--1644},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3123266.3123424},
  url = {https://dl.acm.org/doi/10.1145/3123266.3123424},
  urldate = {2024-05-08},
  abstract = {Multi-task learning aims to boost the performance of multiple prediction tasks by appropriately sharing relevant information among them. However, it always suffers from the negative transfer problem. And due to the diverse learning difficulties and convergence rates of different tasks, jointly optimizing multiple tasks is very challenging. To solve these problems, we present a weighted multi-task deep convolutional neural network for person attribute analysis. A novel validation loss trend algorithm is, for the first time proposed to dynamically and adaptively update the weight for learning each task in the training process. Extensive experiments on CelebA, Market-1501 attribute and Duke attribute datasets clearly show that state-of-the-art performance is obtained; and this validates the effectiveness of our proposed framework.},
  isbn = {978-1-4503-4906-2},
  keywords = {deep learning,facial attribute analysis,multi-task learning,person attribute analysis},
  file = {C:\Users\wenji\Zotero\storage\5M3LXYQD\He 等 - 2017 - Adaptively Weighted Multi-task Deep Network for Pe.pdf}
}

@inproceedings{hongDisentanglingLabelDistribution2021,
  title = {Disentangling {{Label Distribution}} for {{Long-tailed Visual Recognition}}},
  booktitle = {2021 {{IEEE}}/{{CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION}}, {{CVPR}} 2021},
  author = {Hong, Youngkyu and Han, Seungju and Choi, Kwanghee and Seo, Seokjun and Kim, Beomsu and Chang, Buru},
  date = {2021},
  pages = {6622--6632},
  publisher = {IEEE Computer Soc},
  location = {Los Alamitos},
  issn = {1063-6919},
  doi = {10.1109/CVPR46437.2021.00656},
  url = {https://www.webofscience.com/api/gateway?GWVersion=2&SrcAuth=DOISource&SrcApp=WOS&KeyAID=10.1109%2Fcvpr46437.2021.00656&DestApp=DOI&SrcAppSID=EUW1ED0FD3Wtm4INhafXMsvzwUnvb&SrcJTitle=2021+IEEE%2FCVF+CONFERENCE+ON+COMPUTER+VISION+AND+PATTERN+RECOGNITION%2C+CVPR+2021&DestDOIRegistrantName=Institute+of+Electrical+and+Electronics+Engineers},
  urldate = {2024-05-08},
  abstract = {The current evaluation protocol of long-tailed visual recognition trains the classification model on the long-tailed source label distribution and evaluates its performance on the uniform target label distribution. Such protocol has questionable practicality since the target may also be long-tailed. Therefore, we formulate long-tailed visual recognition as a label shift problem where the target and source label distributions are different. One of the significant hurdles in dealing with the label shift problem is the entanglement between the source label distribution and the model prediction. In this paper, we focus on disentangling the source label distribution from the model prediction. We first introduce a simple but overlooked baseline method that matches the target label distribution by post-processing the model prediction trained by the cross-entropy loss and the Softmax function. Although this method surpasses state-of-the-art methods on benchmark datasets, it can be further improved by directly disentangling the source label distribution from the model prediction in the training phase. Thus, we propose a novel method, LAbel distribution DisEntangling (LADE) loss based on the optimal bound of Donsker-Varadhan representation. LADE achieves state-of-the-art performance on benchmark datasets such as CIFAR-100-LL Places-LT ImageNet-LL and iNaturalist 2018. Moreover LADE outperforms existing methods on various shifted target label distributions, showing the general adaptability of our proposed method.},
  eventtitle = {{{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-66544-509-2},
  langid = {english},
  pagetotal = {11},
  keywords = {CLASS IMBALANCE},
  annotation = {Web of Science ID: WOS:000739917306083},
  file = {C:\Users\wenji\Zotero\storage\NTVRVZZX\Hong 等 - 2021 - Disentangling Label Distribution for Long-tailed V.pdf}
}

@inproceedings{akbaniApplyingSupportVector2004,
  title = {Applying {{Support Vector Machines}} to {{Imbalanced Datasets}}},
  booktitle = {Machine {{Learning}}: {{ECML}} 2004},
  author = {Akbani, Rehan and Kwek, Stephen and Japkowicz, Nathalie},
  editor = {Boulicaut, Jean-François and Esposito, Floriana and Giannotti, Fosca and Pedreschi, Dino},
  date = {2004},
  pages = {39--50},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-30115-8_7},
  abstract = {Support Vector Machines (SVM) have been extensively studied and have shown remarkable success in many applications. However the success of SVM is very limited when it is applied to the problem of learning from imbalanced datasets in which negative instances heavily outnumber the positive instances (e.g. in gene profiling and detecting credit card fraud). This paper discusses the factors behind this failure and explains why the common strategy of undersampling the training data may not be the best choice for SVM. We then propose an algorithm for overcoming these problems which is based on a variant of the SMOTE algorithm by Chawla et al, combined with Veropoulos et al’s different error costs algorithm. We compare the performance of our algorithm against these two algorithms, along with undersampling and regular SVM and show that our algorithm outperforms all of them.},
  isbn = {978-3-540-30115-8},
  langid = {english},
  keywords = {Minority Class,Negative Instance,Positive Class,Positive Instance,Support Vector Machine},
  file = {C:\Users\wenji\Zotero\storage\L6B567EE\Akbani 等 - 2004 - Applying Support Vector Machines to Imbalanced Dat.pdf}
}

@inproceedings{paulGaussianMixtureBased2016,
  title = {Gaussian Mixture Based Semi Supervised Boosting for Imbalanced Data Classification},
  booktitle = {2016 2nd {{International Conference}} on {{Electrical}}, {{Computer}} \& {{Telecommunication Engineering}} ({{ICECTE}})},
  author = {Paul, Mahit Kumar and Pal, Biprodip},
  date = {2016-12},
  pages = {1--4},
  doi = {10.1109/ICECTE.2016.7879620},
  url = {https://ieeexplore.ieee.org/abstract/document/7879620?casa_token=Kd-pL2QO5xoAAAAA:kA6WTVTqKm9ikv7EY95VV6Y-HXXBDGDPiwMqJ6ZF1wHRdpZc_CVgvc2KlRMH8rRLaTywUSVfsAg},
  urldate = {2024-05-09},
  abstract = {Semi supervised approaches are practical in problem domain where pattern clustering is supposed to provide good classification. Gaussian Mixture Model (GMM) can approximate arbitrary probability distribution, thus is considered as a dominant tool for classification in such domains. This paper appraises the functioning for GMM as it is applied to imbalanced datasets which consists of uneven distribution of samples from all the classes. Later, an ensemble approach is presented to boost the GMMs in a semi supervised manner via Adaptive Boosting technique. Experiment on benchmark imbalanced datasets with different imbalance ratio has been carried out. Empirical result demonstrates the efficacy of the proposed Boosted GMM classifier compared to baseline approaches like K-means and GMM.},
  eventtitle = {2016 2nd {{International Conference}} on {{Electrical}}, {{Computer}} \& {{Telecommunication Engineering}} ({{ICECTE}})},
  keywords = {AdaBoost,Boosting,Classification,Classification algorithms,Clustering,Computers,Gaussian mixture model,Gaussian Mixture Model,Imbalance data,Maximum likelihood estimation,Probability density function},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\35QUH24Z\\Paul 和 Pal - 2016 - Gaussian mixture based semi supervised boosting fo.pdf;C\:\\Users\\wenji\\Zotero\\storage\\J7JHVKN2\\7879620.html}
}

@inproceedings{xuAdaptiveClassBalancedLoss2022,
  title = {Adaptive {{Class-Balanced Loss Based}} on {{Re-Weighting}}},
  booktitle = {2022 6th {{Asian Conference}} on {{Artificial Intelligence Technology}} ({{ACAIT}})},
  author = {Xu, Chuanyun and Zheng, Yu and Zhang, Yang and Sun, Chengjie and Li, Gang and Zhu, Zhaohan},
  date = {2022-12},
  pages = {1--8},
  doi = {10.1109/ACAIT56212.2022.10137858},
  url = {https://ieeexplore.ieee.org/document/10137858},
  urldate = {2024-05-09},
  abstract = {As real-world data grows fast, the problem of data imbalance has become more prominent. Thus the long-tail problem in deep learning has received lots of attention recently. One of the solutions is to apply a class rebalancing strategy, such as directly using the inverse of the class sample size for reweighting. In past studies, the setting of weights only relates to the number of class samples. Only relying on the information of the number of class samples to determine the size of the weight is very crude in the sensitive method of re-weighting. In this paper, we implement adaptive re-weighting for three essential attributes of the dataset considering several factors: the number of classes, the number of samples, and the degree of class imbalance. We conducted experiments on the commonly used sample imbalance problem solution and proposed a new sample reweighting method. Specifically, a novel re-weighting idea is proposed to optimize Class-Balanced Loss Based on an Effective Number of Samples. Experiments show that the method is superior in re-weighting imbalanced datasets on deep neural networks. We hope our work will stimulate a rethinking of the number-of-samples-based convention in re-weighting.},
  eventtitle = {2022 6th {{Asian Conference}} on {{Artificial Intelligence Technology}} ({{ACAIT}})},
  keywords = {Artificial intelligence,Data imbalance,Deep learning,Long-tail,Neural networks,Optimization,Reweighting},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\I6BMN8P3\\Xu 等 - 2022 - Adaptive Class-Balanced Loss Based on Re-Weighting.pdf;C\:\\Users\\wenji\\Zotero\\storage\\6XHP6Z3X\\10137858.html}
}

@article{sampathSurveyGenerativeAdversarial2021,
  title = {A Survey on Generative Adversarial Networks for Imbalance Problems in Computer Vision Tasks},
  author = {Sampath, Vignesh and Maurtua, Iñaki and Aguilar Martín, Juan José and Gutierrez, Aitor},
  date = {2021-01-29},
  journaltitle = {Journal of Big Data},
  shortjournal = {Journal of Big Data},
  volume = {8},
  number = {1},
  pages = {27},
  issn = {2196-1115},
  doi = {10.1186/s40537-021-00414-0},
  url = {https://doi.org/10.1186/s40537-021-00414-0},
  urldate = {2024-05-09},
  abstract = {Any computer vision application development starts off by acquiring images and data, then preprocessing and pattern recognition steps to perform a task. When the acquired images are highly imbalanced and not adequate, the desired task may not be achievable. Unfortunately, the occurrence of imbalance problems in acquired image datasets in certain complex real-world problems such as anomaly detection, emotion recognition, medical image analysis, fraud detection, metallic surface defect detection, disaster prediction, etc., are inevitable. The performance of computer vision algorithms can significantly deteriorate when the training dataset is imbalanced. In recent years, Generative Adversarial Neural Networks (GANs) have gained immense attention by researchers across a variety of application domains due to their capability to model complex real-world image data. It is particularly important that GANs can not only be used to generate synthetic images, but also its fascinating adversarial learning idea showed good potential in restoring balance in imbalanced datasets.},
  keywords = {Classification,Deep generative model,Deep learning,Generative adversarial neural networks,Imbalanced data,Object detection,Segmentation},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\LDY58F2H\\Sampath 等 - 2021 - A survey on generative adversarial networks for im.pdf;C\:\\Users\\wenji\\Zotero\\storage\\5F4MAGU8\\s40537-021-00414-0.html}
}

@article{sampathSurveyGenerativeAdversarial2021a,
  title = {A Survey on Generative Adversarial Networks for Imbalance Problems in Computer Vision Tasks},
  author = {Sampath, Vignesh and Maurtua, Iñaki and Aguilar Martín, Juan José and Gutierrez, Aitor},
  date = {2021-01-29},
  journaltitle = {Journal of Big Data},
  shortjournal = {Journal of Big Data},
  volume = {8},
  number = {1},
  pages = {27},
  issn = {2196-1115},
  doi = {10.1186/s40537-021-00414-0},
  url = {https://doi.org/10.1186/s40537-021-00414-0},
  urldate = {2024-05-09},
  abstract = {Any computer vision application development starts off by acquiring images and data, then preprocessing and pattern recognition steps to perform a task. When the acquired images are highly imbalanced and not adequate, the desired task may not be achievable. Unfortunately, the occurrence of imbalance problems in acquired image datasets in certain complex real-world problems such as anomaly detection, emotion recognition, medical image analysis, fraud detection, metallic surface defect detection, disaster prediction, etc., are inevitable. The performance of computer vision algorithms can significantly deteriorate when the training dataset is imbalanced. In recent years, Generative Adversarial Neural Networks (GANs) have gained immense attention by researchers across a variety of application domains due to their capability to model complex real-world image data. It is particularly important that GANs can not only be used to generate synthetic images, but also its fascinating adversarial learning idea showed good potential in restoring balance in imbalanced datasets.},
  keywords = {Classification,Deep generative model,Deep learning,Generative adversarial neural networks,Imbalanced data,Object detection,Segmentation},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\NP6F3VN5\\Sampath 等 - 2021 - A survey on generative adversarial networks for im.pdf;C\:\\Users\\wenji\\Zotero\\storage\\8YYPW8CI\\s40537-021-00414-0.html}
}

@inproceedings{wangDeepGenerativeModel2020,
  title = {Deep {{Generative Model}} for {{Robust Imbalance Classification}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Wang, Xinyue and Lyu, Yilin and Jing, Liping},
  date = {2020-06},
  pages = {14112--14121},
  issn = {2575-7075},
  doi = {10.1109/CVPR42600.2020.01413},
  url = {https://ieeexplore.ieee.org/document/9156755},
  urldate = {2024-05-09},
  abstract = {Discovering hidden pattern from imbalanced data is a critical issue in various real-world applications including computer vision. The existing classification methods usually suffer from the limitation of data especially the minority classes, and result in unstable prediction and low performance. In this paper, a deep generative classifier is proposed to mitigate this issue via both data perturbation and model perturbation. Specially, the proposed generative classifier is modeled by a deep latent variable model where the latent variable aims to capture the direct cause of target label. Meanwhile, the latent variable is represented by a probability distribution over possible values rather than a single fixed value, which is able to enforce uncertainty of model and lead to stable prediction. Furthermore, this latent variable, as a confounder, affects the process of data (feature/label) generation, so that we can arrive at well-justified sampling variability considerations in statistics, and implement data perturbation. Extensive experiments have been conducted on widely-used real imbalanced image datasets. By comparing with the state-of-the-art methods, experimental results demonstrate the superiority of our proposed model on imbalance classification task.},
  eventtitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  keywords = {Computer vision,Conferences,Data models,Pattern recognition,Perturbation methods,Uncertainty},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\NEIL62BP\\Wang 等 - 2020 - Deep Generative Model for Robust Imbalance Classif.pdf;C\:\\Users\\wenji\\Zotero\\storage\\ZTW2MM67\\9156755.html}
}

@inproceedings{liDifferentiableAutomaticData2020,
  title = {Differentiable {{Automatic Data Augmentation}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2020},
  author = {Li, Yonggang and Hu, Guosheng and Wang, Yongtao and Hospedales, Timothy and Robertson, Neil M. and Yang, Yongxin},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  date = {2020},
  pages = {580--595},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-58542-6_35},
  abstract = {Data augmentation (DA) techniques aim to increase data variability, and thus train deep networks with better generalisation. The pioneering AutoAugment automated the search for optimal DA policies with reinforcement learning. However, AutoAugment is extremely computationally expensive, limiting its wide applicability. Followup works such as Population Based Augmentation (PBA) and Fast AutoAugment improved efficiency, but their optimization speed remains a bottleneck. In this paper, we propose Differentiable Automatic Data Augmentation (DADA) which dramatically reduces the cost. DADA relaxes the discrete DA policy selection to a differentiable optimization problem via Gumbel-Softmax. In addition, we introduce an unbiased gradient estimator, RELAX, leading to an efficient and effective one-pass optimization strategy to learn an efficient and accurate DA policy. We conduct extensive experiments on CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets. Furthermore, we demonstrate the value of Auto DA in pre-training for downstream detection problems. Results show our DADA is at least one order of magnitude faster than the state-of-the-art while achieving very comparable accuracy. The code is available at https://github.com/VDIGPKU/DADA.},
  isbn = {978-3-030-58542-6},
  langid = {english},
  keywords = {AutoML,Data augmentation,Differentiable optimization},
  file = {C:\Users\wenji\Zotero\storage\NCGTEBUU\Li 等 - 2020 - Differentiable Automatic Data Augmentation.pdf}
}

@inproceedings{zhaoDifferentiableAugmentationDataEfficient2020,
  title = {Differentiable {{Augmentation}} for {{Data-Efficient GAN Training}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhao, Shengyu and Liu, Zhijian and Lin, Ji and Zhu, Jun-Yan and Han, Song},
  date = {2020},
  volume = {33},
  pages = {7559--7570},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2020/hash/55479c55ebd1efd3ff125f1337100388-Abstract.html},
  urldate = {2024-05-11},
  abstract = {The performance of generative adversarial networks (GANs) heavily deteriorates given a limited amount of training data. This is mainly because the discriminatorsis memorizing the exact training set. To combat it, we propose Differentiable Augmentation (DiffAugment), a simple method that improves the data efficiency of GANs by imposing various types of differentiable augmentations on both real and fake samples. Previous attempts to directly augment the training data manipulate the distribution of real images, yielding little benefit; DiffAugment enables us to adopt the differentiable augmentation for the generated samples, effectively stabilizes training, and leads to better convergence. Experiments demonstrate consistent gains of our method over a variety of GAN architectures and loss functions for both unconditional and class-conditional generation. With DiffAugment, we achieve astate-of-the-art FID of 6.80 with an IS of 100.8 on ImageNet 128×128 and 2-4× reductions of FID given 1,000 images on FFHQ and LSUN. Furthermore, with only 20\% training data, we can match the top performance on CIFAR-10 and CIFAR-100. Finally, our method can generate high-fidelity images using only 100 images without pre-training, while being on par with existing transfer learning algorithms. Code is available at https://github.com/mit-han-lab/data-efficient-gans.},
  file = {C:\Users\wenji\Zotero\storage\3NXSSKME\Zhao 等 - 2020 - Differentiable Augmentation for Data-Efficient GAN.pdf}
}

@inproceedings{mondalMinorityOversamplingImbalanced2023,
  title = {Minority {{Oversampling}} for {{Imbalanced Data}} via {{Class-Preserving Regularized Auto-Encoders}}},
  booktitle = {Proceedings of {{The}} 26th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Mondal, Arnab Kumar and Singhal, Lakshya and Tiwary, Piyush and Singla, Parag and Ap, Prathosh},
  date = {2023-04-11},
  pages = {3440--3465},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v206/mondal23a.html},
  urldate = {2024-05-11},
  abstract = {Class imbalance is a common phenomenon in multiple application domains such as healthcare, where the sample occurrence of one or few class categories is more prevalent in the dataset than the rest. This work addresses the class-imbalance issue by proposing an over-sampling method for the minority classes in the latent space of a Regularized Auto-Encoder (RAE). Specifically, we construct a latent space by maximizing the conditional data likelihood using an Encoder-Decoder structure, such that oversampling through convex combinations of latent samples preserves the class identity. A jointly-trained linear classifier that separates convexly coupled latent vectors from different classes is used to impose this property on the AE’s latent space. Further, the aforesaid linear classifier is used for final classification without retraining. We theoretically show that our method can achieve a low variance risk estimate compared to naive oversampling methods and is robust to overfitting. We conduct several experiments on benchmark datasets and show that our method outperforms the existing oversampling techniques for handling class imbalance. The code of the proposed method is available at: https://github.com/arnabkmondal/oversamplingrae.},
  eventtitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  langid = {english},
  file = {C:\Users\wenji\Zotero\storage\6R7YPZVL\Mondal 等 - 2023 - Minority Oversampling for Imbalanced Data via Clas.pdf}
}

@article{oymakIsometricSketchingAny2018,
  title = {Isometric Sketching of Any Set via the {{Restricted Isometry Property}}},
  author = {Oymak, Samet and Recht, Benjamin and Soltanolkotabi, Mahdi},
  date = {2018-12-11},
  journaltitle = {Information and Inference: A Journal of the IMA},
  shortjournal = {Information and Inference: A Journal of the IMA},
  volume = {7},
  number = {4},
  pages = {707--726},
  issn = {2049-8764},
  doi = {10.1093/imaiai/iax019},
  url = {https://doi.org/10.1093/imaiai/iax019},
  urldate = {2024-05-12},
  abstract = {In this paper we show that for the purposes of dimensionality reduction certain class of structured random matrices behave similarly to random Gaussian matrices. This class includes several matrices for which matrix-vector multiply can be computed in log-linear time, providing efficient dimensionality reduction of general sets. In particular, we show that using such matrices any set from high dimensions can be embedded into lower dimensions with near optimal distortion. We obtain our results by connecting dimensionality reduction of any set to dimensionality reduction of sparse vectors via a chaining argument.},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\XIUELLWT\\Oymak 等 - 2018 - Isometric sketching of any set via the Restricted .pdf;C\:\\Users\\wenji\\Zotero\\storage\\YZ4I6PCK\\4924580.html}
}

@incollection{vovkKernelRidgeRegression2013,
  title = {Kernel {{Ridge Regression}}},
  booktitle = {Empirical {{Inference}}: {{Festschrift}} in {{Honor}} of {{Vladimir N}}. {{Vapnik}}},
  author = {Vovk, Vladimir},
  editor = {Schölkopf, Bernhard and Luo, Zhiyuan and Vovk, Vladimir},
  date = {2013},
  pages = {105--116},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-41136-6_11},
  url = {https://doi.org/10.1007/978-3-642-41136-6_11},
  urldate = {2024-05-13},
  abstract = {This chapter discusses the method of Kernel Ridge Regression, which is a very simple special case of Support Vector Regression. The main formula of the method is identical to a formula in Bayesian statistics, but Kernel Ridge Regression has performance guarantees that have nothing to do with Bayesian assumptions. I will discuss two kinds of such performance guarantees: those not requiring any assumptions whatsoever, and those depending on the assumption of randomness.},
  isbn = {978-3-642-41136-6},
  langid = {english},
  keywords = {Bayesian Statistic,Prediction Interval,Ridge Regression,Support Vector Machine,Support Vector Regression},
  file = {C:\Users\wenji\Zotero\storage\PKFTTH3P\Vovk - 2013 - Kernel Ridge Regression.pdf}
}

@inproceedings{zhangDivideConquerKernel2013,
  title = {Divide and {{Conquer Kernel Ridge Regression}}},
  booktitle = {Proceedings of the 26th {{Annual Conference}} on {{Learning Theory}}},
  author = {Zhang, Yuchen and Duchi, John and Wainwright, Martin},
  date = {2013-06-13},
  pages = {592--617},
  publisher = {PMLR},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v30/Zhang13.html},
  urldate = {2024-05-13},
  abstract = {We study a decomposition-based scalable approach to performing kernel ridge regression.  The method is simply described: it randomly partitions a dataset of size N into m subsets of equal size, computes an independent kernel ridge regression estimator for each subset, then averages the local solutions into a global predictor. This partitioning leads to a substantial reduction in computation time versus the standard approach of performing kernel ridge regression on all N samples. Our main theorem establishes that despite the computational speed-up, statistical optimality is retained: that so long as m is not too large, the partition-based estimate achieves optimal rates of convergence for the full sample size N.  As concrete examples, our theory guarantees that m may grow polynomially in N for Sobolev spaces, and nearly linearly for finite-rank kernels and Gaussian kernels. We conclude with simulations complementing our theoretical results and exhibiting the computational and statistical benefits of our approach.},
  eventtitle = {Conference on {{Learning Theory}}},
  langid = {english},
  file = {C:\Users\wenji\Zotero\storage\CJ56YWTU\Zhang 等 - 2013 - Divide and Conquer Kernel Ridge Regression.pdf}
}

@inproceedings{yeGenerationEvaluationCreative2022,
  title = {Generation and {{Evaluation}} of {{Creative Images}} from {{Limited Data}}: {{A Class-to-Class VAE Approach}}.},
  author = {Ye, Xiaomeng and Zhao, Ziwei and Leake, David and Crandall, David},
  date = {2022},
  pages = {314--323},
  eventtitle = {{{ICCC}}}
}

@inproceedings{chenFastStatisticalLeverage2021,
  title = {Fast {{Statistical Leverage Score Approximation}} in {{Kernel Ridge Regression}}},
  booktitle = {Proceedings of {{The}} 24th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Chen, Yifan and Yang, Yun},
  date = {2021-03-18},
  pages = {2935--2943},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v130/chen21e.html},
  urldate = {2024-05-14},
  abstract = {Nyström approximation is a fast randomized method that rapidly solves kernel ridge regression (KRR) problems through sub-sampling the n-by-n empirical kernel matrix appearing in the objective function. However, the performance of such a sub-sampling method heavily relies on correctly estimating the statistical leverage scores for forming the sampling distribution, which can be as costly as solving the original KRR. In this work, we propose a linear time (modulo poly-log terms) algorithm to accurately approximate the statistical leverage scores in the stationary-kernel-based KRR with theoretical guarantees. Particularly, by analyzing the first-order condition of the KRR objective, we derive an analytic formula, which depends on both the input distribution and the spectral density of stationary kernels, for capturing the non-uniformity of the statistical leverage scores. Numerical experiments demonstrate that with the same prediction accuracy our method is orders of magnitude more efficient than existing methods in selecting the representative sub-samples in the Nyström approximation.},
  eventtitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  langid = {english},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\JMHBK4LU\\Chen 和 Yang - 2021 - Fast Statistical Leverage Score Approximation in K.pdf;C\:\\Users\\wenji\\Zotero\\storage\\LSMA6WZW\\Chen 和 Yang - 2021 - Fast Statistical Leverage Score Approximation in K.pdf}
}

@article{csibaImportanceSamplingMinibatches2018,
  title = {Importance {{Sampling}} for {{Minibatches}}},
  author = {Csiba, Dominik and Richtárik, Peter},
  date = {2018},
  journaltitle = {Journal of Machine Learning Research},
  volume = {19},
  number = {27},
  pages = {1--21},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v19/16-241.html},
  urldate = {2024-05-14},
  abstract = {Minibatching is a very well studied and highly popular technique in supervised learning, used by practitioners due to its ability to accelerate training through better utilization of parallel processing power and reduction of stochastic variance. Another popular technique is importance sampling–a strategy for preferential sampling of more important examples also capable of accelerating the training process. However, despite considerable effort by the community in these areas, and due to the inherent technical difficulty of the problem, there is virtually no existing work combining the power of importance sampling with the strength of minibatching. In this paper we propose the first practical importance sampling for minibatches and give simple and rigorous complexity analysis of its performance. We illustrate on synthetic problems that for training data of certain properties, our sampling can lead to several orders of magnitude improvement in training time. We then test the new sampling on several popular data sets, and show that the improvement can reach an order of magnitude.},
  file = {C:\Users\wenji\Zotero\storage\D4FBH6QP\Csiba 和 Richtárik - 2018 - Importance Sampling for Minibatches.pdf}
}

@inproceedings{metzUnrolledGenerativeAdversarial2022,
  title = {Unrolled {{Generative Adversarial Networks}}},
  author = {Metz, Luke and Poole, Ben and Pfau, David and Sohl-Dickstein, Jascha},
  date = {2022-07-21},
  url = {https://openreview.net/forum?id=BydrOIcle},
  urldate = {2024-05-14},
  abstract = {We introduce a method to stabilize Generative Adversarial Networks (GANs) by defining the generator objective with respect to an unrolled optimization of the discriminator. This allows training to be adjusted between using the optimal discriminator in the generator's objective, which is ideal but infeasible in practice, and using the current value of the discriminator, which is often unstable and leads to poor solutions. We show how this technique solves the common problem of mode collapse, stabilizes training of GANs with complex recurrent generators, and increases diversity and coverage of the data distribution by the generator.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {C:\Users\wenji\Zotero\storage\BHNQSZKH\Metz 等 - 2022 - Unrolled Generative Adversarial Networks.pdf}
}

@misc{alma9993527274101488,
  title = {Sparse {{PCA}} in Disentangled Representation Learning},
  author = {Cheung, Wai Chun and Suykens, Johan and Pandey, Arun and Roy Chowdhury, Soutrik},
  date = {2023},
  location = {Leuven},
  abstract = {This thesis concerns the role of sparse principal component analysis (sPCA) in the current frameworks of restricted kernel machines (RKM) for learning disentangled representations from the raw dataset. The study of disentangled representation learning is an emerging field in the context of explainable artificial intelligence, computer vision and representation learning, which draws researchers' attention to develop various models, for example, beta VAE, to capture different sources of variations in the unobserved, groundtruth factors through its raw representation in a training dataset. It provides a unique and broad view of the statistical properties of learnt representations under sparsity enforcement.},
  langid = {english},
  organization = {KU Leuven. Faculteit Wetenschappen}
}

@misc{alma9992544009001488,
  title = {Generative Restricted Kernel Machines for Advanced Multi-View Modelling},
  author = {Yuan, Meng and Suykens, Johan and Schreurs, Joachim and Pandey, Arun},
  date = {2020},
  location = {Leuven},
  abstract = {In this thesis we investigate the applications of Generative Restricted Kernel Machine (Gen-RKM) as introduced by Pandey et al. (2019). Multi-view modeling deals with data that is described using different representations, or views. To realize multi-view generation, Gen-RKM learns a share latent representations of data from various views. Gen-RKM model has already been proven capable of multi-view generation using visual data. This thesis has expanded on it in two ways: 1) demonstrating its capability for text generation 2) further exploring multi-view generation for image-text data. Three datasets will provide illustrative examples of Image-Image-Label generation, Text generation, and Text-Image generation.},
  langid = {english},
  organization = {KU Leuven. Faculteit Ingenieurswetenschappen}
}

@book{scholkopfLearningKernelsSupport2018,
  title = {Learning with {{Kernels}}: {{Support Vector Machines}}, {{Regularization}}, {{Optimization}}, and {{Beyond}}},
  shorttitle = {Learning with {{Kernels}}},
  author = {Schölkopf, Bernhard and Smola, Alexander J.},
  date = {2018-06-05},
  publisher = {The MIT Press},
  doi = {10.7551/mitpress/4175.001.0001},
  url = {https://direct.mit.edu/books/book/1821/Learning-with-KernelsSupport-Vector-Machines},
  urldate = {2024-05-14},
  abstract = {A comprehensive introduction to Support Vector Machines and related kernel methods.In the 1990s, a new type of learning algorithm was developed, based on r},
  isbn = {978-0-262-25693-3},
  langid = {english},
  file = {C:\Users\wenji\Zotero\storage\DBTHQ9I6\Learning-with-KernelsSupport-Vector-Machines.html}
}

@article{wellingKernelRidgeRegression2013,
  title = {Kernel Ridge Regression},
  author = {Welling, Max},
  date = {2013},
  journaltitle = {Max Welling’s classnotes in machine learning},
  shortjournal = {Max Welling’s classnotes in machine learning},
  pages = {1--3}
}

@inproceedings{spelmenReviewHandlingImbalanced2018,
  title = {A {{Review}} on {{Handling Imbalanced Data}}},
  booktitle = {2018 {{International Conference}} on {{Current Trends}} towards {{Converging Technologies}} ({{ICCTCT}})},
  author = {Spelmen, Vimalraj S and Porkodi, R},
  date = {2018-03},
  pages = {1--11},
  doi = {10.1109/ICCTCT.2018.8551020},
  url = {https://ieeexplore.ieee.org/abstract/document/8551020?casa_token=HwZ7e2bvLL8AAAAA:17UDIs1uxk897APAAspwQN-eVhxJ57eJ6FVDMs8e1mwmK9w1ml-nGwxt1gUAH9XYuiPEOwuyTbQ},
  urldate = {2024-05-20},
  abstract = {Computational synthesize of the metabolic pathway is take low cost while comparing with the direct trial and error laboratory process. In real world data, more or less all datasets having a skewed distribution of classes. The skewed and the number of instances for certain classes much higher than other classes, this problem is known as the class imbalance problem. Practically this class imbalance problem reduces the classification accuracy because it predicts the minority class instances inaccurately. Class imbalance is an issue encountered by data mining practitioners in a wide variety of fields. The classification of imbalanced data is a new problem that rises in the machine learning framework and it is the major problem raised for the researches and the use of sampling techniques to improve classification performance has received significant attention in related works. In this article the necessity of balancing an imbalanced data is elaborated and the methods proposed by the various authors for to balance the imbalanced data and the evaluation metrics to assess the accuracy and predictive rate of the classification algorithms also have been discussed.},
  eventtitle = {2018 {{International Conference}} on {{Current Trends}} towards {{Converging Technologies}} ({{ICCTCT}})},
  keywords = {Classification,Classification algorithms,Conferences,Data imbalance,Data mining,Forestry,Hybrid methods,Market research,Oversampling,Support vector machines,Task analysis,Undersampling},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\4VRSTL2J\\Spelmen 和 Porkodi - 2018 - A Review on Handling Imbalanced Data.pdf;C\:\\Users\\wenji\\Zotero\\storage\\QKBLRKL3\\8551020.html}
}

@inproceedings{spelmenReviewHandlingImbalanced2018a,
  title = {A {{Review}} on {{Handling Imbalanced Data}}},
  booktitle = {2018 {{International Conference}} on {{Current Trends}} towards {{Converging Technologies}} ({{ICCTCT}})},
  author = {Spelmen, Vimalraj S and Porkodi, R},
  date = {2018-03},
  pages = {1--11},
  doi = {10.1109/ICCTCT.2018.8551020},
  url = {https://ieeexplore.ieee.org/abstract/document/8551020?casa_token=HwZ7e2bvLL8AAAAA:17UDIs1uxk897APAAspwQN-eVhxJ57eJ6FVDMs8e1mwmK9w1ml-nGwxt1gUAH9XYuiPEOwuyTbQ},
  urldate = {2024-05-20},
  abstract = {Computational synthesize of the metabolic pathway is take low cost while comparing with the direct trial and error laboratory process. In real world data, more or less all datasets having a skewed distribution of classes. The skewed and the number of instances for certain classes much higher than other classes, this problem is known as the class imbalance problem. Practically this class imbalance problem reduces the classification accuracy because it predicts the minority class instances inaccurately. Class imbalance is an issue encountered by data mining practitioners in a wide variety of fields. The classification of imbalanced data is a new problem that rises in the machine learning framework and it is the major problem raised for the researches and the use of sampling techniques to improve classification performance has received significant attention in related works. In this article the necessity of balancing an imbalanced data is elaborated and the methods proposed by the various authors for to balance the imbalanced data and the evaluation metrics to assess the accuracy and predictive rate of the classification algorithms also have been discussed.},
  eventtitle = {2018 {{International Conference}} on {{Current Trends}} towards {{Converging Technologies}} ({{ICCTCT}})},
  keywords = {Classification,Classification algorithms,Conferences,Data imbalance,Data mining,Forestry,Hybrid methods,Market research,Oversampling,Support vector machines,Task analysis,Undersampling},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\AYABNUCK\\Spelmen 和 Porkodi - 2018 - A Review on Handling Imbalanced Data.pdf;C\:\\Users\\wenji\\Zotero\\storage\\DLGUXCWQ\\8551020.html}
}

@inproceedings{mathewKernelbasedSMOTESVM2015,
  title = {Kernel-Based {{SMOTE}} for {{SVM}} Classification of Imbalanced Datasets},
  booktitle = {{{IECON}} 2015 - 41st {{Annual Conference}} of the {{IEEE Industrial Electronics Society}}},
  author = {Mathew, Josey and Luo, Ming and Pang, Chee Khiang and Chan, Hian Leng},
  date = {2015-11},
  pages = {001127--001132},
  doi = {10.1109/IECON.2015.7392251},
  url = {https://ieeexplore.ieee.org/document/7392251},
  urldate = {2024-05-20},
  abstract = {Datasets with an imbalanced class distribution pose a severe challenge to traditional learning algorithms that are designed to improve overall classification accuracy. Preprocessing methods like Synthetic Minority Over-sampling Technique (SMOTE) address this problem by generating data points in the input space to balance the training dataset. However, such artificial sampling methods can distort the performance of Support Vector Machine (SVM) classifiers that operate in a kernel induced feature space. This paper proposes a kernel-based SMOTE (K-SMOTE) algorithm that directly generates synthetically minority data points in the feature space of SVM classifier. The new data points are added by augmenting the original Gram matrix based on neighbourhood information in the feature space. The proposed algorithm is statistically shown to improve performance on 51 benchmark datasets. K-SMOTE is further applied to predict the stage of degradation in a semiconductor etching chamber where it achieves a higher accuracy for the imbalanced faulty stages.},
  eventtitle = {{{IECON}} 2015 - 41st {{Annual Conference}} of the {{IEEE Industrial Electronics Society}}},
  keywords = {Electronic mail,Euclidean distance,Extraterrestrial measurements,Kernel,Sampling methods,Support vector machines,Training},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\ZRYPPAN6\\Mathew 等 - 2015 - Kernel-based SMOTE for SVM classification of imbal.pdf;C\:\\Users\\wenji\\Zotero\\storage\\9APGXSVE\\7392251.html}
}

@article{paganoBiasUnfairnessMachine2023,
  title = {Bias and {{Unfairness}} in {{Machine Learning Models}}: {{A Systematic Review}} on {{Datasets}}, {{Tools}}, {{Fairness Metrics}}, and {{Identification}} and {{Mitigation Methods}}},
  shorttitle = {Bias and {{Unfairness}} in {{Machine Learning Models}}},
  author = {Pagano, Tiago P. and Loureiro, Rafael B. and Lisboa, Fernanda V. N. and Peixoto, Rodrigo M. and Guimarães, Guilherme A. S. and Cruz, Gustavo O. R. and Araujo, Maira M. and Santos, Lucas L. and Cruz, Marco A. S. and Oliveira, Ewerton L. S. and Winkler, Ingrid and Nascimento, Erick G. S.},
  date = {2023-03},
  journaltitle = {Big Data and Cognitive Computing},
  volume = {7},
  number = {1},
  pages = {15},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2504-2289},
  doi = {10.3390/bdcc7010015},
  url = {https://www.mdpi.com/2504-2289/7/1/15},
  urldate = {2024-05-20},
  abstract = {One of the difficulties of artificial intelligence is to ensure that model decisions are fair and free of bias. In research, datasets, metrics, techniques, and tools are applied to detect and mitigate algorithmic unfairness and bias. This study examines the current knowledge on bias and unfairness in machine learning models. The systematic review followed the PRISMA guidelines and is registered on OSF plataform. The search was carried out between 2021 and early 2022 in the Scopus, IEEE Xplore, Web of Science, and Google Scholar knowledge bases and found 128 articles published between 2017 and 2022, of which 45 were chosen based on search string optimization and inclusion and exclusion criteria. We discovered that the majority of retrieved works focus on bias and unfairness identification and mitigation techniques, offering tools, statistical approaches, important metrics, and datasets typically used for bias experiments. In terms of the primary forms of bias, data, algorithm, and user interaction were addressed in connection to the preprocessing, in-processing, and postprocessing mitigation methods. The use of Equalized Odds, Opportunity Equality, and Demographic Parity as primary fairness metrics emphasizes the crucial role of sensitive attributes in mitigating bias. The 25 datasets chosen span a wide range of areas, including criminal justice image enhancement, finance, education, product pricing, and health, with the majority including sensitive attributes. In terms of tools, Aequitas is the most often referenced, yet many of the tools were not employed in empirical experiments. A limitation of current research is the lack of multiclass and multimetric studies, which are found in just a few works and constrain the investigation to binary-focused method. Furthermore, the results indicate that different fairness metrics do not present uniform results for a given use case, and that more research with varied model architectures is necessary to standardize which ones are more appropriate for a given context. We also observed that all research addressed the transparency of the algorithm, or its capacity to explain how decisions are taken.},
  issue = {1},
  langid = {english},
  keywords = {artificial intelligence,bias,machine learning,unfairness},
  file = {C:\Users\wenji\Zotero\storage\VD678TLZ\Pagano 等 - 2023 - Bias and Unfairness in Machine Learning Models A .pdf}
}

@article{reddyBenchmarkingBiasMitigation2022,
  title = {Benchmarking Bias Mitigation Algorithms in Representation Learning through Fairness Metrics},
  author = {Reddy, Charan},
  date = {2022-10-26},
  url = {https://papyrus.bib.umontreal.ca/xmlui/handle/1866/27490},
  urldate = {2024-05-20},
  abstract = {Le succès des modèles d’apprentissage en profondeur et leur adoption rapide dans de nombreux  domaines d’application ont soulevé d’importantes questions sur l’équité de ces modèles lorsqu’ils  sont déployés dans le monde réel. Des études récentes ont mis en évidence les biais encodés  par les algorithmes d’apprentissage des représentations et ont remis en cause la fiabilité de telles  approches pour prendre des décisions. En conséquence, il existe un intérêt croissant pour la  compréhension des sources de biais dans l’apprentissage des algorithmes et le développement de  stratégies d’atténuation des biais. L’objectif des algorithmes d’atténuation des biais est d’atténuer  l’influence des caractéristiques des données sensibles sur les décisions d’éligibilité prises. Les  caractéristiques sensibles sont des caractéristiques privées et protégées d’un ensemble de données  telles que le sexe ou la race, qui ne devraient pas affecter les décisions de sortie d’éligibilité, c’està-dire les critères qui rendent un individu qualifié ou non qualifié pour une tâche donnée, comme  l’octroi de prêts ou l’embauche. Les modèles d’atténuation des biais visent à prendre des décisions  d’éligibilité sur des échantillons d’ensembles de données sans biais envers les attributs sensibles  des données d’entrée. La difficulté des tâches d’atténuation des biais est souvent déterminée par  la distribution de l’ensemble de données, qui à son tour est fonction du déséquilibre potentiel de  l’étiquette et des caractéristiques, de la corrélation des caractéristiques potentiellement sensibles  avec d’autres caractéristiques des données, du décalage de la distribution de l’apprentissage vers  le phase de développement, etc. Sans l’évaluation des modèles d’atténuation des biais dans  diverses configurations difficiles, leurs mérites restent incertains. Par conséquent, une analyse  systématique qui comparerait différentes approches d’atténuation des biais sous la perspective de  différentes mesures d’équité pour assurer la réplication des résultats conclus est nécessaire. À  cette fin, nous proposons un cadre unifié pour comparer les approches d’atténuation des biais.  Nous évaluons différentes méthodes d’équité formées avec des réseaux de neurones profonds sur  un ensemble de données synthétiques commun et un ensemble de données du monde réel pour  obtenir de meilleures informations sur le fonctionnement de ces méthodes. En particulier, nous  formons environ 3000 modèles différents dans diverses configurations, y compris des configurations  de données déséquilibrées et corrélées, pour vérifier les limites des modèles actuels et mieux  comprendre dans quelles configurations ils sont sujets à des défaillances. Nos résultats montrent que  le biais des modèles augmente à mesure que les ensembles de données deviennent plus déséquilibrés   ou que les attributs des ensembles de données deviennent plus corrélés, le niveau de dominance  des caractéristiques des ensembles de données sensibles corrélées a un impact sur le biais, et  les informations sensibles restent dans la représentation latente même lorsque des algorithmes  d’atténuation des biais sont appliqués. Résumant nos contributions - nous présentons un ensemble  de données, proposons diverses configurations d’évaluation difficiles et évaluons rigoureusement  les récents algorithmes prometteurs d’atténuation des biais dans un cadre commun et publions  publiquement cette référence, en espérant que la communauté des chercheurs le considérerait  comme un point d’entrée commun pour un apprentissage en profondeur équitable.},
  langid = {english},
  annotation = {Accepted: 2023-02-09T21:00:44Z},
  file = {C:\Users\wenji\Zotero\storage\73ZM5R32\Reddy - 2022 - Benchmarking bias mitigation algorithms in represe.pdf}
}

@article{reddyBenchmarkingBiasMitigation2021,
  title = {Benchmarking {{Bias Mitigation Algorithms}} in {{Representation Learning}} through {{Fairness Metrics}}},
  author = {Reddy, Charan and Sharma, Deepak and Mehri, Soroush and Romero Soriano, Adriana and Shabanian, Samira and Honari, Sina},
  date = {2021-12-06},
  journaltitle = {Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks},
  volume = {1},
  url = {https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/hash/2723d092b63885e0d7c260cc007e8b9d-Abstract-round1.html},
  urldate = {2024-05-20},
  langid = {english},
  file = {C:\Users\wenji\Zotero\storage\AELCLHU6\Reddy 等 - 2021 - Benchmarking Bias Mitigation Algorithms in Represe.pdf}
}

@inproceedings{creagerFlexiblyFairRepresentation2019,
  title = {Flexibly {{Fair Representation Learning}} by {{Disentanglement}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Creager, Elliot and Madras, David and Jacobsen, Joern-Henrik and Weis, Marissa and Swersky, Kevin and Pitassi, Toniann and Zemel, Richard},
  date = {2019-05-24},
  pages = {1436--1445},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v97/creager19a.html},
  urldate = {2024-05-20},
  abstract = {We consider the problem of learning representations that achieve group and subgroup fairness with respect to multiple sensitive attributes. Taking inspiration from the disentangled representation learning literature, we propose an algorithm for learning compact representations of datasets that are useful for reconstruction and prediction, but are also flexibly fair, meaning they can be easily modified at test time to achieve subgroup demographic parity with respect to multiple sensitive attributes and their conjunctions. We show empirically that the resulting encoder—which does not require the sensitive attributes for inference—allows for the adaptation of a single representation to a variety of fair classification tasks with new target labels and subgroup definitions.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\7EWQHUEE\\Creager 等 - 2019 - Flexibly Fair Representation Learning by Disentang.pdf;C\:\\Users\\wenji\\Zotero\\storage\\C7CCHFEN\\Creager 等 - 2019 - Flexibly Fair Representation Learning by Disentang.pdf}
}

@inproceedings{naDeepGenerativePositiveUnlabeled2020,
  title = {Deep {{Generative Positive-Unlabeled Learning}} under {{Selection Bias}}},
  booktitle = {Proceedings of the 29th {{ACM International Conference}} on {{Information}} \& {{Knowledge Management}}},
  author = {Na, Byeonghu and Kim, Hyemi and Song, Kyungwoo and Joo, Weonyoung and Kim, Yoon-Yeong and Moon, Il-Chul},
  date = {2020-10-19},
  series = {{{CIKM}} '20},
  pages = {1155--1164},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3340531.3411971},
  url = {https://dl.acm.org/doi/10.1145/3340531.3411971},
  urldate = {2024-05-21},
  abstract = {Learning in the positive-unlabeled (PU) setting is prevalent in real world applications. Many previous works depend upon theSelected Completely At Random (SCAR) assumption to utilize unlabeled data, but the SCAR assumption is not often applicable to the real world due to selection bias in label observations. This paper is the first generative PU learning model without the SCAR assumption. Specifically, we derive the PU risk function without the SCAR assumption, and we generate a set of virtual PU examples to train the classifier. Although our PU risk function is more generalizable, the function requires PU instances that do not exist in the observations. Therefore, we introduce the VAE-PU, which is a variant of variational autoencoders to separate two latent variables that generate either features or observation indicators. The separated latent information enables the model to generate virtual PU instances. We test the VAE-PU on benchmark datasets with and without the SCAR assumption. The results indicate that the VAE-PU is superior when selection bias exists, and the VAE-PU is also competent under the SCAR assumption. The results also emphasize that the VAE-PU is effective when there are few positive-labeled instances due to modeling on selection bias.},
  isbn = {978-1-4503-6859-9},
  keywords = {positive-unlabeled learning,selection bias,variational autoencoders},
  file = {C:\Users\wenji\Zotero\storage\4X9WJ4QE\Na 等 - 2020 - Deep Generative Positive-Unlabeled Learning under .pdf}
}

@online{asgaritaghanakiJigsawVAEBalancingFeatures2020,
  title = {Jigsaw-{{VAE}}: {{Towards Balancing Features}} in {{Variational Autoencoders}}},
  shorttitle = {Jigsaw-{{VAE}}},
  author = {Asgari Taghanaki, Saeid and Havaei, Mohammad and Lamb, Alex and Sanghi, Aditya and Danielyan, Ara and Custis, Tonya},
  date = {2020-05-01},
  doi = {10.48550/arXiv.2005.05496},
  url = {https://ui.adsabs.harvard.edu/abs/2020arXiv200505496A},
  urldate = {2024-05-21},
  abstract = {The latent variables learned by VAEs have seen considerable interest as an unsupervised way of extracting features, which can then be used for downstream tasks. There is a growing interest in the question of whether features learned on one environment will generalize across different environments. We demonstrate here that VAE latent variables often focus on some factors of variation at the expense of others - in this case we refer to the features as ``imbalanced''. Feature imbalance leads to poor generalization when the latent variables are used in an environment where the presence of features changes. Similarly, latent variables trained with imbalanced features induce the VAE to generate less diverse (i.e. biased towards dominant features) samples. To address this, we propose a regularization scheme for VAEs, which we show substantially addresses the feature imbalance problem. We also introduce a simple metric to measure the balance of features in generated images.},
  organization = {arXiv e-prints},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {ADS Bibcode: 2020arXiv200505496A},
  file = {C:\Users\wenji\Zotero\storage\L7GSA85H\Asgari Taghanaki 等 - 2020 - Jigsaw-VAE Towards Balancing Features in Variatio.pdf}
}

@article{chatterjeeVariationalAutoencoderBased2023,
  title = {Variational {{Autoencoder Based Imbalanced COVID-19 Detection Using Chest X-Ray Images}}},
  author = {Chatterjee, Sankhadeep and Maity, Soumyajit and Bhattacharjee, Mayukh and Banerjee, Soumen and Das, Asit Kumar and Ding, Weiping},
  date = {2023-03-01},
  journaltitle = {New Generation Computing},
  shortjournal = {New Gener. Comput.},
  volume = {41},
  number = {1},
  pages = {25--60},
  issn = {1882-7055},
  doi = {10.1007/s00354-022-00194-y},
  url = {https://doi.org/10.1007/s00354-022-00194-y},
  urldate = {2024-05-21},
  abstract = {Early and fast detection of disease is essential for the fight against COVID-19 pandemic. Researchers have focused on developing robust and cost-effective detection methods using Deep learning based chest X-Ray image processing. However, such prediction models are often not well suited to address the challenge of highly imabalanced datasets. The current work is an attempt to address the issue by utilizing unsupervised Variational Auto Encoders (VAEs). Firstly, chest X-Ray images are converted to a latent space by learning the most important features using VAEs. Secondly, a wide range of well established data resampling techniques are used to balance the preexisting imbalanced classes in the latent vector form of the dataset. Finally, the modified dataset in the new feature space is used to train well known classification models to classify chest X-Ray images into three different classes viz., ”COVID-19”, ”Pneumonia”, and ”Normal”. In order to capture the quality of resampling methods, 10-folds cross validation technique is applied on the dataset. Extensive experimental analysis have been carried out and results so obtained indicate significant improvement in COVID-19 detection using the proposed VAE based method. Furthermore, the ingenuity of the results have been established by performing Wilcoxon rank test with 95\% level of significance.},
  langid = {english},
  keywords = {Class imbalance,COVID-19,Oversampling,Undersampling,Variational autoencoder},
  file = {C:\Users\wenji\Zotero\storage\9P8DKPJ3\Chatterjee 等 - 2023 - Variational Autoencoder Based Imbalanced COVID-19 .pdf}
}

@inproceedings{seiffertHybridSamplingImbalanced2008,
  title = {Hybrid Sampling for Imbalanced Data},
  booktitle = {2008 {{IEEE International Conference}} on {{Information Reuse}} and {{Integration}}},
  author = {Seiffert, Chris and Khoshgoftaar, Taghi M. and Van Hulse, Jason},
  date = {2008},
  pages = {202--207},
  publisher = {IEEE},
  location = {Las Vegas, NV, USA},
  doi = {10.1109/IRI.2008.4583030},
  url = {http://ieeexplore.ieee.org/document/4583030/},
  urldate = {2024-05-21},
  eventtitle = {2008 {{IEEE International Conference}} on {{Information Reuse}} and {{Integration}}},
  isbn = {978-1-4244-2659-1},
  file = {C:\Users\wenji\Zotero\storage\HH9YDGQK\Seiffert 等 - 2008 - Hybrid sampling for imbalanced data.pdf}
}

@online{zhangDeterminantalPointProcesses2017,
  title = {Determinantal {{Point Processes}} for {{Mini-Batch Diversification}}},
  author = {Zhang, Cheng and Kjellstrom, Hedvig and Mandt, Stephan},
  date = {2017-08-23},
  eprint = {1705.00607},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1705.00607},
  url = {http://arxiv.org/abs/1705.00607},
  urldate = {2024-05-21},
  abstract = {We study a mini-batch diversification scheme for stochastic gradient descent (SGD). While classical SGD relies on uniformly sampling data points to form a mini-batch, we propose a non-uniform sampling scheme based on the Determinantal Point Process (DPP). The DPP relies on a similarity measure between data points and gives low probabilities to mini-batches which contain redundant data, and higher probabilities to mini-batches with more diverse data. This simultaneously balances the data and leads to stochastic gradients with lower variance. We term this approach Diversified Mini-Batch SGD (DM-SGD). We show that regular SGD and a biased version of stratified sampling emerge as special cases. Furthermore, DM-SGD generalizes stratified sampling to cases where no discrete features exist to bin the data into groups. We show experimentally that our method results more interpretable and diverse features in unsupervised setups, and in better classification accuracies in supervised setups.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\2L2PMR79\\Zhang 等 - 2017 - Determinantal Point Processes for Mini-Batch Diver.pdf;C\:\\Users\\wenji\\Zotero\\storage\\P8BBAG9D\\1705.html}
}

@inproceedings{kuleszaKdppsFixedsizeDeterminantal2011,
  title = {K-Dpps: {{Fixed-size}} Determinantal Point Processes},
  author = {Kulesza, Alex and Taskar, Ben},
  date = {2011},
  pages = {1193--1200},
  eventtitle = {Proceedings of the 28th {{International Conference}} on {{Machine Learning}} ({{ICML-11}})}
}

@inproceedings{kuleszaKDPPsFixedsizeDeterminantal2011a,
  title = {K-{{DPPs}}: Fixed-Size Determinantal Point Processes},
  shorttitle = {K-{{DPPs}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{International Conference}} on {{Machine Learning}}},
  author = {Kulesza, Alex and Taskar, Ben},
  date = {2011-06-28},
  series = {{{ICML}}'11},
  pages = {1193--1200},
  publisher = {Omnipress},
  location = {Madison, WI, USA},
  abstract = {Determinantal point processes (DPPs) have recently been proposed as models for set selection problems where diversity is preferred. For example, they can be used to select diverse sets of sentences to form document summaries, or to find multiple non-overlapping human poses in an image. However, DPPs conflate the modeling of two distinct characteristics: the size of the set, and its content. For many realistic tasks, the size of the desired set is known up front; e.g., in search we may want to show the user exactly ten results. In these situations the effort spent by DPPs modeling set size is not only wasteful, but actually introduces unwanted bias into the modeling of content. Instead, we propose the k-DPP, a conditional DPP that models only sets of cardinality k. In exchange for this restriction, k-DPPs offer greater expressiveness and control over content, and simplified integration into applications like search. We derive algorithms for efficiently normalizing, sampling, and marginalizing k-DPPs, and propose an experts-style algorithm for learning combinations of k-DPPs. We demonstrate the usefulness of the model on an image search task, where k-DPPs significantly outperform MMR as judged by human annotators.},
  isbn = {978-1-4503-0619-5},
  file = {C:\Users\wenji\Zotero\storage\YNXSPBEW\Kulesza 和 Taskar - 2011 - k-DPPs fixed-size determinantal point processes.pdf}
}

@article{kuleszaDeterminantalPointProcesses2012,
  title = {Determinantal Point Processes for Machine Learning},
  author = {Kulesza, Alex and Taskar, Ben},
  date = {2012},
  journaltitle = {Foundations and Trends® in Machine Learning},
  shortjournal = {Foundations and Trends® in Machine Learning},
  volume = {5},
  number = {2–3},
  pages = {123--286},
  publisher = {Now Publishers, Inc.},
  issn = {1935-8237},
  file = {C:\Users\wenji\Zotero\storage\3FD4TDYW\Kulesza 和 Taskar - 2012 - Determinantal point processes for machine learning.pdf}
}

@inproceedings{elfekiGdppLearningDiverse2019,
  title = {Gdpp: {{Learning}} Diverse Generations Using Determinantal Point Processes},
  author = {Elfeki, Mohamed and Couprie, Camille and Riviere, Morgane and Elhoseiny, Mohamed},
  date = {2019},
  pages = {1774--1783},
  publisher = {PMLR},
  eventtitle = {International Conference on Machine Learning},
  isbn = {2640-3498}
}

@inproceedings{elfekiGDPPLearningDiverse2019a,
  title = {{{GDPP}}: {{Learning Diverse Generations}} Using {{Determinantal Point Processes}}},
  shorttitle = {{{GDPP}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Elfeki, Mohamed and Couprie, Camille and Riviere, Morgane and Elhoseiny, Mohamed},
  date = {2019-05-24},
  pages = {1774--1783},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v97/elfeki19a.html},
  urldate = {2024-05-21},
  abstract = {Generative models have proven to be an outstanding tool for representing high-dimensional probability distributions and generating realistic looking images. An essential characteristic of generative models is their ability to produce multi-modal outputs. However, while training, they are often susceptible to mode collapse, that is models are limited in mapping input noise to only a few modes of the true data distribution. In this work, we draw inspiration from Determinantal Point Process (DPP) to propose an unsupervised penalty loss that alleviates mode collapse while producing higher quality samples. DPP is an elegant probabilistic measure used to model negative correlations within a subset and hence quantify its diversity. We use DPP kernel to model the diversity in real data as well as in synthetic data. Then, we devise an objective term that encourages generator to synthesize data with a similar diversity to real data. In contrast to previous state-of-the-art generative models that tend to use additional trainable parameters or complex training paradigms, our method does not change the original training scheme. Embedded in an adversarial training and variational autoencoder, our Generative DPP approach shows a consistent resistance to mode-collapse on a wide-variety of synthetic data and natural image datasets including MNIST, CIFAR10, and CelebA, while outperforming state-of-the-art methods for data-efficiency, generation quality, and convergence-time whereas being 5.8x faster than its closest competitor.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\BYT4C2YU\\Elfeki 等 - 2019 - GDPP Learning Diverse Generations using Determina.pdf;C\:\\Users\\wenji\\Zotero\\storage\\H9Q2BTN3\\Elfeki 等 - 2019 - GDPP Learning Diverse Generations using Determina.pdf}
}

@online{zhaoAcceleratingMinibatchStochastic2014,
  title = {Accelerating {{Minibatch Stochastic Gradient Descent}} Using {{Stratified Sampling}}},
  author = {Zhao, Peilin and Zhang, Tong},
  date = {2014-05-13},
  eprint = {1405.3080},
  eprinttype = {arXiv},
  eprintclass = {cs, math, stat},
  doi = {10.48550/arXiv.1405.3080},
  url = {http://arxiv.org/abs/1405.3080},
  urldate = {2024-05-22},
  abstract = {Stochastic Gradient Descent (SGD) is a popular optimization method which has been applied to many important machine learning tasks such as Support Vector Machines and Deep Neural Networks. In order to parallelize SGD, minibatch training is often employed. The standard approach is to uniformly sample a minibatch at each step, which often leads to high variance. In this paper we propose a stratified sampling strategy, which divides the whole dataset into clusters with low within-cluster variance; we then take examples from these clusters using a stratified sampling technique. It is shown that the convergence rate can be significantly improved by the algorithm. Encouraging experimental results confirm the effectiveness of the proposed method.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\DJGN9JA5\\Zhao 和 Zhang - 2014 - Accelerating Minibatch Stochastic Gradient Descent.pdf;C\:\\Users\\wenji\\Zotero\\storage\\PDSJJ866\\1405.html}
}

@article{xuComprehensiveSurveyImage2023,
  title = {A {{Comprehensive Survey}} of {{Image Augmentation Techniques}} for {{Deep Learning}}},
  author = {Xu, Mingle and Yoon, Sook and Fuentes, Alvaro and Park, Dong Sun},
  date = {2023-05-01},
  journaltitle = {Pattern Recognition},
  shortjournal = {Pattern Recognition},
  volume = {137},
  pages = {109347},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2023.109347},
  url = {https://www.sciencedirect.com/science/article/pii/S0031320323000481},
  urldate = {2024-05-22},
  abstract = {Although deep learning has achieved satisfactory performance in computer vision, a large volume of images is required. However, collecting images is often expensive and challenging. Many image augmentation algorithms have been proposed to alleviate this issue. Understanding existing algorithms is, therefore, essential for finding suitable and developing novel methods for a given task. In this study, we perform a comprehensive survey of image augmentation for deep learning using a novel informative taxonomy. To examine the basic objective of image augmentation, we introduce challenges in computer vision tasks and vicinity distribution. The algorithms are then classified among three categories: model-free, model-based, and optimizing policy-based. The model-free category employs the methods from image processing, whereas the model-based approach leverages image generation models to synthesize images. In contrast, the optimizing policy-based approach aims to find an optimal combination of operations. Based on this analysis, we believe that our survey enhances the understanding necessary for choosing suitable methods and designing novel algorithms.},
  keywords = {Computer vision,Data augmentation,Deep learning,Image augmentation,Image variation,Vicinity distribution},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\LEMHXSGK\\Xu 等 - 2023 - A Comprehensive Survey of Image Augmentation Techn.pdf;C\:\\Users\\wenji\\Zotero\\storage\\D4P49QHU\\S0031320323000481.html}
}

@article{douzasEffectiveDataGeneration2018,
  title = {Effective Data Generation for Imbalanced Learning Using Conditional Generative Adversarial Networks},
  author = {Douzas, Georgios and Bacao, Fernando},
  date = {2018-01-01},
  journaltitle = {Expert Systems with Applications},
  shortjournal = {Expert Systems with Applications},
  volume = {91},
  pages = {464--471},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2017.09.030},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417417306346},
  urldate = {2024-05-22},
  abstract = {Learning from imbalanced datasets is a frequent but challenging task for standard classification algorithms. Although there are different strategies to address this problem, methods that generate artificial data for the minority class constitute a more general approach compared to algorithmic modifications. Standard oversampling methods are variations of the SMOTE algorithm, which generates synthetic samples along the line segment that joins minority class samples. Therefore, these approaches are based on local information, rather on the overall minority class distribution. Contrary to these algorithms, in this paper the conditional version of Generative Adversarial Networks (cGAN) is used to approximate the true data distribution and generate data for the minority class of various imbalanced datasets. The performance of cGAN is compared against multiple standard oversampling algorithms. We present empirical results that show a significant improvement in the quality of the generated data when cGAN is used as an oversampling algorithm.},
  keywords = {Artificial data,GAN,Imbalanced learning,Minority class},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\F5TDFJUX\\Douzas 和 Bacao - 2018 - Effective data generation for imbalanced learning .pdf;C\:\\Users\\wenji\\Zotero\\storage\\LMUTZCXX\\S0957417417306346.html}
}

@article{xuOverviewDeepGenerative2015,
  title = {An {{Overview}} of {{Deep Generative Models}}},
  author = {Xu, Jungang and Li, Hui and Zhou, Shilong},
  date = {2015-03-04},
  journaltitle = {IETE Technical Review},
  volume = {32},
  number = {2},
  pages = {131--139},
  publisher = {Taylor \& Francis},
  issn = {0256-4602},
  doi = {10.1080/02564602.2014.987328},
  url = {https://doi.org/10.1080/02564602.2014.987328},
  urldate = {2024-05-22},
  abstract = {As an important category of deep models, deep generative model has attracted more and more attention with the proposal of Deep Belief Networks (DBNs) and the fast greedy training algorithm based on restricted Boltzmann machines (RBMs). In the past few years, many different deep generative models are proposed and used in the area of Artificial Intelligence. In this paper, three important deep generative models including DBNs, deep autoencoder, and deep Boltzmann machine are reviewed. In addition, some successful applications of deep generative models in image processing, speech recognition and information retrieval are also introduced and analysed.},
  keywords = {Deep autoencoder,Deep belief networks,Deep boltzmann machine,Deep generative model,Restricted boltzmann machine},
  file = {C:\Users\wenji\Zotero\storage\6I8TQJLF\Xu 等 - 2015 - An Overview of Deep Generative Models.pdf}
}

@inproceedings{zhangMixupEmpiricalRisk2018,
  title = {Mixup: {{Beyond Empirical Risk Minimization}}},
  shorttitle = {Mixup},
  author = {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N. and Lopez-Paz, David},
  date = {2018-02-15},
  url = {https://openreview.net/forum?id=r1Ddp1-Rb},
  urldate = {2024-05-22},
  abstract = {Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {C:\Users\wenji\Zotero\storage\HX6R2QK6\Zhang 等 - 2018 - mixup Beyond Empirical Risk Minimization.pdf}
}

@article{wangImageQualityAssessment2004,
  title = {Image Quality Assessment: From Error Visibility to Structural Similarity},
  shorttitle = {Image Quality Assessment},
  author = {Wang, Zhou and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
  date = {2004-04},
  journaltitle = {IEEE Transactions on Image Processing},
  volume = {13},
  number = {4},
  pages = {600--612},
  issn = {1941-0042},
  doi = {10.1109/TIP.2003.819861},
  url = {https://ieeexplore.ieee.org/document/1284395},
  urldate = {2024-05-23},
  abstract = {Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a structural similarity index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000. A MATLAB implementation of the proposed algorithm is available online at http://www.cns.nyu.edu//spl sim/lcv/ssim/.},
  eventtitle = {{{IEEE Transactions}} on {{Image Processing}}},
  keywords = {Data mining,Degradation,Humans,Image quality,Indexes,Layout,Quality assessment,Transform coding,Visual perception,Visual system},
  file = {C:\Users\wenji\Zotero\storage\6VJSDXWF\1284395.html}
}

@article{wangImageQualityAssessment2004a,
  title = {Image Quality Assessment: From Error Visibility to Structural Similarity},
  shorttitle = {Image Quality Assessment},
  author = {Wang, Zhou and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
  date = {2004-04},
  journaltitle = {IEEE Transactions on Image Processing},
  volume = {13},
  number = {4},
  pages = {600--612},
  issn = {1941-0042},
  doi = {10.1109/TIP.2003.819861},
  url = {https://ieeexplore.ieee.org/document/1284395},
  urldate = {2024-05-23},
  abstract = {Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a structural similarity index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000. A MATLAB implementation of the proposed algorithm is available online at http://www.cns.nyu.edu//spl sim/lcv/ssim/.},
  eventtitle = {{{IEEE Transactions}} on {{Image Processing}}},
  keywords = {Data mining,Degradation,Humans,Image quality,Indexes,Layout,Quality assessment,Transform coding,Visual perception,Visual system},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\ZXLXWBAV\\Wang 等 - 2004 - Image quality assessment from error visibility to.pdf;C\:\\Users\\wenji\\Zotero\\storage\\P2TBWGP8\\1284395.html}
}

@article{ranaDataAugmentationImproved2022,
  title = {Data Augmentation with Improved Regularisation and Sampling for Imbalanced Blood Cell Image Classification},
  author = {Rana, Priyanka and Sowmya, Arcot and Meijering, Erik and Song, Yang},
  date = {2022-10-27},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {12},
  number = {1},
  pages = {18101},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-022-22882-x},
  url = {https://www.nature.com/articles/s41598-022-22882-x},
  urldate = {2024-05-25},
  abstract = {Due to progression in cell-cycle or duration of storage, classification of morphological changes in human blood cells is important for correct and effective clinical decisions. Automated classification systems help avoid subjective outcomes and are more efficient. Deep learning and more specifically Convolutional Neural Networks have achieved state-of-the-art performance on various biomedical image classification problems. However, real-world data often suffers from the data imbalance problem, owing to which the trained classifier is biased towards the majority classes and does not perform well on the minority classes. This study presents an imbalanced blood cells classification method that utilises Wasserstein divergence GAN, mixup and novel nonlinear mixup for data augmentation to achieve oversampling of the minority classes. We also present a minority class focussed sampling strategy, which allows effective representation of minority class samples produced by all three data augmentation techniques and contributes to the classification performance. The method was evaluated on two publicly available datasets of immortalised human T-lymphocyte cells and Red Blood Cells. Classification performance evaluated using F1-score shows that our proposed approach outperforms existing methods on the same datasets.},
  langid = {english},
  keywords = {Biomedical engineering,Computer science},
  file = {C:\Users\wenji\Zotero\storage\6NYEDMTQ\Rana 等 - 2022 - Data augmentation with improved regularisation and.pdf}
}

@article{houthuysTensorbasedRestrictedKernel2021,
  title = {Tensor-Based Restricted Kernel Machines for Multi-View Classification},
  author = {Houthuys, Lynn and Suykens, Johan A. K.},
  date = {2021-04-01},
  journaltitle = {Information Fusion},
  shortjournal = {Information Fusion},
  volume = {68},
  pages = {54--66},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2020.10.022},
  url = {https://www.sciencedirect.com/science/article/pii/S156625352030405X},
  urldate = {2024-05-25},
  abstract = {Multi-view learning deals with data that is described through multiple representations, or views. While various real-world data can be represented by three or more views, several existing multi-view classification methods can only handle two views. Previously proposed methods usually solve this issue by optimizing pairwise combinations of views. Although this can numerically deal with the issue of multiple views, it ignores the higher order correlations which can only be examined by exploring all views simultaneously. In this work new multi-view classification approaches are introduced which aim to include higher order statistics when three or more views are available. The proposed model is an extension to the recently proposed Restricted Kernel Machine classifier model and assumes shared hidden features for all views, as well as a newly introduced model tensor. Experimental results show an improvement with respect to state-of-the art pairwise multi-view learning methods, both in terms of classification accuracy and runtime.},
  keywords = {Kernel-based learning,Multi-view learning,Tensor},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\RJCJCUFU\\Houthuys 和 Suykens - 2021 - Tensor-based restricted kernel machines for multi-.pdf;C\:\\Users\\wenji\\Zotero\\storage\\RGTR2DW7\\S156625352030405X.html}
}

@article{yenClusterbasedUndersamplingApproaches2009,
  title = {Cluster-Based under-Sampling Approaches for Imbalanced Data Distributions},
  author = {Yen, Show-Jane and Lee, Yue-Shi},
  date = {2009-04-01},
  journaltitle = {Expert Systems with Applications},
  shortjournal = {Expert Systems with Applications},
  volume = {36},
  pages = {5718--5727},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2008.06.108},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417408003527},
  urldate = {2024-05-26},
  abstract = {For classification problem, the training data will significantly influence the classification accuracy. However, the data in real-world applications often are imbalanced class distribution, that is, most of the data are in majority class and little data are in minority class. In this case, if all the data are used to be the training data, the classifier tends to predict that most of the incoming data belongs to the majority class. Hence, it is important to select the suitable training data for classification in the imbalanced class distribution problem. In this paper, we propose cluster-based under-sampling approaches for selecting the representative data as training data to improve the classification accuracy for minority class and investigate the effect of under-sampling methods in the imbalanced class distribution environment. The experimental results show that our cluster-based under-sampling approaches outperform the other under-sampling techniques in the previous studies.},
  issue = {3, Part 1},
  keywords = {Classification,Data mining,Imbalanced data distribution,Under-sampling},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\YHU6KFCT\\Yen 和 Lee - 2009 - Cluster-based under-sampling approaches for imbala.pdf;C\:\\Users\\wenji\\Zotero\\storage\\Y47E75ZF\\S0957417408003527.html}
}

@inproceedings{zhangClusterbasedMajorityUndersampling2010,
  title = {Cluster-Based Majority under-Sampling Approaches for Class Imbalance Learning},
  booktitle = {2010 2nd {{IEEE International Conference}} on {{Information}} and {{Financial Engineering}}},
  author = {Zhang, Yan-Ping and Zhang, Li-Na and Wang, Yong-Cheng},
  date = {2010-09},
  pages = {400--404},
  doi = {10.1109/ICIFE.2010.5609385},
  url = {https://ieeexplore.ieee.org/abstract/document/5609385?casa_token=dNvNi7npstoAAAAA:Vawg7HdewuxXr8ckMz9UDx4n5ZokTY1KOZJgMTRTMTUEanVM1AVd27yZC5_bn8PrZziPacf2-4M},
  urldate = {2024-05-26},
  abstract = {The class imbalance problem usually occurs in real applications. The class imbalance is that the amount of one class may be much less than that of another in training set. Under-sampling is a very popular approach to deal with this problem. Under-sampling approach is very efficient, it only using a subset of the majority class. The drawback of under-sampling is that it throws away many potentially useful majority class examples. To overcome this drawback, we adopt an unsupervised learning technique for supervised learning. We proposes cluster-based majority under-sampling approaches for selecting a representative subset from the majority class. Compared to under-sampling, cluster-based under-sampling can effectively avoid the important information loss of majority class. We adopt two methods to select representative subset from k clusters with certain proportions, and then use the representative subset and the all minority class samples as training data to improve accuracy over minority and majority classes. In the paper, we compared the behaviors of our approaches with the traditional random under-sampling approach on ten UCI repository datasets using the following classifiers: k-nearest neighbor and Naïve Bayes classifier. Recall, Precision, F-measure, G-mean and BACC (balance accuracy) are used for evaluating performance of classifiers. Experimental results show that our cluster-based majority under-sampling approaches outperform the random under-sampling approach. Our approaches attain better overall performance on k-nearest neighbor classifier compared to Naïve Bayes classifier.},
  eventtitle = {2010 2nd {{IEEE International Conference}} on {{Information}} and {{Financial Engineering}}},
  keywords = {Accuracy,class imbalance learning,classification,Classification algorithms,clustering,Conferences,Data mining,Learning,Machine learning,Training,under-sampling},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\S66UZJGH\\Zhang 等 - 2010 - Cluster-based majority under-sampling approaches f.pdf;C\:\\Users\\wenji\\Zotero\\storage\\YYGK8W7N\\5609385.html}
}

@inproceedings{yanRebalancingVariationalAutoencoder2020,
  title = {Re-Balancing {{Variational Autoencoder Loss}} for {{Molecule Sequence Generation}}},
  booktitle = {Proceedings of the 11th {{ACM International Conference}} on {{Bioinformatics}}, {{Computational Biology}} and {{Health Informatics}}},
  author = {Yan, Chaochao and Wang, Sheng and Yang, Jinyu and Xu, Tingyang and Huang, Junzhou},
  date = {2020-11-10},
  series = {{{BCB}} '20},
  pages = {1--7},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3388440.3412458},
  url = {https://dl.acm.org/doi/10.1145/3388440.3412458},
  urldate = {2024-05-28},
  abstract = {Molecule generation is to design new molecules with specific chemical properties and further to optimize the desired chemical properties. Following previous work, we encode molecules into continuous vectors in the latent space and then decode the embedding vectors into molecules under the variational autoencoder (VAE) framework. We investigate the posterior collapse problem of the current widely-used RNN-based VAEs for the molecule sequence generation. For the first time, we point out that the underestimated reconstruction loss of VAEs leads to the posterior collapse, and we also provide both analytical and experimental evidences to support our findings. To fix the problem and avoid the posterior collapse, we propose an effective and efficient solution in this work. Without bells and whistles, our method achieves the state-of-the-art reconstruction accuracy and competitive validity score on the ZINC 250K dataset. When generating 10,000 unique valid molecule sequences from the random prior sampling, it costs the JT-VAE 1450 seconds while our method only needs 9 seconds on a regular desktop machine.},
  isbn = {978-1-4503-7964-9},
  keywords = {molecule generation,posterior collapse,recurrent neural networks,variational autoencoder},
  file = {C:\Users\wenji\Zotero\storage\7ZKF42LI\Yan 等 - 2020 - Re-balancing Variational Autoencoder Loss for Mole.pdf}
}

@incollection{vermaHandlingUnbalancedData2022,
  title = {Handling {{Unbalanced Data}} in {{Clinical Images}}},
  booktitle = {Advanced {{Healthcare Systems}}},
  author = {Verma, Amit},
  date = {2022},
  pages = {69--79},
  publisher = {John Wiley \& Sons, Ltd},
  doi = {10.1002/9781119769293.ch6},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119769293.ch6},
  urldate = {2024-05-28},
  abstract = {Manual detection of abnormalities accurately in the clinical images like MRIs by the operators is tedious work that requires good experience and knowledge, specifically manually segmenting the brain tumor in the MRI for further diagnosis by the doctor. So, multiple automatic and semi-automatic approaches were developed to automate the process of segmenting the malignant area of the tumor. The major problem which arises to train the model for automatic segmentation of clinical images is the imbalanced data set. An imbalanced clinical data set means the healthy tissues are always far greater than the cancerous tissues. This difference between the majority and minority data in the data sets reduces or adversely affects the accuracy of predicting model due to biased training data sets. So, it becomes a major concern for the various researchers to balance the data before using it to train a particular prediction model, and various data-level and algorithm-levelbased approaches were developed to balance the imbalance data for improving the accuracy of the trained model. In this chapter, the concept and problem of imbalanced data are discussed and various approaches for balancing the data are also highlighted in which one of the state-of-the-art method bagging is discussed in detail.},
  isbn = {978-1-119-76929-3},
  langid = {english},
  keywords = {B agging,boosting,brain tumor,deep learning,medical,MRI,unbalanced data},
  file = {C:\Users\wenji\Zotero\storage\Z34YC9IP\9781119769293.html}
}

@inproceedings{zhangClusterbasedMajorityUndersampling2010a,
  title = {Cluster-Based Majority under-Sampling Approaches for Class Imbalance Learning},
  booktitle = {2010 2nd {{IEEE International Conference}} on {{Information}} and {{Financial Engineering}}},
  author = {Zhang, Yan-Ping and Zhang, Li-Na and Wang, Yong-Cheng},
  date = {2010-09},
  pages = {400--404},
  doi = {10.1109/ICIFE.2010.5609385},
  url = {https://ieeexplore.ieee.org/abstract/document/5609385?casa_token=myVDTqrU0J0AAAAA:BkGSCr2MHIks12AplIdLVj1hkZI8Usu0YO_46BoPIsQt6Aay4yjo4twIJ1nUkf2RBSZ0IgHWhG8},
  urldate = {2024-05-29},
  abstract = {The class imbalance problem usually occurs in real applications. The class imbalance is that the amount of one class may be much less than that of another in training set. Under-sampling is a very popular approach to deal with this problem. Under-sampling approach is very efficient, it only using a subset of the majority class. The drawback of under-sampling is that it throws away many potentially useful majority class examples. To overcome this drawback, we adopt an unsupervised learning technique for supervised learning. We proposes cluster-based majority under-sampling approaches for selecting a representative subset from the majority class. Compared to under-sampling, cluster-based under-sampling can effectively avoid the important information loss of majority class. We adopt two methods to select representative subset from k clusters with certain proportions, and then use the representative subset and the all minority class samples as training data to improve accuracy over minority and majority classes. In the paper, we compared the behaviors of our approaches with the traditional random under-sampling approach on ten UCI repository datasets using the following classifiers: k-nearest neighbor and Naïve Bayes classifier. Recall, Precision, F-measure, G-mean and BACC (balance accuracy) are used for evaluating performance of classifiers. Experimental results show that our cluster-based majority under-sampling approaches outperform the random under-sampling approach. Our approaches attain better overall performance on k-nearest neighbor classifier compared to Naïve Bayes classifier.},
  eventtitle = {2010 2nd {{IEEE International Conference}} on {{Information}} and {{Financial Engineering}}},
  keywords = {Accuracy,class imbalance learning,classification,Classification algorithms,clustering,Conferences,Data mining,Learning,Machine learning,Training,under-sampling},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\YAELGR5L\\Zhang 等 - 2010 - Cluster-based majority under-sampling approaches f.pdf;C\:\\Users\\wenji\\Zotero\\storage\\KEKQPYRP\\5609385.html}
}

@article{citovskyBatchActiveLearning2021,
  title = {Batch Active Learning at Scale},
  author = {Citovsky, Gui and DeSalvo, Giulia and Gentile, Claudio and Karydas, Lazaros and Rajagopalan, Anand and Rostamizadeh, Afshin and Kumar, Sanjiv},
  date = {2021},
  journaltitle = {Advances in Neural Information Processing Systems},
  shortjournal = {Advances in Neural Information Processing Systems},
  volume = {34},
  pages = {11933--11944}
}

@inproceedings{citovskyBatchActiveLearning2021a,
  title = {Batch {{Active Learning}} at {{Scale}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Citovsky, Gui and DeSalvo, Giulia and Gentile, Claudio and Karydas, Lazaros and Rajagopalan, Anand and Rostamizadeh, Afshin and Kumar, Sanjiv},
  date = {2021},
  volume = {34},
  pages = {11933--11944},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper_files/paper/2021/hash/64254db8396e404d9223914a0bd355d2-Abstract.html},
  urldate = {2024-05-29},
  abstract = {The ability to train complex and highly effective models often requires an abundance of training data, which can easily become a bottleneck in cost, time, and computational resources. Batch active learning, which adaptively issues batched queries to a labeling oracle, is a common approach for addressing this problem. The practical benefits of batch sampling come with the downside of less adaptivity and the risk of sampling redundant examples within a batch -- a risk that grows with the batch size. In this work, we analyze an efficient active learning algorithm, which focuses on the large batch setting. In particular, we show that our sampling method, which combines notions of uncertainty and diversity, easily scales to batch sizes (100K-1M) several orders of magnitude larger than used in previous studies and provides significant improvements in model training efficiency compared to recent baselines. Finally, we provide an initial theoretical analysis, proving label complexity guarantees for a related sampling method, which we show is approximately equivalent to our sampling method in specific settings.},
  file = {C:\Users\wenji\Zotero\storage\BSUTPW4P\Citovsky 等 - 2021 - Batch Active Learning at Scale.pdf}
}

@inproceedings{aggarwalActiveLearningImbalanced2020,
  title = {Active {{Learning}} for {{Imbalanced Datasets}}},
  author = {Aggarwal, Umang and Popescu, Adrian and Hudelot, Celine},
  date = {2020},
  pages = {1428--1437},
  url = {https://openaccess.thecvf.com/content_WACV_2020/html/Aggarwal_Active_Learning_for_Imbalanced_Datasets_WACV_2020_paper.html},
  urldate = {2024-05-29},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Winter Conference}} on {{Applications}} of {{Computer Vision}}},
  file = {C:\Users\wenji\Zotero\storage\J5XMPEFN\Aggarwal 等 - 2020 - Active Learning for Imbalanced Datasets.pdf}
}

@article{zhangAlgorithmSelectionDeep2023,
  title = {Algorithm {{Selection}} for {{Deep Active Learning}} with {{Imbalanced Datasets}}},
  author = {Zhang, Jifan and Shao, Shuai and Verma, Saurabh and Nowak, Robert},
  date = {2023-12-15},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {9614--9647},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/1e77af93008ee6cd248a31723ce357d8-Abstract-Conference.html},
  urldate = {2024-05-29},
  langid = {english},
  file = {C:\Users\wenji\Zotero\storage\BQV9U2SN\Zhang 等 - 2023 - Algorithm Selection for Deep Active Learning with .pdf}
}

@report{settlesActiveLearningLiterature2009,
  type = {Technical Report},
  title = {Active {{Learning Literature Survey}}},
  author = {Settles, Burr},
  date = {2009},
  institution = {University of Wisconsin-Madison Department of Computer Sciences},
  url = {https://minds.wisconsin.edu/handle/1793/60660},
  urldate = {2024-05-29},
  abstract = {The key idea behind active learning is that a machine learning algorithm can achieve greater accuracy with fewer labeled training instances if it is allowed to choose the training data from which is learns. An active learner may ask queries in the form of unlabeled instances to be labeled by an oracle (e.g., a human annotator). Active learning is well-motivated in many modern machine learning problems, where unlabeled data may be abundant but labels are difficult, time-consuming, or expensive to obtain.    This report provides a general introduction to active learning and a survey of the literature. This includes a discussion of the scenarios in which queries can be formulated, and an overview of the query strategy frameworks proposed in the literature to date. An analysis of the empirical and theoretical evidence for active learning, a summary of several problem setting variants, and a discussion of related topics in machine learning research are also presented.},
  langid = {english},
  annotation = {Accepted: 2012-03-15T17:23:56Z},
  file = {C:\Users\wenji\Zotero\storage\SG9XM4Z7\Settles - 2009 - Active Learning Literature Survey.pdf}
}

@article{tharwatSurveyActiveLearning2023,
  title = {A {{Survey}} on {{Active Learning}}: {{State-of-the-Art}}, {{Practical Challenges}} and {{Research Directions}}},
  shorttitle = {A {{Survey}} on {{Active Learning}}},
  author = {Tharwat, Alaa and Schenck, Wolfram},
  date = {2023-01},
  journaltitle = {Mathematics},
  volume = {11},
  number = {4},
  pages = {820},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2227-7390},
  doi = {10.3390/math11040820},
  url = {https://www.mdpi.com/2227-7390/11/4/820},
  urldate = {2024-05-29},
  abstract = {Despite the availability and ease of collecting a large amount of free, unlabeled data, the expensive and time-consuming labeling process is still an obstacle to labeling a sufficient amount of training data, which is essential for building supervised learning models. Here, with low labeling cost, the active learning (AL) technique could be a solution, whereby a few, high-quality data points are queried by searching for the most informative and representative points within the instance space. This strategy ensures high generalizability across the space and improves classification performance on data we have never seen before. In this paper, we provide a survey of recent studies on active learning in the context of classification. This survey starts with an introduction to the theoretical background of the AL technique, AL scenarios, AL components supported with visual explanations, and illustrative examples to explain how AL simply works and the benefits of using AL. In addition to an overview of the query strategies for the classification scenarios, this survey provides a high-level summary to explain various practical challenges with AL in real-world settings; it also explains how AL can be combined with various research areas. Finally, the most commonly used AL software packages and experimental evaluation metrics with AL are also discussed.},
  issue = {4},
  langid = {english},
  keywords = {active learning,query strategy,semi-supervised learning,supervised learning},
  file = {C:\Users\wenji\Zotero\storage\6FU6NEMX\Tharwat 和 Schenck - 2023 - A Survey on Active Learning State-of-the-Art, Pra.pdf}
}

@inproceedings{shimizuBalancedMiniBatchTraining2018,
  title = {Balanced {{Mini-Batch Training}} for {{Imbalanced Image Data Classification}} with {{Neural Network}}},
  booktitle = {2018 {{First International Conference}} on {{Artificial Intelligence}} for {{Industries}} ({{AI4I}})},
  author = {Shimizu, Ryota and Asako, Kosuke and Ojima, Hiroki and Morinaga, Shohei and Hamada, Mototsugu and Kuroda, Tadahiro},
  date = {2018-09},
  pages = {27--30},
  doi = {10.1109/AI4I.2018.8665709},
  url = {https://ieeexplore.ieee.org/abstract/document/8665709},
  urldate = {2024-05-29},
  abstract = {We propose a novel method of training neural networks for industrial image classification that can reduce the effect of imbalanced data in supervised training. We considered visual quality inspection of industrial products as an image-classification task and attempted to solve this with a convolutional neural network; however, a problem of imbalanced data emerged in supervised training in which the neural network cannot optimize parameters. Since most industrial products are not defective, samples of defective products were fewer than those of the non-defective products; this difference in the number of samples causes an imbalance in training data. A neural network trained with imbalanced data often has varied levels of precision in determining each class depending on the difference in the number of class samples in the training data, which is a significant problem in industrial quality inspection. As a solution to this problem, we propose a balanced mini-batch training method that can virtually balance the class ratio of training samples. In an experiment, the neural network trained with the proposed method achieved higher classification ability than that trained with over-sampled or undersampled data for two types of imbalanced image datasets.},
  eventtitle = {2018 {{First International Conference}} on {{Artificial Intelligence}} for {{Industries}} ({{AI4I}})},
  keywords = {convolutional neural network,Convolutional neural networks,image classification,imbalanced data,Inspection,Manufacturing processes,mini-batch training,Training,Training data,visual quality inspection,Visualization},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\HKQVS4VS\\Shimizu 等 - 2018 - Balanced Mini-Batch Training for Imbalanced Image .pdf;C\:\\Users\\wenji\\Zotero\\storage\\83VIX6Z5\\8665709.html}
}

@article{pengAddressingMultilabelImbalance2021,
  title = {Addressing the Multi-Label Imbalance for Neural Networks: {{An}} Approach Based on Stratified Mini-Batches},
  shorttitle = {Addressing the Multi-Label Imbalance for Neural Networks},
  author = {Peng, Dunlu and Gu, Tianfei and Hu, Xue and Liu, Cong},
  date = {2021-05-07},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {435},
  pages = {91--102},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2020.12.122},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231220320403},
  urldate = {2024-05-29},
  abstract = {Imbalanced learning in the multi-label scenario is a challenging issue, and it also exists in the training of deep neural network. Previous studies have demonstrated that the resampling methods are capable of reducing bias towards the majority group. Nonetheless, when being extended to neural network, these methods display some obvious drawbacks, such as the introduction of extra hyperparameters, the fixed training mode, etc. In order to eliminate the disadvantages, in this paper, an efficient training technique named Mini-Batch Gradient Descent with Stratified sampling (MBGD-Ss) is proposed to alliviate the issue with imbalanced data by dynamically sampling. In view of the specialty of multi-label domain, we put forward two specific strategies as Label Powerset based (SsLP) and Label-based (SsL), respectively. Particularly, SsLP takes the label combination (labelset) that appears in the dataset as a stratum, and SsL directly sets the label as a stratum. Extensive experiments validate the effectiveness of the proposed approach in decreasing the imbalance of sampled data. Moreover, the empirical analysis also shows that the proposed method can mitigate the classifier’s bias against labels, especially improve the prediction accuracy of minority labels.},
  keywords = {Data imbalance,Deep learning,Mini-batch gradient descent,Multi-label classification,Stratified sampling},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\ZZL4K7NX\\Peng 等 - 2021 - Addressing the multi-label imbalance for neural ne.pdf;C\:\\Users\\wenji\\Zotero\\storage\\M6TPN24C\\S0925231220320403.html}
}

@inproceedings{kangExploringBalancedFeature2020,
  title = {Exploring Balanced Feature Spaces for Representation Learning},
  author = {Kang, Bingyi and Li, Yu and Xie, Sa and Yuan, Zehuan and Feng, Jiashi},
  date = {2020},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  file = {C:\Users\wenji\Zotero\storage\RV5ZI7SC\Kang 等 - 2020 - Exploring balanced feature spaces for representati.pdf}
}

@inproceedings{mirzaEfficientRepresentationLearning2016,
  title = {Efficient Representation Learning for High-Dimensional Imbalance Data},
  booktitle = {2016 {{IEEE International Conference}} on {{Digital Signal Processing}} ({{DSP}})},
  author = {Mirza, Bilal and Kok, Stanley and Lin, Zhiping and Kiang Yeo, Yong and Lai, Xiaoping and Cao, Jiuwen and Sepulveda, Jose},
  date = {2016-10},
  pages = {511--515},
  issn = {2165-3577},
  doi = {10.1109/ICDSP.2016.7868610},
  url = {https://ieeexplore.ieee.org/abstract/document/7868610?casa_token=7KUIgJItKf8AAAAA:fPfMKhOoydt6jYJ2Gf_3teNGQfV9GL24ZXVwJ3mzRuEctRDuc-J-j4WXI9SuU_itPL4t8pCGMqU},
  urldate = {2024-05-29},
  abstract = {In this paper, a multi-layer weighted extreme learning machine (ML-WELM) is proposed for high-dimensional datasets with class imbalance. The recently proposed single hidden layer WELM method effectively tackles class imbalance but it may not capture high level abstractions in image datasets. ML-WELM provides efficient representation learning for big image data using multiple hidden layers and at the same time tackles the class imbalance problem using cost-sensitive weighting. Weighted ELM auto-encoder (WELM-AE) is also proposed for layer-by-layer class imbalance feature learning in ML-WELM. We used four imbalance image datasets in our experiments; ML-WELM performs better than the WELM method on all of them.},
  eventtitle = {2016 {{IEEE International Conference}} on {{Digital Signal Processing}} ({{DSP}})},
  keywords = {big data,Big data,class imbalance,extreme learning machine,Image classification,Joining processes,multi-hidden-layer network,Neurons,representation learning,Support vector machines,Testing,Training},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\E29SWY2N\\Mirza 等 - 2016 - Efficient representation learning for high-dimensi.pdf;C\:\\Users\\wenji\\Zotero\\storage\\EEZFSWNL\\7868610.html}
}

@article{maCombatingClassImbalance2018,
  title = {Combating the Class Imbalance Problem in Sparse Representation Learning},
  author = {Ma, Ying and Zhu, Xiatian and Zhu, Shunzhi and Wu, Keshou and Chen, Yuming},
  date = {2018-01-01},
  journaltitle = {Journal of Intelligent \& Fuzzy Systems},
  volume = {35},
  number = {2},
  pages = {1865--1874},
  publisher = {IOS Press},
  issn = {1064-1246},
  doi = {10.3233/JIFS-171342},
  url = {https://content.iospress.com/articles/journal-of-intelligent-and-fuzzy-systems/ifs171342},
  urldate = {2024-05-29},
  abstract = {Recent studies have shown sparse representation learning is a potentially promising method in pattern classification, but very few focused on class imbalanced problems involved in its applications and practice. This problem is particularly important,},
  langid = {english},
  file = {C:\Users\wenji\Zotero\storage\SI3TTC6Q\Ma 等 - 2018 - Combating the class imbalance problem in sparse re.pdf}
}

@online{InclusiveGANImproving,
  title = {Inclusive {{GAN}}: {{Improving Data}} and {{Minority Coverage}} in {{Generative Models}}},
  url = {https://ningyu1991.github.io/projects/InclusiveGAN.html},
  urldate = {2024-05-31},
  file = {C:\Users\wenji\Zotero\storage\NMZG8K84\InclusiveGAN.html}
}

@inproceedings{yuInclusiveGANImproving2020,
  title = {Inclusive {{GAN}}: {{Improving Data}} and {{Minority Coverage}} in {{Generative Models}}},
  shorttitle = {Inclusive {{GAN}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2020},
  author = {Yu, Ning and Li, Ke and Zhou, Peng and Malik, Jitendra and Davis, Larry and Fritz, Mario},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  date = {2020},
  pages = {377--393},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-58542-6_23},
  abstract = {Generative Adversarial Networks (GANs) have brought about rapid progress towards generating photorealistic images. Yet the equitable allocation of their modeling capacity among subgroups has received less attention, which could lead to potential biases against underrepresented minorities if left uncontrolled. In this work, we first formalize the problem of minority inclusion as one of data coverage, and then propose to improve data coverage by harmonizing adversarial training with reconstructive generation. The experiments show that our method outperforms the existing state-of-the-art methods in terms of data coverage on both seen and unseen data. We develop an extension that allows explicit control over the minority subgroups that the model should ensure to include, and validate its effectiveness at little compromise from the overall performance on the entire dataset. Code, models, and supplemental videos are available at GitHub.},
  isbn = {978-3-030-58542-6},
  langid = {english},
  keywords = {Data coverage,GAN,Minority inclusion},
  file = {C:\Users\wenji\Zotero\storage\IM547H6J\Yu 等 - 2020 - Inclusive GAN Improving Data and Minority Coverag.pdf}
}

@inproceedings{anaissiDamageGANGenerative2024,
  title = {Damage {{GAN}}: {{A Generative Model}} for~{{Imbalanced Data}}},
  shorttitle = {Damage {{GAN}}},
  booktitle = {Data {{Science}} and {{Machine Learning}}},
  author = {Anaissi, Ali and Jia, Yuanzhe and Braytee, Ali and Naji, Mohamad and Alyassine, Widad},
  editor = {Benavides-Prado, Diana and Erfani, Sarah and Fournier-Viger, Philippe and Boo, Yee Ling and Koh, Yun Sing},
  date = {2024},
  pages = {48--61},
  publisher = {Springer Nature},
  location = {Singapore},
  doi = {10.1007/978-981-99-8696-5_4},
  abstract = {This study delves into the application of Generative Adversarial Networks (GANs) within the context of imbalanced datasets. Our primary aim is to enhance the performance and stability of GANs in such datasets. In pursuit of this objective, we introduce a novel network architecture known as Damage GAN, building upon the ContraD GAN framework which seamlessly integrates GANs and contrastive learning. Through the utilization of contrastive learning, the discriminator is trained to develop an unsupervised representation capable of distinguishing all provided samples. Our approach draws inspiration from the straightforward framework for contrastive learning of visual representations (SimCLR), leading to the formulation of a distinctive loss function. We also explore the implementation of self-damaging contrastive learning (SDCLR) to further enhance the optimization of the ContraD GAN model. Comparative evaluations against baseline models including the deep convolutional GAN (DCGAN) and ContraD GAN demonstrate the evident superiority of our proposed model, Damage GAN, in terms of generated image distribution, model stability, and image quality when applied to imbalanced datasets.},
  isbn = {978-981-9986-96-5},
  langid = {english},
  keywords = {ContraD GAN,Damage GAN,Imbalanced datasets,SDCLR,SimCLR},
  file = {C:\Users\wenji\Zotero\storage\785WT6F6\Anaissi 等 - 2024 - Damage GAN A Generative Model for Imbalanced Data.pdf}
}

@inproceedings{jeongBiasedExtrapolationLatent2021,
  title = {Biased {{Extrapolation}} in {{Latent Space forImbalanced Deep Learning}}},
  booktitle = {Machine {{Learning}} in {{Medical Imaging}}},
  author = {Jeong, Suhyeon and Lee, Seungkyu},
  editor = {Lian, Chunfeng and Cao, Xiaohuan and Rekik, Islem and Xu, Xuanang and Yan, Pingkun},
  date = {2021},
  pages = {337--346},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-87589-3_35},
  abstract = {Addressing class data imbalance to improve generalization on minor classes is critical in medical applications. Traditional approaches including re-weighing and re-sampling have shown the potential of the generalization but ignore statistical characteristics of classes. We study the potential and effectiveness of data extrapolation in latent space of deep learning networks to address data imbalance. We propose biased normal sample selection and latent space sample extrapolation methods for imbalanced deep learning. Two types of biases in the extrapolations are sample bias and extrapolation bias. Experimental evaluation is performed for ulcer classification in endoscopy images and Cardiomegaly detection from CXR. We show that new abnormal samples extrapolated asymmetrically from biased normal samples of low probability improve the separation between normal and abnormal classes.},
  isbn = {978-3-030-87589-3},
  langid = {english},
  keywords = {Biased learning,Extrapolation,Imbalanced learning},
  file = {C:\Users\wenji\Zotero\storage\7QVXPGIY\Jeong 和 Lee - 2021 - Biased Extrapolation in Latent Space forImbalanced.pdf}
}

@inproceedings{zhaoBiasGeneralizationDeep2018,
  title = {Bias and {{Generalization}} in {{Deep Generative Models}}: {{An Empirical Study}}},
  shorttitle = {Bias and {{Generalization}} in {{Deep Generative Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhao, Shengjia and Ren, Hongyu and Yuan, Arianna and Song, Jiaming and Goodman, Noah and Ermon, Stefano},
  date = {2018},
  volume = {31},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2018/hash/5317b6799188715d5e00a638a4278901-Abstract.html},
  urldate = {2024-06-01},
  abstract = {In high dimensional settings, density estimation algorithms rely crucially on their inductive bias. Despite recent empirical success, the inductive bias of deep generative models is not well understood. In this paper we propose a framework to systematically investigate bias and generalization in deep generative models of images by probing the learning algorithm with carefully designed training datasets. By measuring properties of the learned distribution, we are able to find interesting patterns of generalization. We verify that these patterns are consistent across datasets, common models and architectures.},
  file = {C:\Users\wenji\Zotero\storage\DJ8VQNXV\Zhao 等 - 2018 - Bias and Generalization in Deep Generative Models.pdf}
}

@online{tanImprovingFairnessDeep2021,
  title = {Improving the {{Fairness}} of {{Deep Generative Models}} without {{Retraining}}},
  author = {Tan, Shuhan and Shen, Yujun and Zhou, Bolei},
  date = {2021-03-29},
  eprint = {2012.04842},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2012.04842},
  url = {http://arxiv.org/abs/2012.04842},
  urldate = {2024-06-01},
  abstract = {Generative Adversarial Networks (GANs) advance face synthesis through learning the underlying distribution of observed data. Despite the high-quality generated faces, some minority groups can be rarely generated from the trained models due to a biased image generation process. To study the issue, we first conduct an empirical study on a pre-trained face synthesis model. We observe that after training the GAN model not only carries the biases in the training data but also amplifies them to some degree in the image generation process. To further improve the fairness of image generation, we propose an interpretable baseline method to balance the output facial attributes without retraining. The proposed method shifts the interpretable semantic distribution in the latent space for a more balanced image generation while preserving the sample diversity. Besides producing more balanced data regarding a particular attribute (e.g., race, gender, etc.), our method is generalizable to handle more than one attribute at a time and synthesize samples of fine-grained subgroups. We further show the positive applicability of the balanced data sampled from GANs to quantify the biases in other face recognition systems, like commercial face attribute classifiers and face super-resolution algorithms.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\TK5UWUN2\\Tan 等 - 2021 - Improving the Fairness of Deep Generative Models w.pdf;C\:\\Users\\wenji\\Zotero\\storage\\IFAHI2DI\\2012.html}
}

@incollection{mcduffCharacterizingBiasClassifiers2019,
  title = {Characterizing Bias in Classifiers Using Generative Models},
  booktitle = {Proceedings of the 33rd {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {McDuff, Daniel and Song, Yale and Kapoor, Ashish and Ma, Shuang},
  date = {2019-12-08},
  number = {485},
  pages = {5403--5414},
  publisher = {Curran Associates Inc.},
  location = {Red Hook, NY, USA},
  abstract = {Models that are learned from real-world data are often biased because the data used to train them is biased. This can propagate systemic human biases that exist and ultimately lead to inequitable treatment of people, especially minorities. To characterize bias in learned classifiers, existing approaches rely on human oracles labeling real-world examples to identify the "blind spots" of the classifiers; these are ultimately limited due to the human labor required and the finite nature of existing image examples. We propose a simulation-based approach for interrogating classifiers using generative adversarial models in a systematic manner. We incorporate a progressive conditional generative model for synthesizing photorealistic facial images and Bayesian Optimization for an efficient interrogation of independent facial image classification systems. We show how this approach can be used to efficiently characterize racial and gender biases in commercial systems.},
  file = {C:\Users\wenji\Zotero\storage\435FZH6J\McDuff 等 - 2019 - Characterizing bias in classifiers using generativ.pdf}
}

@inproceedings{mcduffCharacterizingBiasClassifiers2019a,
  title = {Characterizing {{Bias}} in {{Classifiers}} Using {{Generative Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {McDuff, Daniel and Ma, Shuang and Song, Yale and Kapoor, Ashish},
  date = {2019},
  volume = {32},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.neurips.cc/paper_files/paper/2019/hash/7f018eb7b301a66658931cb8a93fd6e8-Abstract.html},
  urldate = {2024-06-01},
  abstract = {Models that are learned from real-world data are often biased because the data used to train them is biased. This can propagate systemic human biases that exist and ultimately lead to inequitable treatment of people, especially minorities. To characterize bias in learned classifiers, existing approaches rely on human oracles labeling real-world examples to identify the "blind spots" of the classifiers; these are ultimately limited due to the human labor required and the finite nature of existing image examples.  We propose a simulation-based approach for interrogating classifiers using generative adversarial models in a systematic manner. We incorporate a progressive conditional generative model for synthesizing photo-realistic facial images and Bayesian Optimization for an efficient interrogation of independent facial image classification systems. We show how this approach can be used to efficiently characterize racial and gender biases in commercial systems.},
  file = {C:\Users\wenji\Zotero\storage\SL76L7LS\McDuff 等 - 2019 - Characterizing Bias in Classifiers using Generativ.pdf}
}

@inproceedings{NEURIPS2019_7f018eb7,
  title = {Characterizing Bias in Classifiers Using Generative Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {McDuff, Daniel and Ma, Shuang and Song, Yale and Kapoor, Ashish},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and family=Buc, given=F., prefix=dAlché-, useprefix=true and Fox, E. and Garnett, R.},
  date = {2019},
  volume = {32},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/7f018eb7b301a66658931cb8a93fd6e8-Paper.pdf}
}

@inproceedings{choiFairGenerativeModeling2020,
  title = {Fair Generative Modeling via Weak Supervision},
  author = {Choi, Kristy and Grover, Aditya and Singh, Trisha and Shu, Rui and Ermon, Stefano},
  date = {2020},
  pages = {1887--1898},
  publisher = {PMLR},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  isbn = {2640-3498},
  file = {C:\Users\wenji\Zotero\storage\7BT7WL2L\Choi 等 - 2020 - Fair generative modeling via weak supervision.pdf}
}

@inproceedings{zemelLearningFairRepresentations2013,
  title = {Learning {{Fair Representations}}},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{Machine Learning}}},
  author = {Zemel, Rich and Wu, Yu and Swersky, Kevin and Pitassi, Toni and Dwork, Cynthia},
  date = {2013-05-26},
  pages = {325--333},
  publisher = {PMLR},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v28/zemel13.html},
  urldate = {2024-06-01},
  abstract = {We propose a learning algorithm for fair classification that achieves both group fairness (the proportion of members in a protected group receiving positive classification is identical to the proportion in the population as a  whole), and individual fairness (similar individuals should be treated similarly).  We formulate fairness as an optimization problem of finding a  good representation of the data with two competing goals: to encode the data as well as possible, while simultaneously obfuscating any information about membership in the protected group.  We show positive results of our algorithm relative to other known techniques, on three datasets.  Moreover, we demonstrate several advantages to our approach.  First, our intermediate representation can be used for other classification tasks (i.e., transfer  learning is possible); secondly, we take a step toward learning a distance metric which can find important dimensions of the data for classification.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english}
}

@article{t.h.teoFairTLTransferLearning2024,
  title = {{{FairTL}}: {{A Transfer Learning Approach}} for {{Bias Mitigation}} in {{Deep Generative Models}}},
  shorttitle = {{{FairTL}}},
  author = {T.H.Teo, Christopher and Abdollahzadeh, Milad and Cheung, Ngai-Man},
  date = {2024},
  journaltitle = {IEEE Journal of Selected Topics in Signal Processing},
  pages = {1--13},
  issn = {1941-0484},
  doi = {10.1109/JSTSP.2024.3363419},
  url = {https://ieeexplore.ieee.org/abstract/document/10436577?casa_token=7NJV34UMHjgAAAAA:SVVng4F5xow1vLfctX6iu0I0MVoKtfRNs7XMQwCqQljiipXtlIePuMyZ6RPkFRuTtsCzMXt1kzM},
  urldate = {2024-06-01},
  abstract = {This work studies fair generative models. We reveal and quantify the biases in state-of-the-art (SOTA) GANs w.r.t. different sensitive attributes. To address the biases, our main contribution is to propose novel methods to learn fair generative models via transfer learning. Specifically, first, we propose FairTL where we pre-train the generative model with a large biased dataset, then adapt the model using a small fair reference dataset. Second, to further improve sample diversity, we propose FairTL++, containing two additional innovations: i) aligned feature adaptation, which preserves learned general knowledge while improving fairness by adapting only sensitive attribute-specific parameters, ii) multiple feedback discrimination, which introduces a frozen discriminator for quality feedback and another evolving discriminator for fairness feedback. Taking one step further, we consider an alternative challenging and practical setup. Here, only a pre-trained model is available but the dataset used to pre-train the model is inaccessible. We remark that previous work requires access to large, biased datasets and cannot handle this setup. Extensive experimental results show that FairTL and FairTL++ achieve state-of-the-art performance in quality, diversity and fairness in both setups. Code is available at: https://github.com/sutd-visual-computing-group/FairTL.},
  eventtitle = {{{IEEE Journal}} of {{Selected Topics}} in {{Signal Processing}}},
  keywords = {Adaptation models,Data models,Fairness,Generative adversarial networks,Generative Adversarial Networks,Generative Models,Generators,Task analysis,Training,Transfer learning,Transfer Learning},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\P45KNQMR\\T.H.Teo 等 - 2024 - FairTL A Transfer Learning Approach for Bias Miti.pdf;C\:\\Users\\wenji\\Zotero\\storage\\47S3P827\\10436577.html}
}

@article{singhDiverseBiasedMitigating2024,
  title = {Diverse {{Yet Biased}}: {{Towards Mitigating Biases}} in {{Generative AI}} ({{Student Abstract}})},
  shorttitle = {Diverse {{Yet Biased}}},
  author = {Singh, Akshit},
  date = {2024-03-24},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {38},
  number = {21},
  pages = {23653--23654},
  issn = {2374-3468},
  doi = {10.1609/aaai.v38i21.30512},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/30512},
  urldate = {2024-06-01},
  abstract = {Generative Artificial Intelligence (AI) has garnered significant attention for its remarkable ability to generate text, images, and other forms of content. However, an inherent and increasingly concerning issue within generative AI systems is bias. These AI models often exhibit an Anglo-centric bias and tend to overlook the importance of diversity. This can be attributed to their training on extensive datasets sourced from the internet, which inevitably inherit the biases present in those data sources. Employing these datasets leads to AI-generated content that mirrors and perpetuates existing biases, encompassing various aspects such as gender, ethnic and cultural stereotypes. Addressing bias in generative AI is a complex challenge that necessitates substantial efforts. In order to tackle this issue, we propose a methodology for constructing moderately sized datasets with a social inclination. These datasets can be employed to rectify existing imbalances in datasets or to train models to generate socially inclusive material. Additionally, we present preliminary findings derived from training our model on these socially inclined datasets.},
  issue = {21},
  langid = {english},
  keywords = {Fairness},
  file = {C:\Users\wenji\Zotero\storage\RLRLGGKZ\Singh - 2024 - Diverse Yet Biased Towards Mitigating Biases in G.pdf}
}

@inproceedings{liREPAIRRemovingRepresentation2019,
  title = {{{REPAIR}}: {{Removing Representation Bias}} by {{Dataset Resampling}}},
  shorttitle = {{{REPAIR}}},
  author = {Li, Yi and Vasconcelos, Nuno},
  date = {2019},
  pages = {9572--9581},
  url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Li_REPAIR_Removing_Representation_Bias_by_Dataset_Resampling_CVPR_2019_paper.html},
  urldate = {2024-06-01},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {C:\Users\wenji\Zotero\storage\N767XGAR\Li 和 Vasconcelos - 2019 - REPAIR Removing Representation Bias by Dataset Re.pdf}
}

@inproceedings{bellingerCalibratedResamplingImbalanced2021,
  title = {Calibrated {{Resampling}} for {{Imbalanced}} and {{Long-Tails}} in {{Deep Learning}}},
  booktitle = {Discovery {{Science}}},
  author = {Bellinger, Colin and Corizzo, Roberto and Japkowicz, Nathalie},
  editor = {Soares, Carlos and Torgo, Luis},
  date = {2021},
  pages = {242--252},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-88942-5_19},
  abstract = {Long-tailed distributions and class imbalance are problems of significant importance in applied deep learning where trained models are exploited for decision support and decision automation in critical areas such as health and medicine, transportation and finance. The challenge of learning deep models from such data remains high, and the state-of-the-art solutions are typically data dependent and primarily focused on images. Important real-world problems, however, are much more diverse thus necessitating a general solution that can be applied to diverse data types. In this paper, we propose ReMix, a training technique that seamlessly leverages batch resampling, instance mixing and soft-labels to efficiently enable the induction of robust deep models from imbalanced and long-tailed datasets. Our results show that fully connected neural networks and Convolutional Neural Networks (CNNs) trained with ReMix generally outperform the alternatives according to the g-mean and are better calibrated according to the balanced Brier score.},
  isbn = {978-3-030-88942-5},
  langid = {english},
  keywords = {Calibration,Class imbalance,Deep learning,Long-tail distribution},
  file = {C:\Users\wenji\Zotero\storage\DECZPTWI\Bellinger 等 - 2021 - Calibrated Resampling for Imbalanced and Long-Tail.pdf}
}

@article{kimFairClassificationLoss2023,
  title = {Fair Classification by Loss Balancing via Fairness-Aware Batch Sampling},
  author = {Kim, Dohyung and Park, Sungho and Hwang, Sunhee and Byun, Hyeran},
  date = {2023-01-21},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {518},
  pages = {231--241},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2022.11.018},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231222013984},
  urldate = {2024-06-01},
  abstract = {Existing classification models often output discriminatory results since they learn the target attribute without addressing data imbalance with respect to the protected attributes (e.g., gender). The models tend to focus on learning toward demographic groups containing the larger number of training samples, which consequently leads to training loss discrepancy between the groups. Our work focuses on addressing the occurrence of training loss discrepancy between the groups to improve the model’s fairness. To this end, we firstly define the target-protected group using the target and protected attribute labels and observe the group-wise training loss in terms of previous fairness approaches. From the observation, we figure out that balancing the total loss across all the groups allows to mitigate fairness issue significantly, and meanwhile, only considering the sample size of each group to obtain a balanced mini-batch is not enough for mitigating fairness. Motivated by the observations, we propose a fairness-aware batch sampling scheme that adaptively updates batch sampling probability (BSP) and constructs a fairness-aware mini-batch from the model’s point of view. Our key idea is to balance the training losses via training with fairness-aware mini-batch. Through extensive experiments on two facial attribute benchmark datasets and one tabular dataset, our simple and effective sampling strategy achieves superior improvement in terms of two standard fairness metrics. We validate our algorithm with various experimental settings (e.g, multi-attribute classification, binary classification with multiple protected attributes). Moreover, we introduce a new metric for measuring the trade-off between fairness and classification performance. On this metric, our algorithm also achieves the best trade-off performance.},
  keywords = {Classification,Data imbalance,Deep neural network,Fairness AI,Resampling},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\QJY2644C\\Kim 等 - 2023 - Fair classification by loss balancing via fairness.pdf;C\:\\Users\\wenji\\Zotero\\storage\\6PBFWFHQ\\S0925231222013984.html}
}

@article{kimFairClassificationLoss2023a,
  title = {Fair Classification by Loss Balancing via Fairness-Aware Batch Sampling},
  author = {Kim, Dohyung and Park, Sungho and Hwang, Sunhee and Byun, Hyeran},
  date = {2023-01-21},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {518},
  pages = {231--241},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2022.11.018},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231222013984},
  urldate = {2024-06-01},
  abstract = {Existing classification models often output discriminatory results since they learn the target attribute without addressing data imbalance with respect to the protected attributes (e.g., gender). The models tend to focus on learning toward demographic groups containing the larger number of training samples, which consequently leads to training loss discrepancy between the groups. Our work focuses on addressing the occurrence of training loss discrepancy between the groups to improve the model’s fairness. To this end, we firstly define the target-protected group using the target and protected attribute labels and observe the group-wise training loss in terms of previous fairness approaches. From the observation, we figure out that balancing the total loss across all the groups allows to mitigate fairness issue significantly, and meanwhile, only considering the sample size of each group to obtain a balanced mini-batch is not enough for mitigating fairness. Motivated by the observations, we propose a fairness-aware batch sampling scheme that adaptively updates batch sampling probability (BSP) and constructs a fairness-aware mini-batch from the model’s point of view. Our key idea is to balance the training losses via training with fairness-aware mini-batch. Through extensive experiments on two facial attribute benchmark datasets and one tabular dataset, our simple and effective sampling strategy achieves superior improvement in terms of two standard fairness metrics. We validate our algorithm with various experimental settings (e.g, multi-attribute classification, binary classification with multiple protected attributes). Moreover, we introduce a new metric for measuring the trade-off between fairness and classification performance. On this metric, our algorithm also achieves the best trade-off performance.},
  keywords = {Classification,Data imbalance,Deep neural network,Fairness AI,Resampling},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\SPYUA8BM\\Kim 等 - 2023 - Fair classification by loss balancing via fairness.pdf;C\:\\Users\\wenji\\Zotero\\storage\\KGDWSBZG\\S0925231222013984.html}
}

@inproceedings{10.1145/3306618.3314243,
  title = {Uncovering and Mitigating Algorithmic Bias through Learned Latent Structure},
  booktitle = {Proceedings of the 2019 {{AAAI}}/{{ACM}} Conference on {{AI}}, Ethics, and Society},
  author = {Amini, Alexander and Soleimany, Ava P. and Schwarting, Wilko and Bhatia, Sangeeta N. and Rus, Daniela},
  date = {2019},
  series = {Aies '19},
  pages = {289--295},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3306618.3314243},
  url = {https://doi-org.kuleuven.e-bronnen.be/10.1145/3306618.3314243},
  abstract = {Recent research has highlighted the vulnerabilities of modern machine learning based systems to bias, especially towards segments of society that are under-represented in training data. In this work, we develop a novel, tunable algorithm for mitigating the hidden, and potentially unknown, biases within training data. Our algorithm fuses the original learning task with a variational autoencoder to learn the latent structure within the dataset and then adaptively uses the learned latent distributions to re-weight the importance of certain data points while training. While our method is generalizable across various data modalities and learning tasks, in this work we use our algorithm to address the issue of racial and gender bias in facial detection systems. We evaluate our algorithm on the Pilot Parliaments Benchmark (PPB), a dataset specifically designed to evaluate biases in computer vision systems, and demonstrate increased overall performance as well as decreased categorical bias with our debiasing approach.},
  isbn = {978-1-4503-6324-2},
  pagetotal = {7},
  keywords = {algorithmic bias,deep learning,facial detection,neural networks},
  file = {C:\Users\wenji\Zotero\storage\ZN3XSJUC\Amini 等 - 2019 - Uncovering and mitigating algorithmic bias through.pdf}
}

@inproceedings{zhangSimilarityBasedActiveLearning2018,
  title = {Similarity-{{Based Active Learning}} for {{Image Classification Under Class Imbalance}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Data Mining}} ({{ICDM}})},
  author = {Zhang, Chuanhai and Tavanapong, Wallapak and Kijkul, Gavin and Wong, Johnny and family=Groen, given=Piet C., prefix=de, useprefix=true and Oh, JungHwan},
  date = {2018-11},
  pages = {1422--1427},
  issn = {2374-8486},
  doi = {10.1109/ICDM.2018.00196},
  url = {https://ieeexplore.ieee.org/abstract/document/8595005?casa_token=_mgpMvCi280AAAAA:ikRpg_w_IgpBOAiINP-LfUMpoDq7-glHKgEt0k4aUJ34D0hXjBfPI53oMTtwZMGUerWQ2XZdCCE},
  urldate = {2024-06-02},
  abstract = {Many image classification tasks (e.g., medical image classification) have a severe class imbalance problem. Convolutional neural network (CNN) is currently a state-of-the-art method for image classification. CNN relies on a large training dataset to achieve high classification performance. However, manual labeling is costly and may not even be feasible for medical domain. In this paper, we propose a novel similarity-based active deep learning framework (SAL) that deals with class imbalance. SAL actively learns a similarity model to recommend unlabeled rare class samples for experts' manual labeling. Based on similarity ranking, SAL recommends high confidence unlabeled common class samples for automatic pseudo-labeling without experts' labeling effort. To the best of our knowledge, SAL is the first active deep learning framework that deals with a significant class imbalance. Our experiments show that SAL consistently outperforms two other recent active deep learning methods on two challenging datasets. What's more, SAL obtains nearly the upper bound classification performance (using all the images in the training dataset) while the domain experts labeled only 5.6\% and 7.5\% of all images in the Endoscopy dataset and the Caltech-256 dataset, respectively. SAL significantly reduces the experts' manual labeling efforts while achieving near optimal classification performance.},
  eventtitle = {2018 {{IEEE International Conference}} on {{Data Mining}} ({{ICDM}})},
  keywords = {Active deep learning,class imbalance,Deep learning,image classification,Image classification,Labeling,Learning systems,Manuals,similarity learning,Task analysis,Training},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\PTJVUR4J\\Zhang 等 - 2018 - Similarity-Based Active Learning for Image Classif.pdf;C\:\\Users\\wenji\\Zotero\\storage\\BB392GYT\\8595005.html}
}

@inproceedings{naeemReliableFidelityDiversity2020,
  title = {Reliable {{Fidelity}} and {{Diversity Metrics}} for {{Generative Models}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Naeem, Muhammad Ferjad and Oh, Seong Joon and Uh, Youngjung and Choi, Yunjey and Yoo, Jaejun},
  date = {2020-11-21},
  pages = {7176--7185},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v119/naeem20a.html},
  urldate = {2024-06-02},
  abstract = {Devising indicative evaluation metrics for the image generation task remains an open problem. The most widely used metric for measuring the similarity between real and generated images has been the Frechet Inception Distance (FID) score. Since it does not differentiate the fidelity and diversity aspects of the generated images, recent papers have introduced variants of precision and recall metrics to diagnose those properties separately. In this paper, we show that even the latest version of the precision and recall metrics are not reliable yet. For example, they fail to detect the match between two identical distributions, they are not robust against outliers, and the evaluation hyperparameters are selected arbitrarily. We propose density and coverage metrics that solve the above issues. We analytically and experimentally show that density and coverage provide more interpretable and reliable signals for practitioners than the existing metrics.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {C:\Users\wenji\Zotero\storage\LWDDQW4Y\Naeem 等 - 2020 - Reliable Fidelity and Diversity Metrics for Genera.pdf}
}

@article{gaoBalancedSemisupervisedGenerative2021,
  title = {Balanced Semisupervised Generative Adversarial Network for Damage Assessment from Low-Data Imbalanced-Class Regime},
  author = {Gao, Yuqing and Zhai, Pengyuan and Mosalam, Khalid M.},
  date = {2021},
  journaltitle = {Computer-Aided Civil and Infrastructure Engineering},
  volume = {36},
  number = {9},
  pages = {1094--1113},
  issn = {1467-8667},
  doi = {10.1111/mice.12741},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/mice.12741},
  urldate = {2024-06-02},
  abstract = {In recent years, applying deep learning (DL) to assess structural damages has gained growing popularity in vision-based structural health monitoring (SHM). However, both data deficiency and class imbalance hinder the wide adoption of DL in practical applications of SHM. Common mitigation strategies include transfer learning, oversampling, and undersampling, yet these ad hoc methods only provide limited performance boost that varies from one case to another. In this work, we introduce one variant of the generative adversarial network (GAN), named the balanced semisupervised GAN (BSS-GAN). It adopts the semisupervised learning concept and applies balanced-batch sampling in training to resolve low-data and imbalanced-class problems. A series of computer experiments on concrete cracking and spalling classification were conducted under the low-data imbalanced-class regime with limited computing power. The results show that the BSS-GAN is able to achieve better damage detection in terms of recall and score than other conventional methods, indicating its state-of-the-art performance.},
  langid = {english},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\EKCR3F5L\\Gao 等 - 2021 - Balanced semisupervised generative adversarial net.pdf;C\:\\Users\\wenji\\Zotero\\storage\\862687U3\\mice.html}
}

@inproceedings{zhuBalancedContrastiveLearning2022,
  title = {Balanced {{Contrastive Learning}} for {{Long-Tailed Visual Recognition}}},
  author = {Zhu, Jianggang and Wang, Zheng and Chen, Jingjing and Chen, Yi-Ping Phoebe and Jiang, Yu-Gang},
  date = {2022},
  pages = {6908--6917},
  url = {https://openaccess.thecvf.com/content/CVPR2022/html/Zhu_Balanced_Contrastive_Learning_for_Long-Tailed_Visual_Recognition_CVPR_2022_paper.html},
  urldate = {2024-06-02},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  langid = {english},
  file = {C:\Users\wenji\Zotero\storage\6WCZ2J6I\Zhu 等 - 2022 - Balanced Contrastive Learning for Long-Tailed Visu.pdf}
}

@article{yangDiversityBasedSyntheticOversampling2023,
  title = {A {{Diversity-Based Synthetic Oversampling Using Clustering}} for {{Handling Extreme Imbalance}}},
  author = {Yang, Yuxuan and Akbarzadeh Khorshidi, Hadi and Aickelin, Uwe},
  date = {2023-11-08},
  journaltitle = {SN Computer Science},
  shortjournal = {SN COMPUT. SCI.},
  volume = {4},
  number = {6},
  pages = {848},
  issn = {2661-8907},
  doi = {10.1007/s42979-023-02249-3},
  url = {https://doi.org/10.1007/s42979-023-02249-3},
  urldate = {2024-06-02},
  abstract = {Imbalanced data are typically observed in many real-life classification problems. However, mainstream machine learning algorithms are mostly designed with the underlying assumption of a relatively well-balanced distribution of classes. The mismatch between reality and algorithm assumption results in a deterioration of classification performance. One form of approach to address this problem is through re-sampling methods, although its effectiveness is limited; most re-sampling methods fail to consider the distribution of minority and majority instances and the diversity within synthetically generated data. Diversity becomes increasingly important when minority data becomes more sparse, as each data point becomes more valuable. They should all be considered during the generation process instead of being regarded as noise. In this paper, we propose a cluster-based diversity re-sampling method, combined with NOAH algorithm. Neighbourhood-based Clustering Diversity Over-sampling (NBCDO) is introduced with the aim to complement our previous cluster-based diversity algorithm Density-based Clustering Diversity Over-sampling (DBCDO). It first uses a neighbourhood-based clustering algorithm to consider the distribution of both minority and majority class instances, before applying NOAH algorithm to encourage diversity optimisation during the generation of synthetic instances. We demonstrate the implementation of both cluster-based diversity methods by conducting experiments over 10 real-life datasets with\,≤\,5\% imbalance ratio and show that our proposed cluster-based diversity algorithm (NBCDO, DBCDO) brings performance improvements over its comparable methods (DB-SMOTE, MAHAKIL, KMEANS-SMOTE, MC-SMOTE).},
  langid = {english},
  keywords = {Clustering,Diversity optimisation,Genetic algorithm,Imbalanced data,Over-sampling},
  file = {C:\Users\wenji\Zotero\storage\8P6RE27W\Yang 等 - 2023 - A Diversity-Based Synthetic Oversampling Using Clu.pdf}
}

@online{DeepSMOTEFusingDeep,
  title = {{{DeepSMOTE}}: {{Fusing Deep Learning}} and {{SMOTE}} for {{Imbalanced Data}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9694621},
  urldate = {2024-06-02},
  file = {C:\Users\wenji\Zotero\storage\YKU8DBPM\9694621.html}
}

@article{dablainDeepSMOTEFusingDeep2023,
  title = {{{DeepSMOTE}}: {{Fusing Deep Learning}} and {{SMOTE}} for {{Imbalanced Data}}},
  shorttitle = {{{DeepSMOTE}}},
  author = {Dablain, Damien and Krawczyk, Bartosz and Chawla, Nitesh V.},
  date = {2023-09},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {34},
  number = {9},
  pages = {6390--6404},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2021.3136503},
  url = {https://ieeexplore.ieee.org/abstract/document/9694621},
  urldate = {2024-06-02},
  abstract = {Despite over two decades of progress, imbalanced data is still considered a significant challenge for contemporary machine learning models. Modern advances in deep learning have further magnified the importance of the imbalanced data problem, especially when learning from images. Therefore, there is a need for an oversampling method that is specifically tailored to deep learning models, can work on raw images while preserving their properties, and is capable of generating high-quality, artificial images that can enhance minority classes and balance the training set. We propose Deep synthetic minority oversampling technique (SMOTE), a novel oversampling algorithm for deep learning models that leverages the properties of the successful SMOTE algorithm. It is simple, yet effective in its design. It consists of three major components: 1) an encoder/decoder framework; 2) SMOTE-based oversampling; and 3) a dedicated loss function that is enhanced with a penalty term. An important advantage of DeepSMOTE over generative adversarial network (GAN)-based oversampling is that DeepSMOTE does not require a discriminator, and it generates high-quality artificial images that are both information-rich and suitable for visual inspection. DeepSMOTE code is publicly available at https://github.com/dd1github/DeepSMOTE.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}} and {{Learning Systems}}},
  keywords = {Class imbalance,Data models,deep learning,Deep learning,Image reconstruction,Inspection,Learning systems,machine learning,oversampling,synthetic minority oversampling technique (SMOTE),Training,Visualization},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\C6Q6ZZLW\\Dablain 等 - 2023 - DeepSMOTE Fusing Deep Learning and SMOTE for Imba.pdf;C\:\\Users\\wenji\\Zotero\\storage\\V43MYV7M\\9694621.html}
}

@article{gerychDebiasingPretrainedGenerative2023,
  title = {Debiasing {{Pretrained Generative Models}} by {{Uniformly Sampling Semantic Attributes}}},
  author = {Gerych, Walter and Hickey, Kevin and Buquicchio, Luke and Chandrasekaran, Kavin and Alajaji, Abdulaziz and Rundensteiner, Elke A. and Agu, Emmanuel},
  date = {2023-12-15},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {45083--45101},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/8d7060b2ee6ff728692398783e3d59d1-Abstract-Conference.html},
  urldate = {2024-06-03},
  langid = {english},
  file = {C:\Users\wenji\Zotero\storage\6VBCEYLN\Gerych 等 - 2023 - Debiasing Pretrained Generative Models by Uniforml.pdf}
}

@article{zhaiBinaryImbalancedData2022,
  title = {Binary Imbalanced Data Classification Based on Diversity Oversampling by Generative Models},
  author = {Zhai, Junhai and Qi, Jiaxing and Shen, Chu},
  date = {2022-03-01},
  journaltitle = {Information Sciences},
  shortjournal = {Information Sciences},
  volume = {585},
  pages = {313--343},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2021.11.058},
  url = {https://www.sciencedirect.com/science/article/pii/S0020025521011804},
  urldate = {2024-06-03},
  abstract = {In many practical applications, the data are class imbalanced. Accordingly, it is very meaningful and valuable to investigate the classification of imbalanced data. In the framework of binary imbalanced data classification, the synthetic minority oversampling technique (SMOTE) is the best-known oversampling method. However, for each positive sample, SMOTE generates only k synthetic samples on the lines between the positive sample and its k-nearest neighbors, resulting in three drawbacks: (1) SMOTE cannot effectively extend the training field of positive samples; (2) the generated positive samples lack diversity; (3) SMOTE does not accurately approximate the probability distribution of the positive samples. Therefore, two binary imbalanced data classification methods named BIDC1 and BIDC2 based on diversity oversampling by generative models are proposed. The BIDC1 and BIDC2 conduct diversity oversampling using extreme learning machine autoencoder and generative adversarial network, respectively. Extensive experiments on 26 data sets are conducted to compare the two methods with 14 state-of-the-art methods using five metrics: F-measure, G-means, AUC-area, MMD-score, and Silhouette-score. The experimental results demonstrate that the two methods outperform the other 14 methods.},
  keywords = {Binary imbalanced data classification,Diversity oversampling,Extreme learning machine autoencoder,Generative adversarial network,Imbalanced learning},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\QQLNSXYV\\Zhai 等 - 2022 - Binary imbalanced data classification based on div.pdf;C\:\\Users\\wenji\\Zotero\\storage\\96CFSDJE\\S0020025521011804.html}
}

@inproceedings{zhangDeterminantalPointProcesses2017a,
  title = {Determinantal {{Point Processes}} for {{Mini-Batch Diversification}}},
  booktitle = {{{CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE}} ({{UAI2017}})},
  author = {Zhang, Cheng and Kjellstrom, Hedvig and Mandt, Stephan},
  date = {2017},
  publisher = {Auai Press},
  location = {Corvallis},
  url = {https://www-webofscience-com.kuleuven.e-bronnen.be/wos/woscc/full-record/WOS:000493309500018},
  urldate = {2024-06-03},
  abstract = {We study a mini-batch diversification scheme for stochastic gradient descent (SGD). While classical SGD relies on uniformly sampling data points to form a mini-batch, we propose a non-uniform sampling scheme based on the Determinantal Point Process (DPP). The DPP relies on a similarity measure between data points and gives low probabilities to mini-batches which contain redundant data, and higher probabilities to mini-batches with more diverse data. This simultaneously balances the data and leads to stochastic gradients with lower variance. We term this approach Diversified Mini-Batch SGD (DM-SGD). We show that regular SGD and a biased version of stratified sampling emerge as special cases. Furthermore, DM-SGD generalizes stratified sampling to cases where no discrete features exist to bin the data into groups. We show experimentally that our method results more interpretable and diverse features in unsupervised setups, and in better classification accuracies in supervised setups.},
  eventtitle = {Conference on {{Uncertainty}} in {{Artificial Intelligence}} ({{UAI}})},
  langid = {english},
  pagetotal = {10},
  annotation = {Web of Science ID: WOS:000493309500018},
  file = {C:\Users\wenji\Zotero\storage\QXWWPFLL\Zhang 等 - 2017 - Determinantal Point Processes for Mini-Batch Diver.pdf}
}

@inproceedings{zhangActiveMiniBatchSampling2019,
  title = {Active {{Mini-Batch Sampling Using Repulsive Point Processes}}},
  booktitle = {{{THIRTY-THIRD AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE}} / {{THIRTY-FIRST INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE}} / {{NINTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE}}},
  author = {Zhang, Cheng and Oztireli, Cengiz and Mandt, Stephan and Salvi, Giampiero},
  date = {2019},
  pages = {5741--5748},
  publisher = {Assoc Advancement Artificial Intelligence},
  location = {Palo Alto},
  issn = {2159-5399, 2374-3468},
  url = {https://www-webofscience-com.kuleuven.e-bronnen.be/wos/woscc/full-record/WOS:000486572500033},
  urldate = {2024-06-03},
  abstract = {The convergence speed of stochastic gradient descent (SGD) can be improved by actively selecting mini-batches. We explore sampling schemes where similar data points are less likely to be selected in the same mini-batch. In particular, we prove that such repulsive sampling schemes lower the variance of the gradient estimator. This generalizes recent work on using Determinantal Point Processes (DPPs) for mini-batch diversification (Zhang et al., 2017) to the broader class of repulsive point processes. We first show that the phenomenon of variance reduction by diversified sampling generalizes in particular to non-stationary point processes. We then show that other point processes may be computationally much more efficient than DPPs. In particular, we propose and investigate Poisson Disk sampling-frequently encountered in the computer graphics community-for this task. We show empirically that our approach improves over standard SGD both in terms of convergence speed as well as final model performance.},
  eventtitle = {33rd {{AAAI Conference}} on {{Artificial Intelligence}} / 31st {{Innovative Applications}} of {{Artificial Intelligence Conference}} / 9th {{AAAI Symposium}} on {{Educational Advances}} in {{Artificial Intelligence}}},
  isbn = {978-1-57735-809-1},
  langid = {english},
  pagetotal = {8},
  annotation = {Web of Science ID: WOS:000486572500033},
  file = {C:\Users\wenji\Zotero\storage\BNUYUUYS\Zhang 等 - 2019 - Active Mini-Batch Sampling Using Repulsive Point P.pdf}
}

@article{laiMultiviewRobustRegression2024,
  title = {Multi-View Robust Regression for Feature Extraction},
  author = {Lai, Zhihui and Chen, Foping and Wen, Jiajun},
  date = {2024-05-01},
  journaltitle = {Pattern Recognition},
  shortjournal = {Pattern Recognition},
  volume = {149},
  pages = {110219},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2023.110219},
  url = {https://www.sciencedirect.com/science/article/pii/S0031320323009160},
  urldate = {2024-06-26},
  abstract = {Recently, Multi-view Discriminant Analysis (MVDA) has been proposed and achieves good performance in multi-view recognition tasks. However, as an extension of LDA, this method still suffers from the small-class problem and has the sensitivity to outliers. In order to address these drawbacks and achieve better performance on multi-view recognition tasks, we proposed Multi-view Robust Regression (MVRR) for multi-view feature extraction. MVRR is a regression based method that imposes L2,1 norm as the metric of the loss function and the regularization term to improve robustness and obtain jointly sparse projection matrices for effective feature extraction. Moreover, we incorporate an orthogonal matrix to regress the extracted features to their scaled label to avoid the small-class problem. Therefore, MVRR guarantees the projection matrix to break through the restriction of the number of class for solving the small-class problem. We also propose an iterative algorithm to compute the optimal solution of MVRR and the convergence of MVRR is proved. Experiments are conducted on four databases to verify the performance of MVRR and the result illustrates that MVRR is robust on multi-view feature extraction.},
  keywords = {Image classification,Linear regression (LR),Small-class problem},
  file = {C:\Users\wenji\Zotero\storage\R3B6AL68\S0031320323009160.html}
}

@software{AddonItem,
  title = {Addon {{Item}}}
}

@inproceedings{oberPromisesPitfallsDeep2021,
  title = {The Promises and Pitfalls of Deep Kernel Learning},
  booktitle = {Proceedings of the {{Thirty-Seventh Conference}} on {{Uncertainty}} in {{Artificial Intelligence}}},
  author = {Ober, Sebastian W. and Rasmussen, Carl E. and family=Wilk, given=Mark, prefix=van der, useprefix=false},
  date = {2021-12-01},
  pages = {1206--1216},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v161/ober21a.html},
  urldate = {2024-07-01},
  abstract = {Deep kernel learning and related techniques promise to combine the representational power of neural networks with the reliable uncertainty estimates of Gaussian processes. One crucial aspect of these models is an expectation that, because they are treated as Gaussian process models optimized using the marginal likelihood, they are protected from overfitting. However, we identify pathological behavior, including overfitting, on a simple toy example. We explore this pathology, explaining its origins and considering how it applies to real datasets. Through careful experimentation on UCI datasets, CIFAR-10, and the UTKFace dataset, we find that the overfitting from overparameterized deep kernel learning, in which the model is “somewhat Bayesian”, can in certain scenarios be worse than that from not being Bayesian at all. However, we find that a fully Bayesian treatment of deep kernel learning can rectify this overfitting and obtain the desired performance improvements over standard neural networks and Gaussian processes.},
  eventtitle = {Uncertainty in {{Artificial Intelligence}}},
  langid = {english},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\K6EBKPYL\\Ober 等 - 2021 - The promises and pitfalls of deep kernel learning.pdf;C\:\\Users\\wenji\\Zotero\\storage\\YW4FN9IZ\\Ober 等 - 2021 - The promises and pitfalls of deep kernel learning.pdf}
}

@article{chrysosRoCGANRobustConditional2020,
  title = {{{RoCGAN}}: {{Robust Conditional GAN}}},
  shorttitle = {{{RoCGAN}}},
  author = {Chrysos, Grigorios G. and Kossaifi, Jean and Zafeiriou, Stefanos},
  date = {2020-11-01},
  journaltitle = {International Journal of Computer Vision},
  shortjournal = {Int J Comput Vis},
  volume = {128},
  number = {10},
  pages = {2665--2683},
  issn = {1573-1405},
  doi = {10.1007/s11263-020-01348-5},
  url = {https://doi.org/10.1007/s11263-020-01348-5},
  urldate = {2024-07-02},
  abstract = {Conditional image generation lies at the heart of computer vision and conditional generative adversarial networks (cGAN) have recently become the method of choice for this task, owing to their superior performance. The focus so far has largely been on performance improvement, with little effort in making cGANs more robust to noise. However, the regression (of the generator) might lead to arbitrarily large errors in the output, which makes cGANs unreliable for real-world applications. In this work, we introduce a novel conditional GAN model, called RoCGAN, which leverages structure in the target space of the model to address the issue. Specifically, we augment the generator with an unsupervised pathway, which promotes the outputs of the generator to span the target manifold, even in the presence of intense noise. We prove that RoCGAN share similar theoretical properties as GAN and establish with both synthetic and real data the merits of our model. We perform a thorough experimental validation on large scale datasets for natural scenes and faces and observe that our model outperforms existing cGAN architectures by a large margin. We also empirically demonstrate the performance of our approach in the face of two types of noise (adversarial and Bernoulli).},
  langid = {english},
  keywords = {Adversarial attacks,Autoencoder,Conditional GAN,Cross-noise experiments,Robust regression,Super-resolution,Unsupervised learning},
  file = {C:\Users\wenji\Zotero\storage\N5UEB5SL\Chrysos 等 - 2020 - RoCGAN Robust Conditional GAN.pdf}
}

@online{mirzaConditionalGenerativeAdversarial2014,
  title = {Conditional {{Generative Adversarial Nets}}},
  author = {Mirza, Mehdi and Osindero, Simon},
  date = {2014-11-06},
  eprint = {1411.1784},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1411.1784},
  url = {http://arxiv.org/abs/1411.1784},
  urldate = {2024-07-02},
  abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\YVBDEFE7\\Mirza 和 Osindero - 2014 - Conditional Generative Adversarial Nets.pdf;C\:\\Users\\wenji\\Zotero\\storage\\BCVZLAP8\\1411.html}
}

@inproceedings{mnihConditionalRestrictedBoltzmann2011,
  title = {Conditional Restricted {{Boltzmann}} Machines for Structured Output Prediction},
  booktitle = {Proceedings of the {{Twenty-Seventh Conference}} on {{Uncertainty}} in {{Artificial Intelligence}}},
  author = {Mnih, Volodymyr and Larochelle, Hugo and Hinton, Geoffrey E.},
  date = {2011-07-14},
  series = {{{UAI}}'11},
  pages = {514--522},
  publisher = {AUAI Press},
  location = {Arlington, Virginia, USA},
  abstract = {Conditional Restricted Boltzmann Machines (CRBMs) are rich probabilistic models that have recently been applied to a wide range of problems, including collaborative filtering, classification, and modeling motion capture data. While much progress has been made in training non-conditional RBMs, these algorithms are not applicable to conditional models and there has been almost no work on training and generating predictions from conditional RBMs for structured output problems. We first argue that standard Contrastive Divergence-based learning may not be suitable for training CRBMs. We then identify two distinct types of structured output prediction problems and propose an improved learning algorithm for each. The first problem type is one where the output space has arbitrary structure but the set of likely output configurations is relatively small, such as in multi-label classification. The second problem is one where the output space is arbitrarily structured but where the output space variability is much greater, such as in image denoising or pixel labeling. We show that the new learning algorithms can work much better than Contrastive Divergence on both types of problems.},
  isbn = {978-0-9749039-7-2},
  file = {C:\Users\wenji\Zotero\storage\PA2VEKY8\Mnih 等 - 2011 - Conditional restricted Boltzmann machines for stru.pdf}
}

@article{zhangConditionalVariationalAutoencoder2021,
  title = {Conditional {{Variational Autoencoder}} for {{Learned Image Reconstruction}}},
  author = {Zhang, Chen and Barbano, Riccardo and Jin, Bangti},
  date = {2021-11},
  journaltitle = {Computation},
  volume = {9},
  number = {11},
  pages = {114},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2079-3197},
  doi = {10.3390/computation9110114},
  url = {https://www.mdpi.com/2079-3197/9/11/114},
  urldate = {2024-07-02},
  abstract = {Learned image reconstruction techniques using deep neural networks have recently gained popularity and have delivered promising empirical results. However, most approaches focus on one single recovery for each observation, and thus neglect information uncertainty. In this work, we develop a novel computational framework that approximates the posterior distribution of the unknown image at each query observation. The proposed framework is very flexible: it handles implicit noise models and priors, it incorporates the data formation process (i.e., the forward operator), and the learned reconstructive properties are transferable between different datasets. Once the network is trained using the conditional variational autoencoder loss, it provides a computationally efficient sampler for the approximate posterior distribution via feed-forward propagation, and the summarizing statistics of the generated samples are used for both point-estimation and uncertainty quantification. We illustrate the proposed framework with extensive numerical experiments on positron emission tomography (with both moderate and low-count levels) showing that the framework generates high-quality samples when compared with state-of-the-art methods.},
  issue = {11},
  langid = {english},
  keywords = {conditional variational autoencoder,deep learning,image reconstruction,uncertainty quantification},
  file = {C:\Users\wenji\Zotero\storage\S9HWT3B2\Zhang 等 - 2021 - Conditional Variational Autoencoder for Learned Im.pdf}
}

@article{liuDECVAEDataAugmentation2024,
  title = {{{DECVAE}}: {{Data}} Augmentation via Conditional Variational Auto-Encoder with Distribution Enhancement for Few-Shot Fault Diagnosis of Mechanical System},
  shorttitle = {{{DECVAE}}},
  author = {Liu, Yikun and Fu, Song and Lin, Lin and Zhang, Sihao and Suo, Shiwei and Xi, Jianjun},
  date = {2024-01},
  journaltitle = {Measurement Science and Technology},
  shortjournal = {Meas. Sci. Technol.},
  volume = {35},
  number = {4},
  pages = {046104},
  publisher = {IOP Publishing},
  issn = {0957-0233},
  doi = {10.1088/1361-6501/ad197c},
  url = {https://dx.doi.org/10.1088/1361-6501/ad197c},
  urldate = {2024-07-04},
  abstract = {Conditional variational autoencoder (CVAE) has the potential for few-sample fault diagnosis of mechanical systems. Nevertheless, the scarcity of faulty samples leads the augmented samples generated using CVAE suffer from limited diversity. To address the issue, a novel CVAE variant namely CVAE with distribution augmentation (DECVAE) is developed, to generate a set of high-quality augmented samples that are different but share very similar characteristics and categories with the corresponding real samples. First, DECVAE add a new sample distribution distance loss into the optimization objective of traditional CVAE. Amplifying this loss in training process can make the augmented samples cover a larger space, thereby improving diversity. Second, DECVAE introduces an auxiliary classifier into traditional CVAE to enhance the sensitivity to category information, keeping the augmented samples class invariance. Furthermore, to ensure that the information of edge-distributed samples can be fully learned and make augmented samples representative and authentic, a novel multi-model independent fine-tuning strategy is designed to train the DECVAE, which utilizes multiple independent models to fairly focus on all samples of the minority class during DECVAE training. Finally, the effectiveness of the developed DECVAE in few-shot fault diagnosis of mechanical systems is verified on a series of comparative experiments.},
  langid = {english}
}

@online{AcceleratingMinibatchStochastic,
  title = {Accelerating {{Minibatch Stochastic Gradient Descent Using Typicality Sampling}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/document/8945166},
  urldate = {2024-07-05},
  file = {C:\Users\wenji\Zotero\storage\S6NRWA8Q\8945166.html}
}

@article{kuleszaDeterminantalPointProcesses2012a,
  title = {Determinantal Point Processes for Machine Learning},
  author = {Kulesza, Alex and Taskar, Ben},
  date = {2012},
  journaltitle = {Foundations and Trends® in Machine Learning},
  shortjournal = {Foundations and Trends® in Machine Learning},
  volume = {5},
  number = {2–3},
  pages = {123--286},
  publisher = {Now Publishers, Inc.},
  issn = {1935-8237},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\R3NVXEDL\\dpps_fnt12.pdf;C\:\\Users\\wenji\\Zotero\\storage\\SC3H7ED3\\Kulesza 和 Taskar - 2012 - Determinantal point processes for machine learning.pdf}
}

@inproceedings{choKernelMethodsDeep2009,
  title = {Kernel {{Methods}} for {{Deep Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Cho, Youngmin and Saul, Lawrence},
  date = {2009},
  volume = {22},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2009/hash/5751ec3e9a4feab575962e78e006250d-Abstract.html},
  urldate = {2024-07-07},
  abstract = {We introduce a new family of positive-definite kernel functions that mimic the computation in large, multilayer neural nets.  These kernel functions can be used in shallow architectures, such as support vector machines (SVMs), or in deep kernel-based architectures that we call multilayer kernel machines (MKMs).  We evaluate SVMs and MKMs with these kernel functions on problems designed to illustrate the advantages of deep architectures.  On several problems, we obtain better results than previous, leading benchmarks from both SVMs with Gaussian kernels as well as deep belief nets.},
  file = {C:\Users\wenji\Zotero\storage\CCTH7RAU\Cho 和 Saul - 2009 - Kernel Methods for Deep Learning.pdf}
}

@online{LearningKernelsSupport,
  title = {Learning with {{Kernels}}: {{Support Vector Machines}}, {{Regularization}}, {{Optimization}}, and {{Beyond}} | {{MIT Press eBooks}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/book/6267332},
  urldate = {2024-07-07},
  file = {C:\Users\wenji\Zotero\storage\F527BBFB\6267332.html}
}

@book{scholkopfLearningKernelsSupport2001,
  title = {Learning with {{Kernels}}: {{Support Vector Machines}}, {{Regularization}}, {{Optimization}}, and {{Beyond}}},
  shorttitle = {Learning with {{Kernels}}},
  author = {Schölkopf, Bernhard and Smola, Alexander J.},
  date = {2001-12-07},
  publisher = {The MIT Press},
  doi = {10.7551/mitpress/4175.001.0001},
  url = {https://direct.mit.edu/books/book/1821/Learning-with-KernelsSupport-Vector-Machines},
  urldate = {2024-07-07},
  abstract = {A comprehensive introduction to Support Vector Machines and related kernel methods.             In the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine (SVM). This gave rise to a new class of theoretically elegant learning machines that use a central concept of SVMs—-kernels—for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics.             Learning with Kernels provides an introduction to SVMs and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years.},
  isbn = {978-0-262-25693-3},
  langid = {english}
}

@inproceedings{boserTrainingAlgorithmOptimal1992,
  title = {A Training Algorithm for Optimal Margin Classifiers},
  booktitle = {Proceedings of the Fifth Annual Workshop on {{Computational}} Learning Theory},
  author = {Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
  date = {1992-07-01},
  series = {{{COLT}} '92},
  pages = {144--152},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/130385.130401},
  url = {https://doi.org/10.1145/130385.130401},
  urldate = {2024-07-07},
  abstract = {A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms.},
  isbn = {978-0-89791-497-0},
  file = {C:\Users\wenji\Zotero\storage\CNJQJHC7\Boser 等 - 1992 - A training algorithm for optimal margin classifier.pdf}
}

@article{perez-cruzKernelMethodsTheir2004,
  title = {Kernel Methods and Their Potential Use in Signal Processing},
  author = {Perez-Cruz, F. and Bousquet, O.},
  date = {2004-05},
  journaltitle = {IEEE Signal Processing Magazine},
  volume = {21},
  number = {3},
  pages = {57--65},
  issn = {1558-0792},
  doi = {10.1109/MSP.2004.1296543},
  url = {https://ieeexplore.ieee.org/document/1296543},
  urldate = {2024-07-07},
  abstract = {The notion of kernels, recently introduced, has drawn much interest as it allows one to obtain nonlinear algorithms from linear ones in a simple and elegant manner. This, in conjunction with the introduction of new linear classification methods such as the support vector machines (SVMs), has produced significant progress in machine learning and related research topics. The success of such algorithms is now spreading as they are applied to more and more domains. Signal processing procedures can benefit from a kernel perspective, making them more powerful and applicable to nonlinear processing in a simpler and nicer way. We present an overview of kernel methods and provide some guidelines for future development in kernel methods, as well as, some perspectives to the actual signal processing problems in which kernel methods are being applied.},
  eventtitle = {{{IEEE Signal Processing Magazine}}},
  keywords = {Data analysis,Hilbert space,Kernel,Least squares methods,Machine learning,Principal component analysis,Signal processing,Signal processing algorithms,Symmetric matrices,Vectors},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\KPVP98QC\\Perez-Cruz 和 Bousquet - 2004 - Kernel methods and their potential use in signal p.pdf;C\:\\Users\\wenji\\Zotero\\storage\\5LTEZKR9\\1296543.html}
}

@article{chenOverviewDeepKernel2016,
  title = {Overview of {{Deep Kernel Learning Based Techniques}} and {{Applications}}.},
  author = {Chen, Xiuyuan and Peng, Xiyuan and Li, Jun-Bao and Peng, Yu},
  date = {2016},
  journaltitle = {J. Netw. Intell.},
  shortjournal = {J. Netw. Intell.},
  volume = {1},
  number = {3},
  pages = {83--98},
  file = {C:\Users\wenji\Zotero\storage\ZDYJCA6C\Chen 等 - 2016 - Overview of Deep Kernel Learning Based Techniques .pdf}
}

@book{vapnikNatureStatisticalLearning2000,
  title = {The {{Nature}} of {{Statistical Learning Theory}}},
  author = {Vapnik, Vladimir N.},
  date = {2000},
  publisher = {Springer},
  location = {New York, NY},
  doi = {10.1007/978-1-4757-3264-1},
  url = {http://link.springer.com/10.1007/978-1-4757-3264-1},
  urldate = {2024-07-07},
  isbn = {978-1-4419-3160-3 978-1-4757-3264-1},
  keywords = {cognition,Conditional probability,control,learning,pattern recognition,Statistical Learning,Statistical Theory,statistics},
  file = {C:\Users\wenji\Zotero\storage\MV4UDRSH\Vapnik - 2000 - The Nature of Statistical Learning Theory.pdf}
}

@inproceedings{macdonaldKernelMethodClassification2004,
  title = {A {{Kernel Method}} for {{Classification}}},
  booktitle = {{{MICAI}} 2004: {{Advances}} in {{Artificial Intelligence}}},
  author = {MacDonald, Donald and Koetsier, Jos and Corchado, Emilio and Fyfe, Colin and Corchado, Juan},
  editor = {Monroy, Raúl and Arroyo-Figueroa, Gustavo and Sucar, Luis Enrique and Sossa, Humberto},
  date = {2004},
  pages = {823--832},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-24694-7_85},
  abstract = {Kernel Maximum Likelihood Hebbian Learning Scale Invariant Maps is a novel technique developed to facilitate the clustering of complex data effectively and efficiently and that is characterised for converging remarkably quickly. The combination of Maximum Likelihood Hebbian Learning Scale Invariant Map and the Kernel Space provides a very smooth scale invariant quantisation which can be used as a clustering technique. The efficiency of this method have been used to analyse an oceanographic problem.},
  isbn = {978-3-540-24694-7},
  langid = {english},
  keywords = {Kernel Method,Kernel Space,Output Neuron,Scale Invariant,Winning Neuron},
  file = {C:\Users\wenji\Zotero\storage\EV8XKTYQ\MacDonald 等 - 2004 - A Kernel Method for Classification.pdf}
}

@article{kwokPreimageProblemKernel2004,
  title = {The Pre-Image Problem in Kernel Methods},
  author = {Kwok, J.T.-Y. and Tsang, I.W.-H.},
  date = {2004-11},
  journaltitle = {IEEE Transactions on Neural Networks},
  volume = {15},
  number = {6},
  pages = {1517--1525},
  issn = {1941-0093},
  doi = {10.1109/TNN.2004.837781},
  url = {https://ieeexplore.ieee.org/abstract/document/1353287?casa_token=dAxW7M-0Gj8AAAAA:pLr0LwXL12q50zJNJnHUyly35b0XuVQfidDNicC8B6muhplz_MoHlyP_64Hno7jTQ7cF6mXveG0},
  urldate = {2024-07-07},
  abstract = {In this paper, we address the problem of finding the pre-image of a feature vector in the feature space induced by a kernel. This is of central importance in some kernel applications, such as on using kernel principal component analysis (PCA) for image denoising. Unlike the traditional method in which relies on nonlinear optimization, our proposed method directly finds the location of the pre-image based on distance constraints in the feature space. It is noniterative, involves only linear algebra and does not suffer from numerical instability or local minimum problems. Evaluations on performing kernel PCA and kernel clustering on the USPS data set show much improved performance.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}}},
  keywords = {Clustering algorithms,Constraint optimization,Image denoising,Kernel,Kernel principal component analysis (PCA),Linear algebra,multidimensional scaling (MDS),Noise reduction,Optimization methods,Performance evaluation,pre-image,Principal component analysis,Space technology},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\I9ZKDW7C\\Kwok 和 Tsang - 2004 - The pre-image problem in kernel methods.pdf;C\:\\Users\\wenji\\Zotero\\storage\\A3KBUI8S\\1353287.html}
}

@article{cortesSupportvectorNetworks1995,
  title = {Support-Vector Networks},
  author = {Cortes, Corinna and Vapnik, Vladimir},
  date = {1995-09-01},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {20},
  number = {3},
  pages = {273--297},
  issn = {1573-0565},
  doi = {10.1007/BF00994018},
  url = {https://doi.org/10.1007/BF00994018},
  urldate = {2024-07-07},
  abstract = {Thesupport-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.},
  langid = {english},
  keywords = {efficient learning algorithms,neural networks,pattern recognition,polynomial classifiers,radial basis function classifiers},
  file = {C:\Users\wenji\Zotero\storage\638LMW5U\Cortes 和 Vapnik - 1995 - Support-vector networks.pdf}
}

@online{LeastSquaresSupport,
  title = {Least {{Squares Support Vector Machines}}},
  url = {https://www.worldscientific.com/worldscibooks/10.1142/5089},
  urldate = {2024-07-07},
  abstract = {This book focuses on Least Squares Support Vector Machines (LS-SVMs) which are reformulations to standard SVMs. LS-SVMs are closely related to regularization networks and Gaussian processes but add...},
  langid = {english},
  file = {C:\Users\wenji\Zotero\storage\I69I4ZSG\5089.html}
}

@book{suykensLeastSquaresSupport2002,
  title = {Least Squares Support Vector Machines},
  author = {Suykens, Johan A. K.},
  date = {2002},
  publisher = {World Scientific},
  location = {River Edge, NJ},
  abstract = {Annotation Focuses on the Least Squares Support Vector Machines (LS-SVMs) which are reformulations to standard SVMs. The authors explain the natural links between LS-SVM classifiers and kernel Fisher discriminant analysis. Bayesian inference of LS-SVM models is discussed, together with methods for imposing sparseness and employing robust statistics.},
  isbn = {981-238-151-1},
  keywords = {Algorithms,Kernel functions,Least squares,Machine learning}
}

@online{KernelMethodOverview,
  title = {Kernel {{Method}} - an Overview | {{ScienceDirect Topics}}},
  url = {https://www-sciencedirect-com.kuleuven.e-bronnen.be/topics/mathematics/kernel-method},
  urldate = {2024-07-08},
  file = {C:\Users\wenji\Zotero\storage\D279PPXD\kernel-method.html}
}

@inproceedings{ngSpectralClusteringAnalysis2001,
  title = {On {{Spectral Clustering}}: {{Analysis}} and an Algorithm},
  shorttitle = {On {{Spectral Clustering}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ng, Andrew and Jordan, Michael and Weiss, Yair},
  date = {2001},
  volume = {14},
  publisher = {MIT Press},
  url = {https://proceedings.neurips.cc/paper/2001/hash/801272ee79cfde7fa5960571fee36b9b-Abstract.html},
  urldate = {2024-07-08},
  abstract = {Despite many empirical successes of spectral  clustering  methods(cid:173) algorithms  that  cluster  points  using  eigenvectors  of  matrices  de(cid:173) rived  from  the  data- there  are  several  unresolved  issues.  First,  there  are  a  wide  variety  of  algorithms  that  use  the  eigenvectors  in  slightly  different  ways.  Second,  many of these  algorithms  have  no  proof that  they  will  actually  compute  a  reasonable  clustering.  In  this  paper,  we  present  a  simple  spectral  clustering  algorithm  that can be implemented using a  few  lines  of Matlab.  Using  tools  from  matrix  perturbation  theory,  we  analyze  the  algorithm,  and  give  conditions  under  which  it  can  be  expected  to  do  well.  We  also  show  surprisingly  good  experimental  results  on  a  number  of  challenging clustering problems.},
  file = {C:\Users\wenji\Zotero\storage\78WB7FRF\Ng 等 - 2001 - On Spectral Clustering Analysis and an algorithm.pdf}
}

@incollection{rasmussenGaussianProcessesMachine2004,
  title = {Gaussian {{Processes}} in {{Machine Learning}}},
  booktitle = {Advanced {{Lectures}} on {{Machine Learning}}: {{ML Summer Schools}} 2003, {{Canberra}}, {{Australia}}, {{February}} 2 - 14, 2003, {{Tübingen}}, {{Germany}}, {{August}} 4 - 16, 2003, {{Revised Lectures}}},
  author = {Rasmussen, Carl Edward},
  editor = {Bousquet, Olivier and family=Luxburg, given=Ulrike, prefix=von, useprefix=true and Rätsch, Gunnar},
  date = {2004},
  pages = {63--71},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-28650-9_4},
  url = {https://doi.org/10.1007/978-3-540-28650-9_4},
  urldate = {2024-07-08},
  abstract = {We give a basic introduction to Gaussian Process regression models. We focus on understanding the role of the stochastic process and how it is used to define a distribution over functions. We present the simple equations for incorporating training data and examine how to learn the hyperparameters using the marginal likelihood. We explain the practical advantages of Gaussian Process and end with conclusions and a look at the current trends in GP work.},
  isbn = {978-3-540-28650-9},
  langid = {english},
  keywords = {Covariance Function,Gaussian Process,Joint Gaussian Distribution,Marginal Likelihood,Posterior Variance},
  file = {C:\Users\wenji\Zotero\storage\KXS84CE2\Rasmussen - 2004 - Gaussian Processes in Machine Learning.pdf}
}

@article{xuOverviewDeepGenerative2015a,
  title = {An {{Overview}} of {{Deep Generative Models}}},
  author = {Xu, Jungang and Li, Hui and Zhou, Shilong},
  date = {2015-03-04},
  journaltitle = {IETE Technical Review},
  volume = {32},
  number = {2},
  pages = {131--139},
  publisher = {Taylor \& Francis},
  issn = {0256-4602},
  doi = {10.1080/02564602.2014.987328},
  url = {https://doi.org/10.1080/02564602.2014.987328},
  urldate = {2024-07-09},
  abstract = {As an important category of deep models, deep generative model has attracted more and more attention with the proposal of Deep Belief Networks (DBNs) and the fast greedy training algorithm based on restricted Boltzmann machines (RBMs). In the past few years, many different deep generative models are proposed and used in the area of Artificial Intelligence. In this paper, three important deep generative models including DBNs, deep autoencoder, and deep Boltzmann machine are reviewed. In addition, some successful applications of deep generative models in image processing, speech recognition and information retrieval are also introduced and analysed.},
  keywords = {Deep autoencoder,Deep belief networks,Deep boltzmann machine,Deep generative model,Restricted boltzmann machine}
}

@book{jebaraMachineLearning2004,
  title = {Machine {{Learning}}},
  author = {Jebara, Tony},
  date = {2004},
  publisher = {Springer US},
  location = {Boston, MA},
  doi = {10.1007/978-1-4419-9011-2},
  url = {http://link.springer.com/10.1007/978-1-4419-9011-2},
  urldate = {2024-07-09},
  isbn = {978-1-4613-4756-9 978-1-4419-9011-2},
  keywords = {computer science,Extension,learning,machine learning}
}

@inproceedings{oussidiDeepGenerativeModels2018,
  title = {Deep Generative Models: {{Survey}}},
  shorttitle = {Deep Generative Models},
  booktitle = {2018 {{International Conference}} on {{Intelligent Systems}} and {{Computer Vision}} ({{ISCV}})},
  author = {Oussidi, Achraf and Elhassouny, Azeddine},
  date = {2018-04},
  pages = {1--8},
  doi = {10.1109/ISACV.2018.8354080},
  url = {https://ieeexplore.ieee.org/abstract/document/8354080},
  urldate = {2024-07-12},
  abstract = {Generative models have found their way to the forefront of deep learning the last decade and so far, it seems that the hype will not fade away any time soon. In this paper, we give an overview of the most important building blocks of most recent revolutionary deep generative models such as RBM, DBM, DBN, VAE and GAN. We will also take a look at three of state-of-the-art generative models, namely PixelRNN, DRAW and NADE. We will delve into their unique architectures, the learning procedures and their potential and limitations. We will also review some of the known issues that arise when trying to design and train deep generative architectures using shallow ones and how different models deal with these issues. This paper is not meant to be a comprehensive study of these models, but rather a starting point for those who bear an interest in the field.},
  eventtitle = {2018 {{International Conference}} on {{Intelligent Systems}} and {{Computer Vision}} ({{ISCV}})},
  keywords = {Architecture,Data models,Decoding,Neural networks,Neurons,Probabilistic logic,Training},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\JRX76552\\Oussidi 和 Elhassouny - 2018 - Deep generative models Survey.pdf;C\:\\Users\\wenji\\Zotero\\storage\\SNRMU9BY\\8354080.html}
}

@inproceedings{sajjadiAssessingGenerativeModels2018,
  title = {Assessing {{Generative Models}} via {{Precision}} and {{Recall}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Sajjadi, Mehdi S. M. and Bachem, Olivier and Lucic, Mario and Bousquet, Olivier and Gelly, Sylvain},
  date = {2018},
  volume = {31},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2018/hash/f7696a9b362ac5a51c3dc8f098b73923-Abstract.html},
  urldate = {2024-07-12},
  abstract = {Recent advances in generative modeling have led to an increased interest in the study of statistical divergences as means of model comparison. Commonly used evaluation methods, such as the Frechet Inception Distance (FID), correlate well with the perceived quality of samples and are sensitive to mode dropping. However, these metrics are unable to distinguish between different failure cases since they only yield one-dimensional scores. We propose a novel definition of precision and recall for distributions which disentangles the divergence into two separate dimensions. The proposed notion is intuitive, retains desirable properties, and naturally leads to an efficient algorithm that can be used to evaluate generative models. We relate this notion to total variation as well as to recent evaluation metrics such as Inception Score and FID. To demonstrate the practical utility of the proposed approach we perform an empirical study on several variants of Generative Adversarial Networks and Variational Autoencoders. In an extensive set of experiments we show that the proposed metric is able to disentangle the quality of generated samples from the coverage of the target distribution.},
  file = {C:\Users\wenji\Zotero\storage\HRW57XX4\Sajjadi 等 - 2018 - Assessing Generative Models via Precision and Reca.pdf}
}

@inproceedings{choiFairGenerativeModeling2020a,
  title = {Fair Generative Modeling via Weak Supervision},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Choi, Kristy and Grover, Aditya and Singh, Trisha and Shu, Rui and Ermon, Stefano},
  date = {2020-07-13},
  series = {{{ICML}}'20},
  volume = {119},
  pages = {1887--1898},
  publisher = {JMLR.org},
  abstract = {Real-world datasets are often biased with respect to key demographic factors such as race and gender. Due to the latent nature of the underlying factors, detecting and mitigating bias is especially challenging for unsupervised machine learning. We present a weakly supervised algorithm for overcoming dataset bias for deep generative models. Our approach requires access to an additional small, unlabeled reference dataset as the supervision signal, thus sidestepping the need for explicit labels on the underlying bias factors. Using this supplementary dataset, we detect the bias in existing datasets via a density ratio technique and learn generative models which efficiently achieve the twin goals of: 1) data efficiency by using training examples from both biased and reference datasets for learning; and 2) data generation close in distribution to the reference dataset at test time. Empirically, we demonstrate the efficacy of our approach which reduces bias w.r.t. latent factors by an average of up to 34.6\% over baselines for comparable image generation using generative adversarial networks.},
  file = {C:\Users\wenji\Zotero\storage\6X63A8R7\Choi 等 - 2020 - Fair generative modeling via weak supervision.pdf}
}

@article{sattigeriFairnessGANGenerating2019,
  title = {Fairness {{GAN}}: {{Generating}} Datasets with Fairness Properties Using a Generative Adversarial Network},
  shorttitle = {Fairness {{GAN}}},
  author = {Sattigeri, P. and Hoffman, S. C. and Chenthamarakshan, V. and Varshney, K. R.},
  date = {2019-07},
  journaltitle = {IBM Journal of Research and Development},
  volume = {63},
  number = {4/5},
  pages = {3:1-3:9},
  issn = {0018-8646},
  doi = {10.1147/JRD.2019.2945519},
  url = {https://ieeexplore.ieee.org/document/8869910},
  urldate = {2024-07-12},
  abstract = {We introduce the Fairness GAN (generative adversarial network), an approach for generating a dataset that is plausibly similar to a given multimedia dataset, but is more fair with respect to protected attributes in decision making. We propose a novel auxiliary classifier GAN that strives for demographic parity or equality of opportunity and show empirical results on several datasets, including the CelebFaces Attributes (CelebA) dataset, the Quick, Draw! dataset, and a dataset of soccer player images and the offenses for which they were called. The proposed formulation is well suited to absorbing unlabeled data; we leverage this to augment the soccer dataset with the much larger CelebA dataset. The methodology tends to improve demographic parity and equality of opportunity while generating plausible images.},
  eventtitle = {{{IBM Journal}} of {{Research}} and {{Development}}},
  keywords = {Decision making,Employment,Gallium nitride,Generative adversarial networks,Generators,Sports,Training},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\VR5NP3D2\\Sattigeri 等 - 2019 - Fairness GAN Generating datasets with fairness pr.pdf;C\:\\Users\\wenji\\Zotero\\storage\\ZQ3EIMPW\\8869910.html}
}

@inproceedings{xuFairGANAchievingFair2019,
  title = {{{FairGAN}}+: {{Achieving Fair Data Generation}} and {{Classification}} through {{Generative Adversarial Nets}}},
  shorttitle = {{{FairGAN}}+},
  booktitle = {2019 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  author = {Xu, Depeng and Yuan, Shuhan and Zhang, Lu and Wu, Xintao},
  date = {2019-12},
  pages = {1401--1406},
  doi = {10.1109/BigData47090.2019.9006322},
  url = {https://ieeexplore.ieee.org/abstract/document/9006322?casa_token=qygDgheQE3sAAAAA:V7T6w_6HUOu3Hbswdpy2sC8I3l01CARKKiM7aHsGUQUn17o0y19CDVTaXJbefYRcnx29ZCA63AA},
  urldate = {2024-07-12},
  abstract = {How to achieve fairness is important for next generation machine learning. Two tasks that are equally important in fair machine learning are how to obtain fair datasets and how to build fair classifiers. In this work, we propose a new generative adversarial network (GAN) model for fair machine learning, named FairGAN+. FairGAN+ contains a generator to generate close-to-real samples, a classifier to predict class labels and three discriminators to assist adversarial learning. FairGAN+ simultaneously achieves fair data generation and classification by co-training the generative model and the classifier through joint adversarial games with the discriminators. Evaluations on real world data show the effectiveness of FairGAN+ on both fair data generation and fair classification.},
  eventtitle = {2019 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  keywords = {Data models,fair classification,fair data generation,fairness-aware learning,Gallium nitride,Games,generative adversarial networks,Generative adversarial networks,Generators,Machine learning,Task analysis},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\DVU3P5FL\\Xu 等 - 2019 - FairGAN+ Achieving Fair Data Generation and Class.pdf;C\:\\Users\\wenji\\Zotero\\storage\\44U3VB4D\\9006322.html}
}

@article{bernardoGenerativeDiscriminativeGetting2007,
  title = {Generative or Discriminative? Getting the Best of Both Worlds},
  author = {family=Bernardo, given=JM, given-i=JM and family=Bayarri, given=MJ, given-i=MJ and family=Berger, given=JO, given-i=JO and family=Dawid, given=AP, given-i=AP and Heckerman, D and family=Smith, given=AFM, given-i=AFM and West, M},
  date = {2007},
  journaltitle = {Bayesian statistics},
  shortjournal = {Bayesian statistics},
  volume = {8},
  number = {3},
  pages = {3--24},
  file = {C:\Users\wenji\Zotero\storage\BAYEM7ZK\Bernardo 等 - 2007 - Generative or discriminative getting the best of .pdf}
}

@online{ComprehensiveSurveyAnalysis,
  title = {A Comprehensive Survey and Analysis of Generative Models in Machine Learning - {{ScienceDirect}}},
  url = {https://www-sciencedirect-com.kuleuven.e-bronnen.be/science/article/pii/S1574013720303853},
  urldate = {2024-07-13}
}

@article{gmComprehensiveSurveyAnalysis2020,
  title = {A Comprehensive Survey and Analysis of Generative Models in Machine Learning},
  author = {Gm, Harshvardhan and Gourisaria, Mahendra Kumar and Pandey, Manjusha and Rautaray, Siddharth Swarup},
  date = {2020-11-01},
  journaltitle = {Computer Science Review},
  shortjournal = {Computer Science Review},
  volume = {38},
  pages = {100285},
  issn = {1574-0137},
  doi = {10.1016/j.cosrev.2020.100285},
  url = {https://www.sciencedirect.com/science/article/pii/S1574013720303853},
  urldate = {2024-07-13},
  abstract = {Generative models have been in existence for many decades. In the field of machine learning, we come across many scenarios when directly learning a target is intractable through discriminative models, and in such cases the joint distribution of the target and the training data is approximated and generated. These generative models help us better represent or model a set of data by generating data in the form of Markov chains or simply employing a generative iterative process to do the same. With the recent innovation of Generative Adversarial Networks (GANs), it is now possible to make use of AI to generate pieces of art, music, etc. with a high extent of realism. In this paper, we review and analyse critically all the generative models, namely Gaussian Mixture Models (GMM), Hidden Markov Models (HMM), Latent Dirichlet Allocation (LDA), Restricted Boltzmann Machines (RBM), Deep Belief Networks (DBN), Deep Boltzmann Machines (DBM), and GANs. We study their algorithms and implement each of the models to provide the reader some insights on which generative model to pick from while dealing with a problem. We also provide some noteworthy contributions done in the past to these models from the literature.},
  keywords = {Bayesian inference,Deep learning,Generative models,Machine learning,Neural networks},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\Z2Z4W3CQ\\Gm 等 - 2020 - A comprehensive survey and analysis of generative .pdf;C\:\\Users\\wenji\\Zotero\\storage\\YH3HZK5Y\\S1574013720303853.html}
}

@article{girinDynamicalVariationalAutoencoders2021,
  title = {Dynamical {{Variational Autoencoders}}: {{A Comprehensive Review}}},
  shorttitle = {Dynamical {{Variational Autoencoders}}},
  author = {Girin, Laurent and Leglaive, Simon and Bie, Xiaoyu and Diard, Julien and Hueber, Thomas and Alameda-Pineda, Xavier},
  date = {2021},
  journaltitle = {Foundations and Trends® in Machine Learning},
  shortjournal = {FNT in Machine Learning},
  volume = {15},
  number = {1-2},
  eprint = {2008.12595},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  pages = {1--175},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000089},
  url = {http://arxiv.org/abs/2008.12595},
  urldate = {2024-07-13},
  abstract = {Variational autoencoders (VAEs) are powerful deep generative models widely used to represent high-dimensional complex data through a low-dimensional latent space learned in an unsupervised manner. In the original VAE model, the input data vectors are processed independently. Recently, a series of papers have presented different extensions of the VAE to process sequential data, which model not only the latent space but also the temporal dependencies within a sequence of data vectors and corresponding latent vectors, relying on recurrent neural networks or state-space models. In this paper, we perform a literature review of these models. We introduce and discuss a general class of models, called dynamical variational autoencoders (DVAEs), which encompasses a large subset of these temporal VAE extensions. Then, we present in detail seven recently proposed DVAE models, with an aim to homogenize the notations and presentation lines, as well as to relate these models with existing classical temporal models. We have reimplemented those seven DVAE models and present the results of an experimental benchmark conducted on the speech analysis-resynthesis task (the PyTorch code is made publicly available). The paper concludes with a discussion on important issues concerning the DVAE class of models and future research guidelines.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\73SNW6LS\\Girin 等 - 2021 - Dynamical Variational Autoencoders A Comprehensiv.pdf;C\:\\Users\\wenji\\Zotero\\storage\\RUG4CVUR\\2008.html}
}

@article{creswellGenerativeAdversarialNetworks2018,
  title = {Generative {{Adversarial Networks}}: {{An Overview}}},
  shorttitle = {Generative {{Adversarial Networks}}},
  author = {Creswell, Antonia and White, Tom and Dumoulin, Vincent and Arulkumaran, Kai and Sengupta, Biswa and Bharath, Anil A.},
  date = {2018-01},
  journaltitle = {IEEE Signal Processing Magazine},
  volume = {35},
  number = {1},
  pages = {53--65},
  issn = {1558-0792},
  doi = {10.1109/MSP.2017.2765202},
  url = {https://ieeexplore.ieee.org/abstract/document/8253599?casa_token=uF8uFpDhXIAAAAAA:Xi7VQ97_SWZIBZ6q11j_rhyLiz11bILFTIzzERzsRzizkKwwQxMbeR93fsyWG42bNO9a2TTqm5w},
  urldate = {2024-07-14},
  abstract = {Generative adversarial networks (GANs) provide a way to learn deep representations without extensively annotated training data. They achieve this by deriving backpropagation signals through a competitive process involving a pair of networks. The representations that can be learned by GANs may be used in a variety of applications, including image synthesis, semantic image editing, style transfer, image superresolution, and classification. The aim of this review article is to provide an overview of GANs for the signal processing community, drawing on familiar analogies and concepts where possible. In addition to identifying different methods for training and constructing GANs, we also point to remaining challenges in their theory and application.},
  eventtitle = {{{IEEE Signal Processing Magazine}}},
  keywords = {Convolutional codes,Data models,Generators,Image resolution,Machine learning,Semantics,Signal resolution,Training data},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\IYZJLWJA\\Creswell 等 - 2018 - Generative Adversarial Networks An Overview.pdf;C\:\\Users\\wenji\\Zotero\\storage\\28ECR3ZU\\8253599.html}
}

@article{aggarwalGenerativeAdversarialNetwork2021,
  title = {Generative Adversarial Network: {{An}} Overview of Theory and Applications},
  shorttitle = {Generative Adversarial Network},
  author = {Aggarwal, Alankrita and Mittal, Mamta and Battineni, Gopi},
  date = {2021-04-01},
  journaltitle = {International Journal of Information Management Data Insights},
  shortjournal = {International Journal of Information Management Data Insights},
  volume = {1},
  number = {1},
  pages = {100004},
  issn = {2667-0968},
  doi = {10.1016/j.jjimei.2020.100004},
  url = {https://www.sciencedirect.com/science/article/pii/S2667096820300045},
  urldate = {2024-07-14},
  abstract = {In recent times, image segmentation has been involving everywhere including disease diagnosis to autonomous vehicle driving. In computer vision, this image segmentation is one of the vital works and it is relatively complicated than other vision undertakings as it needs low-level spatial data. Especially, Deep Learning has impacted the field of segmentation incredibly and gave us today different successful models. The deep learning associated Generated Adversarial Networks (GAN) has presenting remarkable outcomes on image segmentation. In this study, the authors have presented a systematic review analysis on recent publications of GAN models and their applications. Three libraries such as Embase (Scopus), WoS, and PubMed have been considered for searching the relevant papers available in this area. Search outcomes have identified 2084 documents, after two-phase screening 52 potential records are included for final review. The following applications of GAN have been emerged: 3D object generation, medicine, pandemics, image processing, face detection, texture transfer, and traffic controlling. Before 2016, research in this field was limited and thereafter its practical usage came into existence worldwide. The present study also envisions the challenges associated with GAN and paves the path for future research in this realm.},
  keywords = {Big data,Deep learning,GAN,Image mining,Literature review,Neural networks},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\YRLWHEE4\\Aggarwal 等 - 2021 - Generative adversarial network An overview of the.pdf;C\:\\Users\\wenji\\Zotero\\storage\\DWP5L6J3\\S2667096820300045.html}
}

@inproceedings{odenaConditionalImageSynthesis2017a,
  title = {Conditional {{Image Synthesis}} with {{Auxiliary Classifier GANs}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Odena, Augustus and Olah, Christopher and Shlens, Jonathon},
  date = {2017-07-17},
  pages = {2642--2651},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v70/odena17a.html},
  urldate = {2024-07-15},
  abstract = {In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128×128128×128128\textbackslash times 128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128×128128×128128\textbackslash times 128 samples are more than twice as discriminable as artificially resized 32×3232×3232\textbackslash times 32 samples. In addition, 84.7\textbackslash\% of the classes have samples exhibiting diversity comparable to real ImageNet data.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\UIBUI2CP\\Odena et al_2017_Conditional Image Synthesis with Auxiliary Classifier GANs.pdf;C\:\\Users\\wenji\\Zotero\\storage\\WU9SPD3N\\Odena 等 - 2017 - Conditional Image Synthesis with Auxiliary Classif.pdf}
}

@online{betzalelStudyEvaluationGenerative2022,
  title = {A {{Study}} on the {{Evaluation}} of {{Generative Models}}},
  author = {Betzalel, Eyal and Penso, Coby and Navon, Aviv and Fetaya, Ethan},
  date = {2022-06-22},
  eprint = {2206.10935},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2206.10935},
  url = {http://arxiv.org/abs/2206.10935},
  urldate = {2024-07-17},
  abstract = {Implicit generative models, which do not return likelihood values, such as generative adversarial networks and diffusion models, have become prevalent in recent years. While it is true that these models have shown remarkable results, evaluating their performance is challenging. This issue is of vital importance to push research forward and identify meaningful gains from random noise. Currently, heuristic metrics such as the Inception score (IS) and Frechet Inception Distance (FID) are the most common evaluation metrics, but what they measure is not entirely clear. Additionally, there are questions regarding how meaningful their score actually is. In this work, we study the evaluation metrics of generative models by generating a high-quality synthetic dataset on which we can estimate classical metrics for comparison. Our study shows that while FID and IS do correlate to several f-divergences, their ranking of close models can vary considerably making them problematic when used for fain-grained comparison. We further used this experimental setting to study which evaluation metric best correlates with our probabilistic metrics. Lastly, we look into the base features used for metrics such as FID.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\8SKIB94M\\Betzalel et al_2022_A Study on the Evaluation of Generative Models.pdf;C\:\\Users\\wenji\\Zotero\\storage\\HQDX5EGG\\2206.html}
}

@software{AddonItema,
  title = {Addon {{Item}}}
}

@article{karamizadehOverviewPrincipalComponent2013,
  title = {An {{Overview}} of {{Principal Component Analysis}}},
  author = {Karamizadeh, Sasan and Abdullah, Shahidan M. and Manaf, Azizah A. and Zamani, Mazdak and Hooman, Alireza},
  date = {2013-08-01},
  journaltitle = {Journal of Signal and Information Processing},
  volume = {4},
  number = {3},
  pages = {173--175},
  publisher = {Scientific Research Publishing},
  doi = {10.4236/jsip.2013.43B031},
  url = {https://www.scirp.org/journal/paperinformation?paperid=38103},
  urldate = {2024-07-17},
  abstract = {The principal component analysis (PCA) is a kind of algorithms in biometrics. It is a statistics technical and used orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables. PCA also is a tool to reduce multidimensional data to lower dimensions while retaining most of the information. It covers standard deviation, covariance, and eigenvectors. This background knowledge is meant to make the PCA section very straightforward, but can be skipped if the concepts are already familiar.},
  issue = {3},
  langid = {english},
  file = {C:\Users\wenji\Zotero\storage\BJKFKFKB\Karamizadeh et al_2013_An Overview of Principal Component Analysis.pdf}
}

@book{PatternRecognitionMachine,
  title = {Pattern {{Recognition}} and {{Machine Learning}}},
  url = {https://link.springer.com/book/9780387310732},
  urldate = {2024-07-17},
  langid = {english},
  file = {C:\Users\wenji\Zotero\storage\ACMKLL6J\9780387310732.html}
}

@book{PatternRecognitionMachinea,
  title = {Pattern {{Recognition}} and {{Machine Learning}}},
  url = {https://link.springer.com/book/9780387310732},
  urldate = {2024-07-17},
  langid = {english}
}

@book{bishopPatternRecognitionMachine2006,
  title = {Pattern Recognition and Machine Learning},
  author = {Bishop, Christopher M and Nasrabadi, Nasser M},
  date = {2006},
  volume = {4},
  number = {4},
  publisher = {Springer},
  file = {C:\Users\wenji\Zotero\storage\LG38XKTU\Bishop_Nasrabadi_2006_Pattern recognition and machine learning.pdf}
}

@article{suykensLeastSquaresSupport1999,
  title = {Least {{Squares Support Vector Machine Classifiers}}},
  author = {Suykens, J.A.K. and Vandewalle, J.},
  date = {1999-06-01},
  journaltitle = {Neural Processing Letters},
  shortjournal = {Neural Processing Letters},
  volume = {9},
  number = {3},
  pages = {293--300},
  issn = {1573-773X},
  doi = {10.1023/A:1018628609742},
  url = {https://doi.org/10.1023/A:1018628609742},
  urldate = {2024-07-18},
  abstract = {In this letter we discuss a least squares version for support vector machine (SVM) classifiers. Due to equality type constraints in the formulation, the solution follows from solving a set of linear equations, instead of quadratic programming for classical SVM's. The approach is illustrated on a two-spiral benchmark classification problem.},
  langid = {english},
  keywords = {classification,linear least squares,radial basis function kernel,support vector machines},
  file = {C:\Users\wenji\Zotero\storage\HG46K5L7\Suykens_Vandewalle_1999_Least Squares Support Vector Machine Classifiers.pdf}
}

@article{suykensSupportVectorMachine2003a,
  title = {A Support Vector Machine Formulation to {{PCA}} Analysis and Its Kernel Version},
  author = {Suykens, J.A.K. and Van Gestel, T. and Vandewalle, J. and De Moor, B.},
  date = {2003-03},
  journaltitle = {IEEE Transactions on Neural Networks},
  volume = {14},
  number = {2},
  pages = {447--450},
  issn = {1941-0093},
  doi = {10.1109/TNN.2003.809414},
  url = {https://ieeexplore.ieee.org/abstract/document/1189643?casa_token=T0B5ihziz2MAAAAA:bbyAWhrgkD5gEPOK-DJwnPul-BZLNDJg3k5lEkMRWPNgWvF1Q06_i8WOeFByFg-tSdr3midJ0_s},
  urldate = {2024-07-18},
  abstract = {In this paper, we present a simple and straightforward primal-dual support vector machine formulation to the problem of principal component analysis (PCA) in dual variables. By considering a mapping to a high-dimensional feature space and application of the kernel trick (Mercer theorem), kernel PCA is obtained as introduced by Scholkopf et al. (2002). While least squares support vector machine classifiers have a natural link with the kernel Fisher discriminant analysis (minimizing the within class scatter around targets +1 and -1), for PCA analysis one can take the interpretation of a one-class modeling problem with zero target value around which one maximizes the variance. The score variables are interpreted as error variables within the problem formulation. In this way primal-dual constrained optimization problem interpretations to the linear and kernel PCA analysis are obtained in a similar style as for least square-support vector machine classifiers.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}}},
  keywords = {Analysis of variance,Constraint optimization,Kernel,Knowledge management,Least squares methods,Predictive models,Principal component analysis,Scattering,Support vector machine classification,Support vector machines},
  file = {C:\Users\wenji\Zotero\storage\RR3ABL26\1189643.html}
}

@online{PreimageProblemKernelBased,
  title = {Preimage {{Problem}} in {{Kernel-Based Machine Learning}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/document/5714388},
  urldate = {2024-07-19},
  file = {C:\Users\wenji\Zotero\storage\3MKPX3SZ\5714388.html}
}

@article{honeinePreimageProblemKernelBased2011,
  title = {Preimage {{Problem}} in {{Kernel-Based Machine Learning}}},
  author = {Honeine, Paul and Richard, Cedric},
  date = {2011-03},
  journaltitle = {IEEE Signal Processing Magazine},
  volume = {28},
  number = {2},
  pages = {77--88},
  issn = {1558-0792},
  doi = {10.1109/MSP.2010.939747},
  url = {https://ieeexplore.ieee.org/document/5714388},
  urldate = {2024-07-19},
  abstract = {While the nonlinear mapping from the input space to the feature space is central in kernel methods, the reverse mapping from the feature space back to the input space is also of primary interest. This is the case in many applications, including kernel principal component analysis (PCA) for signal and image denoising. Unfortunately, it turns out that the reverse mapping generally does not exist and only a few elements in the feature space have a valid preimage in the input space. The preimage problem consists of finding an approximate solution by identifying data in the input space based on their corresponding features in the high dimensional feature space. It is essentially a dimensionality-reduction problem, and both have been intimately connected in their historical evolution, as studied in this article.},
  eventtitle = {{{IEEE Signal Processing Magazine}}},
  keywords = {Classification algorithms,Kernel,Machine learning,Noise reduction,Optimization,Principal component analysis,Signal processing algorithms},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\8ADWQNHC\\Honeine_Richard_2011_Preimage Problem in Kernel-Based Machine Learning.pdf;C\:\\Users\\wenji\\Zotero\\storage\\X8LMHERF\\5714388.html}
}

@article{kwokPreimageProblemKernel2004a,
  title = {The Pre-Image Problem in Kernel Methods},
  author = {Kwok, J.T.-Y. and Tsang, I.W.-H.},
  date = {2004-11},
  journaltitle = {IEEE Transactions on Neural Networks},
  volume = {15},
  number = {6},
  pages = {1517--1525},
  issn = {1941-0093},
  doi = {10.1109/TNN.2004.837781},
  url = {https://ieeexplore.ieee.org/abstract/document/1353287?casa_token=1Ylk9ZB3xgQAAAAA:yIB8Ku8aOiskJnA71L1Vw_WdPxRI7YN0WRRMciMiU1yBCEYxHnQUQ0tUPIacgjsr4CpJfu_Xles},
  urldate = {2024-07-19},
  abstract = {In this paper, we address the problem of finding the pre-image of a feature vector in the feature space induced by a kernel. This is of central importance in some kernel applications, such as on using kernel principal component analysis (PCA) for image denoising. Unlike the traditional method in which relies on nonlinear optimization, our proposed method directly finds the location of the pre-image based on distance constraints in the feature space. It is noniterative, involves only linear algebra and does not suffer from numerical instability or local minimum problems. Evaluations on performing kernel PCA and kernel clustering on the USPS data set show much improved performance.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}}},
  keywords = {Clustering algorithms,Constraint optimization,Image denoising,Kernel,Kernel principal component analysis (PCA),Linear algebra,multidimensional scaling (MDS),Noise reduction,Optimization methods,Performance evaluation,pre-image,Principal component analysis,Space technology},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\MACU7GMT\\Kwok_Tsang_2004_The pre-image problem in kernel methods.pdf;C\:\\Users\\wenji\\Zotero\\storage\\IBDU3NLX\\1353287.html}
}

@inproceedings{mikaKernelPCADeNoising1998a,
  title = {Kernel {{PCA}} and {{De-Noising}} in {{Feature Spaces}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Mika, Sebastian and Schölkopf, Bernhard and Smola, Alex and Müller, Klaus-Robert and Scholz, Matthias and Rätsch, Gunnar},
  date = {1998},
  volume = {11},
  publisher = {MIT Press},
  url = {https://papers.nips.cc/paper_files/paper/1998/hash/226d1f15ecd35f784d2a20c3ecf56d7f-Abstract.html},
  urldate = {2024-07-19},
  abstract = {Kernel  PCA  as  a  nonlinear feature  extractor has  proven powerful  as  a  preprocessing step for classification algorithms.  But it can also be con(cid:173) sidered  as  a  natural  generalization of linear principal  component anal(cid:173) ysis.  This  gives  rise  to  the  question  how  to  use  nonlinear features  for  data compression, reconstruction, and de-noising, applications common  in  linear PCA.  This is  a nontrivial  task,  as the results provided by  ker(cid:173) nel PCA live in some high dimensional feature space and need not have  pre-images in  input space.  This work presents ideas for finding approxi(cid:173) mate pre-images, focusing on Gaussian kernels, and shows experimental  results  using  these pre-images in  data reconstruction and de-noising on  toy examples as well as on real world data.},
  file = {C:\Users\wenji\Zotero\storage\VQ2T23RD\Mika et al_1998_Kernel PCA and De-Noising in Feature Spaces.pdf}
}

@inproceedings{honeineSolvingPreimageProblem2009,
  title = {Solving the Pre-Image Problem in Kernel Machines: {{A}} Direct Method},
  shorttitle = {Solving the Pre-Image Problem in Kernel Machines},
  booktitle = {2009 {{IEEE International Workshop}} on {{Machine Learning}} for {{Signal Processing}}},
  author = {Honeine, Paul and Richard, Cedric},
  date = {2009-09},
  pages = {1--6},
  issn = {2378-928X},
  doi = {10.1109/MLSP.2009.5306204},
  url = {https://ieeexplore.ieee.org/abstract/document/5306204?casa_token=dk1AFL0IeX4AAAAA:gm2FGHpe3cI8fNKHeLw8NUN1uw6zKLQbsr6HpGaLVp53DQQCpPRRlqQ2sVL2jYpgYQkbhEnd54w},
  urldate = {2024-07-19},
  abstract = {In this paper, we consider the pre-image problem in kernel machines, such as denoising with kernel-PCA. For a given reproducing kernel Hilbert space (RKHS), by solving the pre-image problem one seeks a pattern whose image in the RKHS is approximately a given feature. Traditional techniques include an iterative technique (Mika et al.) and a multidimensional scaling (MDS) approach (Kwok et al.). In this paper, we propose a new technique to learn the pre-image. In the RKHS, we construct a basis having an isometry with the input space, with respect to a training data. Then representing any feature in this basis gives us information regarding its pre-image in the input space. We show that doing a pre-image can be done directly using the kernel values, without having to compute distances in any of the spaces as with the MDS approach. Simulation results illustrates the relevance of the proposed method, as we compare it to these techniques.},
  eventtitle = {2009 {{IEEE International Workshop}} on {{Machine Learning}} for {{Signal Processing}}},
  keywords = {Computational modeling,denoising,Hilbert space,Iterative methods,Kernel,kernel machines,kernel matrix regression,Multidimensional systems,Noise reduction,pre-image problem,Space technology,Statistical learning,Support vector machines,Training data},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\SJD96NUF\\Honeine_Richard_2009_Solving the pre-image problem in kernel machines.pdf;C\:\\Users\\wenji\\Zotero\\storage\\CPIHRFG6\\5306204.html}
}

@article{zhangOverviewRestrictedBoltzmann2018,
  title = {An Overview on {{Restricted Boltzmann Machines}}},
  author = {Zhang, Nan and Ding, Shifei and Zhang, Jian and Xue, Yu},
  date = {2018-01-31},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {275},
  pages = {1186--1199},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2017.09.065},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231217315849},
  urldate = {2024-07-20},
  abstract = {The Restricted Boltzmann Machine (RBM) has aroused wide interest in machine learning fields during the past decade. This review aims to report the recent developments in theoretical research and applications of the RBM. We first give an overview of the general RBM from the theoretical perspective, including stochastic approximation methods, stochastic gradient methods, and preventing overfitting methods. And then this review focuses on the RBM variants which further improve the learning ability of the RBM under general or specific applications. The RBM has recently been extended for representational learning, document modeling, multi-label learning, weakly supervised learning and many other tasks. The RBM and RBM variants provide powerful tools for representing dependency in the data, and they can be used as the basic building blocks to create deep networks. Apart from the Deep Belief Network (DBN) and the Deep Boltzmann Machine (DBM), the RBM can also be combined with the Convolutional Neural Network (CNN) to create deep networks. This review provides a comprehensive view of these advances in the RBM together with its future perspectives.},
  keywords = {Classification,Deep networks,Representational learning,Restricted Boltzmann Machine},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\IQ9M37P2\\Zhang et al_2018_An overview on Restricted Boltzmann Machines.pdf;C\:\\Users\\wenji\\Zotero\\storage\\UYQD9YK6\\S0925231217315849.html}
}

@inproceedings{fischerIntroductionRestrictedBoltzmann2012,
  title = {An {{Introduction}} to {{Restricted Boltzmann Machines}}},
  booktitle = {Progress in {{Pattern Recognition}}, {{Image Analysis}}, {{Computer Vision}}, and {{Applications}}},
  author = {Fischer, Asja and Igel, Christian},
  editor = {Alvarez, Luis and Mejail, Marta and Gomez, Luis and Jacobo, Julio},
  date = {2012},
  pages = {14--36},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-33275-3_2},
  abstract = {Restricted Boltzmann machines (RBMs) are probabilistic graphical models that can be interpreted as stochastic neural networks. The increase in computational power and the development of faster learning algorithms have made them applicable to relevant machine learning problems. They attracted much attention recently after being proposed as building blocks of multi-layer learning systems called deep belief networks. This tutorial introduces RBMs as undirected graphical models. The basic concepts of graphical models are introduced first, however, basic knowledge in statistics is presumed. Different learning algorithms for RBMs are discussed. As most of them are based on Markov chain Monte Carlo (MCMC) methods, an introduction to Markov chains and the required MCMC techniques is provided.},
  isbn = {978-3-642-33275-3},
  langid = {english},
  keywords = {Gibbs Sampling,Markov Chain,Markov Chain Monte Carlo,Markov Chain Monte Carlo Method,Restrict Boltzmann Machine},
  file = {C:\Users\wenji\Zotero\storage\4HF3UKNI\Fischer_Igel_2012_An Introduction to Restricted Boltzmann Machines.pdf}
}

@inproceedings{wangAnalysisGaussianbinaryRestricted2012,
  title = {An Analysis of {{Gaussian-binary}} Restricted {{Boltzmann}} Machines for Natural Images.},
  author = {Wang, Nan and Melchior, Jan and Wiskott, Laurenz},
  date = {2012},
  publisher = {Citeseer},
  eventtitle = {{{ESANN}}}
}

@inproceedings{ranzatoFactored3WayRestricted2010,
  title = {Factored 3-{{Way Restricted Boltzmann Machines For Modeling Natural Images}}},
  booktitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Ranzato, Marc’Aurelio and Krizhevsky, Alex and Hinton, Geoffrey},
  date = {2010-03-31},
  pages = {621--628},
  publisher = {{JMLR Workshop and Conference Proceedings}},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v9/ranzato10a.html},
  urldate = {2024-07-20},
  abstract = {Deep belief nets have been successful in modeling handwritten characters, but it has proved more difficult to apply them to real images. The problem lies in the restricted Boltzmann machine (RBM) which is used as a module for learning deep belief nets one layer at a time. The Gaussian-Binary RBMs that have been used to model real-valued data are not a good way to model the covariance structure of natural images. We propose a factored 3-way RBM that uses the states of its hidden units to represent abnormalities in the local covariance structure of an image. This provides a probabilistic framework for the widely used simple/complex cell architecture. Our model learns binary features that work very well for object recognition on the “tiny images” data set. Even better features are obtained by then using standard binary RBM’s to learn a deeper model.},
  eventtitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  langid = {english},
  file = {C:\Users\wenji\Zotero\storage\EVIWFKW2\Ranzato et al_2010_Factored 3-Way Restricted Boltzmann Machines For Modeling Natural Images.pdf}
}

@article{hintonFastLearningAlgorithm2006,
  title = {A Fast Learning Algorithm for Deep Belief Nets},
  author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee-Whye},
  date = {2006-07-01},
  journaltitle = {Neural Comput.},
  volume = {18},
  number = {7},
  pages = {1527--1554},
  issn = {0899-7667},
  doi = {10.1162/neco.2006.18.7.1527},
  url = {https://doi.org/10.1162/neco.2006.18.7.1527},
  urldate = {2024-07-20},
  abstract = {We show how to use "complementary priors" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.}
}

@inproceedings{carreira-perpinanContrastiveDivergenceLearning2005,
  title = {On {{Contrastive Divergence Learning}}},
  booktitle = {International {{Workshop}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Carreira-Perpiñán, Miguel Á and Hinton, Geoffrey},
  date = {2005-01-06},
  pages = {33--40},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/r5/carreira-perpinan05a.html},
  urldate = {2024-07-20},
  abstract = {On Contrastive Divergence LearningMiguel Á. Carreira-Perpiñán, Geoffrey Hinton},
  eventtitle = {International {{Workshop}} on {{Artificial Intelligence}} and {{Statistics}}},
  langid = {english},
  file = {C:\Users\wenji\Zotero\storage\E4X8MPYQ\Carreira-Perpiñán_Hinton_2005_On Contrastive Divergence Learning.pdf}
}

@article{toninUnsupervisedLearningDisentangled2021a,
  title = {Unsupervised Learning of Disentangled Representations in Deep Restricted Kernel Machines with Orthogonality Constraints},
  author = {Tonin, Francesco and Patrinos, Panagiotis and Suykens, Johan A. K.},
  date = {2021-10-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {142},
  pages = {661--679},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2021.07.023},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608021002860},
  urldate = {2024-07-20},
  abstract = {We introduce Constr-DRKM, a deep kernel method for the unsupervised learning of disentangled data representations. We propose augmenting the original deep restricted kernel machine formulation for kernel PCA by orthogonality constraints on the latent variables to promote disentanglement and to make it possible to carry out optimization without first defining a stabilized objective. After discussing a number of algorithms for end-to-end training, we quantitatively evaluate the proposed method’s effectiveness in disentangled feature learning. We demonstrate on four benchmark datasets that this approach performs similarly overall to β-VAE on several disentanglement metrics when few training points are available while being less sensitive to randomness and hyperparameter selection than β-VAE. We also present a deterministic initialization of Constr-DRKM’s training algorithm that significantly improves the reproducibility of the results. Finally, we empirically evaluate and discuss the role of the number of layers in the proposed methodology, examining the influence of each principal component in every layer and showing that components in lower layers act as local feature detectors capturing the broad trends of the data distribution, while components in deeper layers use the representation learned by previous layers and more accurately reproduce higher-level features.},
  keywords = {Kernel methods,Learning disentangled representations,Manifold learning,Unsupervised learning},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\L7HT9FFA\\Tonin et al_2021_Unsupervised learning of disentangled representations in deep restricted kernel.pdf;C\:\\Users\\wenji\\Zotero\\storage\\2I3TDCIH\\S0893608021002860.html}
}

@inproceedings{schreursGenerativeKernelPCA2018,
  title = {Generative {{Kernel PCA}}.},
  author = {Schreurs, Joachim and Suykens, Johan AK},
  date = {2018},
  volume = {2018},
  pages = {129--134},
  eventtitle = {{{ESANN}}},
  file = {C:\Users\wenji\Zotero\storage\V587G5FZ\Schreurs_Suykens_2018_Generative Kernel PCA.pdf}
}

@inproceedings{winantLatentSpaceExploration2020a,
  title = {Latent {{Space Exploration Using Generative Kernel PCA}}},
  booktitle = {Artificial {{Intelligence}} and {{Machine Learning}}},
  author = {Winant, David and Schreurs, Joachim and Suykens, Johan A. K.},
  editor = {Bogaerts, Bart and Bontempi, Gianluca and Geurts, Pierre and Harley, Nick and Lebichot, Bertrand and Lenaerts, Tom and Louppe, Gilles},
  date = {2020},
  pages = {70--82},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-65154-1_5},
  abstract = {Kernel PCA is a powerful feature extractor which recently has seen a reformulation in the context of Restricted Kernel Machines (RKMs). These RKMs allow for a representation of kernel PCA in terms of hidden and visible units similar to Restricted Boltzmann Machines. This connection has led to insights on how to use kernel PCA in a generative procedure, called generative kernel PCA. In this paper, the use of generative kernel PCA for exploring latent spaces of datasets is investigated. New points can be generated by gradually moving in the latent space, which allows for an interpretation of the components. Firstly, examples of this feature space exploration on three datasets are shown with one of them leading to an interpretable representation of ECG signals. Afterwards, the use of the tool in combination with novelty detection is shown, where the latent space around novel patterns in the data is explored. This helps in the interpretation of why certain points are considered as novel.},
  isbn = {978-3-030-65154-1},
  langid = {english},
  keywords = {Kernel PCA,Latent space exploration,Restricted Kernel Machines},
  file = {C:\Users\wenji\Zotero\storage\2RG7Q6H7\Winant et al_2020_Latent Space Exploration Using Generative Kernel PCA.pdf}
}

@article{zhaoMultiviewLearningOverview2017,
  title = {Multi-View Learning Overview: {{Recent}} Progress and New Challenges},
  shorttitle = {Multi-View Learning Overview},
  author = {Zhao, Jing and Xie, Xijiong and Xu, Xin and Sun, Shiliang},
  date = {2017-11-01},
  journaltitle = {Information Fusion},
  shortjournal = {Information Fusion},
  volume = {38},
  pages = {43--54},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2017.02.007},
  url = {https://www.sciencedirect.com/science/article/pii/S1566253516302032},
  urldate = {2024-07-22},
  abstract = {Multi-view learning is an emerging direction in machine learning which considers learning with multiple views to improve the generalization performance. Multi-view learning is also known as data fusion or data integration from multiple feature sets. Since the last survey of multi-view machine learning in early 2013, multi-view learning has made great progress and developments in recent years, and is facing new challenges. This overview first reviews theoretical underpinnings to understand the properties and behaviors of multi-view learning. Then multi-view learning methods are described in terms of three classes to offer a neat categorization and organization. For each category, representative algorithms and newly proposed algorithms are presented. The main feature of this survey is that we provide comprehensive introduction for the recent developments of multi-view learning methods on the basis of coherence with early methods. We also attempt to identify promising venues and point out some specific challenges which can hopefully promote further research in this rapidly developing field.},
  keywords = {Co-regularization,Co-training,Margin consistency,Multi-view learning,Statistical learning theory},
  file = {C:\Users\wenji\Zotero\storage\5WMTVHZC\S1566253516302032.html}
}

@article{zhaoMultiviewLearningOverview2017a,
  title = {Multi-View Learning Overview: {{Recent}} Progress and New Challenges},
  shorttitle = {Multi-View Learning Overview},
  author = {Zhao, Jing and Xie, Xijiong and Xu, Xin and Sun, Shiliang},
  date = {2017-11-01},
  journaltitle = {Information Fusion},
  shortjournal = {Information Fusion},
  volume = {38},
  pages = {43--54},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2017.02.007},
  url = {https://www.sciencedirect.com/science/article/pii/S1566253516302032},
  urldate = {2024-07-22},
  abstract = {Multi-view learning is an emerging direction in machine learning which considers learning with multiple views to improve the generalization performance. Multi-view learning is also known as data fusion or data integration from multiple feature sets. Since the last survey of multi-view machine learning in early 2013, multi-view learning has made great progress and developments in recent years, and is facing new challenges. This overview first reviews theoretical underpinnings to understand the properties and behaviors of multi-view learning. Then multi-view learning methods are described in terms of three classes to offer a neat categorization and organization. For each category, representative algorithms and newly proposed algorithms are presented. The main feature of this survey is that we provide comprehensive introduction for the recent developments of multi-view learning methods on the basis of coherence with early methods. We also attempt to identify promising venues and point out some specific challenges which can hopefully promote further research in this rapidly developing field.},
  keywords = {Co-regularization,Co-training,Margin consistency,Multi-view learning,Statistical learning theory},
  file = {C:\Users\wenji\Zotero\storage\9M3RFHJ7\S1566253516302032.html}
}

@article{zhaoMultiviewLearningOverview2017b,
  title = {Multi-View Learning Overview: {{Recent}} Progress and New Challenges},
  shorttitle = {Multi-View Learning Overview},
  author = {Zhao, Jing and Xie, Xijiong and Xu, Xin and Sun, Shiliang},
  date = {2017-11-01},
  journaltitle = {Information Fusion},
  shortjournal = {Information Fusion},
  volume = {38},
  pages = {43--54},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2017.02.007},
  url = {https://www.sciencedirect.com/science/article/pii/S1566253516302032},
  urldate = {2024-07-22},
  abstract = {Multi-view learning is an emerging direction in machine learning which considers learning with multiple views to improve the generalization performance. Multi-view learning is also known as data fusion or data integration from multiple feature sets. Since the last survey of multi-view machine learning in early 2013, multi-view learning has made great progress and developments in recent years, and is facing new challenges. This overview first reviews theoretical underpinnings to understand the properties and behaviors of multi-view learning. Then multi-view learning methods are described in terms of three classes to offer a neat categorization and organization. For each category, representative algorithms and newly proposed algorithms are presented. The main feature of this survey is that we provide comprehensive introduction for the recent developments of multi-view learning methods on the basis of coherence with early methods. We also attempt to identify promising venues and point out some specific challenges which can hopefully promote further research in this rapidly developing field.},
  keywords = {Co-regularization,Co-training,Margin consistency,Multi-view learning,Statistical learning theory},
  file = {C:\Users\wenji\Zotero\storage\DAZ6XKAF\S1566253516302032.html}
}

@article{yanDeepMultiviewLearning2021,
  title = {Deep Multi-View Learning Methods: {{A}} Review},
  shorttitle = {Deep Multi-View Learning Methods},
  author = {Yan, Xiaoqiang and Hu, Shizhe and Mao, Yiqiao and Ye, Yangdong and Yu, Hui},
  date = {2021-08-11},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {448},
  pages = {106--129},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2021.03.090},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231221004768},
  urldate = {2024-07-22},
  abstract = {Multi-view learning (MVL) has attracted increasing attention and achieved great practical success by exploiting complementary information of multiple features or modalities. Recently, due to the remarkable performance of deep models, deep MVL has been adopted in many domains, such as machine learning, artificial intelligence and computer vision. This paper presents a comprehensive review on deep MVL from the following two perspectives: MVL methods in deep learning scope and deep MVL extensions of traditional methods. Specifically, we first review the representative MVL methods in the scope of deep learning, such as multi-view auto-encoder, conventional neural networks and deep brief networks. Then, we investigate the advancements of the MVL mechanism when traditional learning methods meet deep learning models, such as deep multi-view canonical correlation analysis, matrix factorization and information bottleneck. Moreover, we also summarize the main applications, widely-used datasets and performance comparison in the domain of deep MVL. Finally, we attempt to identify some open challenges to inform future research directions.},
  keywords = {Deep multi-view learning,deep neural networks,representation learning,statistical learning survey},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\9VIW8LD2\\Yan et al_2021_Deep multi-view learning methods.pdf;C\:\\Users\\wenji\\Zotero\\storage\\QIPLLFIS\\S0925231221004768.html}
}

@article{tagareNotesOptimizationStiefel2011,
  title = {Notes on Optimization on Stiefel Manifolds},
  author = {Tagare, Hemant D},
  date = {2011},
  journaltitle = {Yale University, New Haven},
  shortjournal = {Yale University, New Haven}
}

@inproceedings{liuDiverseImageGeneration2020,
  title = {Diverse {{Image Generation}} via {{Self-Conditioned GANs}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Liu, Steven and Wang, Tongzhou and Bau, David and Zhu, Jun-Yan and Torralba, Antonio},
  date = {2020-06},
  pages = {14274--14283},
  issn = {2575-7075},
  doi = {10.1109/CVPR42600.2020.01429},
  url = {https://ieeexplore.ieee.org/document/9157790},
  urldate = {2024-07-23},
  abstract = {We introduce a simple but effective unsupervised method for generating diverse images. We train a class-conditional GAN model without using manually annotated class labels. Instead, our model is conditional on labels automatically derived from clustering in the discriminator’s feature space. Our clustering step automatically discovers diverse modes, and explicitly requires the generator to cover them. Experiments on standard mode collapse benchmarks show that our method outperforms several competing methods when addressing mode collapse. Our method also performs well on large-scale datasets such as ImageNet and Places365, improving both diversity and standard metrics (e.g., Fréchet Inception Distance), compared to previous methods.},
  eventtitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  keywords = {Clustering algorithms,Computational modeling,Gallium nitride,Generators,Image generation,Partitioning algorithms,Training},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\7RH6TABD\\Liu et al_2020_Diverse Image Generation via Self-Conditioned GANs.pdf;C\:\\Users\\wenji\\Zotero\\storage\\F727X8KS\\9157790.html}
}

@inproceedings{mccurdyRidgeRegressionProvable2018,
  title = {Ridge {{Regression}} and {{Provable Deterministic Ridge Leverage Score Sampling}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {McCurdy, Shannon},
  date = {2018},
  volume = {31},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2018/hash/e1d5be1c7f2f456670de3d53c7b54f4a-Abstract.html},
  urldate = {2024-07-29},
  file = {C:\Users\wenji\Zotero\storage\PEPI4SYE\McCurdy_2018_Ridge Regression and Provable Deterministic Ridge Leverage Score Sampling.pdf}
}

@inproceedings{heDeepResidualLearning2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2016},
  pages = {770--778},
  url = {https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html},
  urldate = {2024-07-31},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {C:\Users\wenji\Zotero\storage\F8NZN3N9\He et al_2016_Deep Residual Learning for Image Recognition.pdf}
}

@inproceedings{dengImageNetLargescaleHierarchical2009,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database},
  shorttitle = {{{ImageNet}}},
  booktitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  date = {2009-06},
  pages = {248--255},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2009.5206848},
  url = {https://ieeexplore.ieee.org/document/5206848},
  urldate = {2024-07-31},
  abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500–1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  eventtitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  keywords = {Explosions,Image databases,Image retrieval,Information retrieval,Internet,Large-scale systems,Multimedia databases,Ontologies,Robustness,Spine},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\J4SRTIAA\\Deng et al_2009_ImageNet.pdf;C\:\\Users\\wenji\\Zotero\\storage\\CQALMRYG\\5206848.html}
}

@inproceedings{krizhevskyImageNetClassificationDeep2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  date = {2012},
  volume = {25},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
  urldate = {2024-07-31},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7\textbackslash\% and 18.9\textbackslash\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.}
}

@inproceedings{krizhevskyImageNetClassificationDeep2012a,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  date = {2012},
  volume = {25},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
  urldate = {2024-07-31},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7\textbackslash\% and 18.9\textbackslash\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.}
}

@inproceedings{krizhevskyImageNetClassificationDeep2012b,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  date = {2012},
  volume = {25},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
  urldate = {2024-07-31},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7\textbackslash\% and 18.9\textbackslash\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.}
}

@online{mcinnesUMAPUniformManifold2020,
  title = {{{UMAP}}: {{Uniform Manifold Approximation}} and {{Projection}} for {{Dimension Reduction}}},
  shorttitle = {{{UMAP}}},
  author = {McInnes, Leland and Healy, John and Melville, James},
  date = {2020-09-17},
  eprint = {1802.03426},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1802.03426},
  url = {http://arxiv.org/abs/1802.03426},
  urldate = {2024-07-31},
  abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Geometry,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\24E4QALG\\McInnes et al_2020_UMAP.pdf;C\:\\Users\\wenji\\Zotero\\storage\\2FWMK82P\\1802.html}
}

@inproceedings{kpotufeGaussianSketchingYields2020,
  title = {Gaussian {{Sketching}} Yields a {{J-L Lemma}} in {{RKHS}}},
  booktitle = {Proceedings of the {{Twenty Third International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Kpotufe, Samory and Sriperumbudur, Bharath},
  date = {2020-06-03},
  pages = {3928--3937},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v108/kpotufe20a.html},
  urldate = {2024-07-31},
  abstract = {The main contribution of the paper is to show that Gaussian sketching of a kernel-Gram matrix \textbackslash bmK\textbackslash bmK\textbackslash bm K yields an operator whose counterpart in an RKHS HH\textbackslash cal H, is a \textbackslash emph\{random projection\} operator—in the spirit of Johnson-Lindenstrauss (J-L) lemma. To be precise, given a random matrix ZZZ with i.i.d. Gaussian entries, we show that a sketch Z\textbackslash bmKZ\textbackslash bmKZ\textbackslash bm\{K\} corresponds to a particular random operator in (infinite-dimensional) Hilbert space HH\textbackslash cal H that maps functions f∈Hf∈Hf \textbackslash in \textbackslash cal H to a low-dimensional space \textbackslash bbRd\textbackslash bbRd\textbackslash bb R\textasciicircum d, while preserving a weighted RKHS inner-product of the form ⟨f,g⟩Σ≐⟨f,Σ3g⟩H⟨f,g⟩Σ≐⟨f,Σ3g⟩H⟨f, g \textbackslash rangle\_\{\textbackslash Sigma\} \textbackslash doteq ⟨f, \textbackslash Sigma\textasciicircum 3 g \textbackslash rangle\_\{\textbackslash cal H\}, where ΣΣ\textbackslash Sigma is the \textbackslash emph\{covariance\} operator induced by the data distribution. In particular, under similar assumptions as in kernel PCA (KPCA), or kernel kkk-means (K-kkk-means), well-separated subsets of feature-space \{K(⋅,x):x∈X\}\{K(⋅,x):x∈X\}\textbackslash\{K(\textbackslash cdot, x): x \textbackslash in \textbackslash cal X\textbackslash\} remain well-separated after such operation, which suggests similar benefits as in KPCA and/or K-kkk-means, albeit at the much cheaper cost of a random projection. In particular, our convergence rates suggest that, given a large dataset \{Xi\}Ni=1\{Xi\}i=1N\textbackslash\{X\_i\textbackslash\}\_\{i=1\}\textasciicircum N of size NNN, we can build the Gram matrix \textbackslash bmK\textbackslash bmK\textbackslash bm K on a much smaller subsample of size n≪Nn≪Nn\textbackslash ll N, so that the sketch Z\textbackslash bmKZ\textbackslash bmKZ\textbackslash bm K is very cheap to obtain and subsequently apply as a projection operator on the original data \{Xi\}Ni=1\{Xi\}i=1N\textbackslash\{X\_i\textbackslash\}\_\{i=1\}\textasciicircum N.  We verify these insights empirically on synthetic data, and on real-world clustering applications.},
  eventtitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  langid = {english},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\56YKQ847\\Kpotufe_Sriperumbudur_2020_Gaussian Sketching yields a J-L Lemma in RKHS.pdf;C\:\\Users\\wenji\\Zotero\\storage\\7RSHFX5R\\Kpotufe 和 Sriperumbudur - 2020 - Gaussian Sketching yields a J-L Lemma in RKHS.pdf}
}

@article{woodruffSketchingToolNumerical2014,
  title = {Sketching as a {{Tool}} for {{Numerical Linear Algebra}}},
  author = {Woodruff, David P.},
  date = {2014-10-28},
  journaltitle = {Foundations and Trends® in Theoretical Computer Science},
  shortjournal = {TCS},
  volume = {10},
  number = {1–2},
  pages = {1--157},
  publisher = {Now Publishers, Inc.},
  issn = {1551-305X, 1551-3068},
  doi = {10.1561/0400000060},
  url = {https://www-nowpublishers-com.kuleuven.e-bronnen.be/article/Details/TCS-060},
  urldate = {2024-07-31},
  abstract = {Sketching as a Tool for Numerical Linear Algebra},
  langid = {english},
  file = {C:\Users\wenji\Zotero\storage\733M3TM3\Woodruff_2014_Sketching as a Tool for Numerical Linear Algebra.pdf}
}

@inproceedings{gilardiConditionalGaussianMixture2002,
  title = {Conditional {{Gaussian}} Mixture Models for Environmental Risk Mapping},
  booktitle = {Proceedings of the 12th {{IEEE Workshop}} on {{Neural Networks}} for {{Signal Processing}}},
  author = {Gilardi, N. and Bengio, S. and Kanevski, M.},
  date = {2002-09},
  pages = {777--786},
  doi = {10.1109/NNSP.2002.1030100},
  url = {https://ieeexplore-ieee-org.kuleuven.e-bronnen.be/abstract/document/1030100},
  urldate = {2024-08-01},
  abstract = {This paper proposes the use of Gaussian mixture models to estimate conditional probability density functions in an environmental risk mapping context. A conditional Gaussian mixture model has been compared to, the geostatistical method of sequential Gaussian simulations and shows good performance in reconstructing the local PDF. The data sets used for this comparison are parts of the digital elevation model of Switzerland.},
  eventtitle = {Proceedings of the 12th {{IEEE Workshop}} on {{Neural Networks}} for {{Signal Processing}}},
  keywords = {Artificial intelligence,Artificial neural networks,Covariance matrix,Decision making,Digital elevation models,Neural networks,Prediction methods,Probability density function,Smoothing methods,Stochastic processes},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\PS3F5M8W\\Gilardi et al_2002_Conditional Gaussian mixture models for environmental risk mapping.pdf;C\:\\Users\\wenji\\Zotero\\storage\\UQDBMWM2\\1030100.html}
}

@article{chehadeConditionalGaussianMixture2022,
  title = {Conditional {{Gaussian}} Mixture Model for Warranty Claims Forecasting},
  author = {Chehade, Abdallah and Savargaonkar, Mayuresh and Krivtsov, Vasiliy},
  date = {2022-02-01},
  journaltitle = {Reliability Engineering \& System Safety},
  shortjournal = {Reliability Engineering \& System Safety},
  volume = {218},
  pages = {108180},
  issn = {0951-8320},
  doi = {10.1016/j.ress.2021.108180},
  url = {https://www.sciencedirect.com/science/article/pii/S0951832021006645},
  urldate = {2024-08-01},
  abstract = {Forecasting warranty claims for complex products is a reliability challenge for most manufacturers. Several factors increase the complexity of warranty claims forecasting, including, the limited number of claims reported at the early stage of launch, reporting delays, dynamic change in the fleet size, and design/manufacturing adjustments for the production line. The aggregated effect of those complexities is often referred to as the “warranty data maturation” effect. Unfortunately, most of the existing models for warranty claims forecasting fail to explicitly consider warranty data maturation. This work address warranty data maturation by proposing the Conditional Gaussian Mixture Model (CGMM). CGMM uses historical warranty data from similar products to develop a robust prior joint Gaussian mixture distribution of warranty trends at both, the current and future maturation levels. CGMM then utilizes Bayesian theories to estimate the conditional posterior distribution of the warranty claims at the future maturation level conditional on the warranty data available at the current maturation level. The CGMM identifies non-parametric temporal warranty trends and automatically clusters products into latent groups to establish (learn) an effective prior joint distribution. The CGMM is validated on an extensive automotive warranty claims dataset comprising of four model years and {$>$}15,000 different components from {$>$}10 million vehicles.},
  keywords = {Bayesian statistics,Gaussian mixture model,Machine learning,Reliability,Warranty},
  file = {C:\Users\wenji\Zotero\storage\XFE77XLE\S0951832021006645.html}
}

@article{chehadeConditionalGaussianMixture2022a,
  title = {Conditional {{Gaussian}} Mixture Model for Warranty Claims Forecasting},
  author = {Chehade, Abdallah and Savargaonkar, Mayuresh and Krivtsov, Vasiliy},
  date = {2022-02-01},
  journaltitle = {Reliability Engineering \& System Safety},
  shortjournal = {Reliability Engineering \& System Safety},
  volume = {218},
  pages = {108180},
  issn = {0951-8320},
  doi = {10.1016/j.ress.2021.108180},
  url = {https://www.sciencedirect.com/science/article/pii/S0951832021006645},
  urldate = {2024-08-01},
  abstract = {Forecasting warranty claims for complex products is a reliability challenge for most manufacturers. Several factors increase the complexity of warranty claims forecasting, including, the limited number of claims reported at the early stage of launch, reporting delays, dynamic change in the fleet size, and design/manufacturing adjustments for the production line. The aggregated effect of those complexities is often referred to as the “warranty data maturation” effect. Unfortunately, most of the existing models for warranty claims forecasting fail to explicitly consider warranty data maturation. This work address warranty data maturation by proposing the Conditional Gaussian Mixture Model (CGMM). CGMM uses historical warranty data from similar products to develop a robust prior joint Gaussian mixture distribution of warranty trends at both, the current and future maturation levels. CGMM then utilizes Bayesian theories to estimate the conditional posterior distribution of the warranty claims at the future maturation level conditional on the warranty data available at the current maturation level. The CGMM identifies non-parametric temporal warranty trends and automatically clusters products into latent groups to establish (learn) an effective prior joint distribution. The CGMM is validated on an extensive automotive warranty claims dataset comprising of four model years and {$>$}15,000 different components from {$>$}10 million vehicles.},
  keywords = {Bayesian statistics,Gaussian mixture model,Machine learning,Reliability,Warranty},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\52RIRVEH\\Chehade et al_2022_Conditional Gaussian mixture model for warranty claims forecasting.pdf;C\:\\Users\\wenji\\Zotero\\storage\\AAEX5BRR\\S0951832021006645.html}
}

@online{xiaoFashionMNISTNovelImage2017,
  title = {Fashion-{{MNIST}}: A {{Novel Image Dataset}} for {{Benchmarking Machine Learning Algorithms}}},
  shorttitle = {Fashion-{{MNIST}}},
  author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  date = {2017-09-15},
  eprint = {1708.07747},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1708.07747},
  url = {http://arxiv.org/abs/1708.07747},
  urldate = {2024-08-06},
  abstract = {We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https://github.com/zalandoresearch/fashion-mnist},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\7NG5WYU3\\Xiao et al_2017_Fashion-MNIST.pdf;C\:\\Users\\wenji\\Zotero\\storage\\J9YREG8K\\1708.html}
}

@inproceedings{heuselGANsTrainedTwo2017,
  title = {{{GANs Trained}} by a {{Two Time-Scale Update Rule Converge}} to a {{Local Nash Equilibrium}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  date = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper_files/paper/2017/hash/8a1d694707eb0fefe65871369074926d-Abstract.html},
  urldate = {2024-08-07},
  abstract = {Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the `Fréchet Inception Distance'' (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.},
  file = {C:\Users\wenji\Zotero\storage\GBAS8TQA\Heusel et al_2017_GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash.pdf}
}

@online{simonyanVeryDeepConvolutional2015,
  title = {Very {{Deep Convolutional Networks}} for {{Large-Scale Image Recognition}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  date = {2015-04-10},
  eprint = {1409.1556},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1409.1556},
  url = {http://arxiv.org/abs/1409.1556},
  urldate = {2024-08-09},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\5FUNJVWH\\Simonyan_Zisserman_2015_Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf;C\:\\Users\\wenji\\Zotero\\storage\\RL3USRPE\\1409.html}
}

@inproceedings{szegedyRethinkingInceptionArchitecture2016,
  title = {Rethinking the {{Inception Architecture}} for {{Computer Vision}}},
  author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  date = {2016},
  pages = {2818--2826},
  url = {https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.html},
  urldate = {2024-08-09},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {C:\Users\wenji\Zotero\storage\PXK82ULX\Szegedy et al_2016_Rethinking the Inception Architecture for Computer Vision.pdf}
}

@inproceedings{bhattacharjeeDatacopyingGenerativeModels2023,
  title = {Data-Copying in Generative Models: A Formal Framework},
  shorttitle = {Data-Copying in Generative Models},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Machine Learning}}},
  author = {Bhattacharjee, Robi and Dasgupta, Sanjoy and Chaudhuri, Kamalika},
  date = {2023-07-23},
  series = {{{ICML}}'23},
  volume = {202},
  pages = {2364--2396},
  publisher = {JMLR.org},
  location = {Honolulu, Hawaii, USA},
  abstract = {There has been some recent interest in detecting and addressing memorization of training data by deep neural networks. A formal framework for memorization in generative models, called "data-copying" was proposed by Meehan et. al (2020). We build upon their work to show that their framework may fail to detect certain kinds of blatant memorization. Motivated by this and the theory of non-parametric methods, we provide an alternative definition of data-copying that applies more locally. We provide a method to detect data-copying, and provably show that it works with high probability when enough data is available. We also provide lower bounds that characterize the sample requirement for reliable detection.}
}

@inproceedings{kossaleModeCollapseGenerative2022,
  title = {Mode {{Collapse}} in {{Generative Adversarial Networks}}: {{An Overview}}},
  shorttitle = {Mode {{Collapse}} in {{Generative Adversarial Networks}}},
  booktitle = {2022 8th {{International Conference}} on {{Optimization}} and {{Applications}} ({{ICOA}})},
  author = {Kossale, Youssef and Airaj, Mohammed and Darouichi, Aziz},
  date = {2022-10},
  pages = {1--6},
  issn = {2768-6388},
  doi = {10.1109/ICOA55659.2022.9934291},
  url = {https://ieeexplore.ieee.org/abstract/document/9934291},
  urldate = {2024-08-12},
  abstract = {With the rise of a new framework known as Generative Adversarial Networks (GANs), generative models have gained considerable amount of attention in the area of unsupervised learning. GANs have been thoroughly studied since their emergence in 2014, leading to an enormous amount of new models and applications built on this said framework. Although despite their success, GANs suffer from some notorious problems during training, hindering further advances in the field. This paper seeks to highlight one of the most encountered problems in GAN training, namely the “Helvetica scenario” as stated by its authors or “mode collapse” as widely known. We will try to provide an overview of this said challenge, what is it, why it occurs, and some suggested workarounds to reduce its impact on training.},
  eventtitle = {2022 8th {{International Conference}} on {{Optimization}} and {{Applications}} ({{ICOA}})},
  keywords = {Computational modeling,GANs,Generative adversarial networks,Generative models,Mode collapse,Optimization,Training,Tuning,Unsupervised learning},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\48SLU4ZC\\Kossale et al_2022_Mode Collapse in Generative Adversarial Networks.pdf;C\:\\Users\\wenji\\Zotero\\storage\\SLU4YSEA\\9934291.html}
}

@incollection{chawlaDataMiningImbalanced2005,
  title = {Data {{Mining}} for {{Imbalanced Datasets}}: {{An Overview}}},
  shorttitle = {Data {{Mining}} for {{Imbalanced Datasets}}},
  booktitle = {Data {{Mining}} and {{Knowledge Discovery Handbook}}},
  author = {Chawla, Nitesh V.},
  editor = {Maimon, Oded and Rokach, Lior},
  date = {2005},
  pages = {853--867},
  publisher = {Springer US},
  location = {Boston, MA},
  doi = {10.1007/0-387-25465-X_40},
  url = {https://doi.org/10.1007/0-387-25465-X_40},
  urldate = {2024-08-12},
  abstract = {A dataset is imbalanced if the classification categories are not approximately equally represented. Recent years brought increased interest in applying machine learning techniques to difficult “real-world” problems, many of which are characterized by imbalanced data. Additionally the distribution of the testing data may differ from that of the training data, and the true misclassification costs may be unknown at learning time. Predictive accuracy, a popular choice for evaluating performance of a classifier, might not be appropriate when the data is imbalanced and/or the costs of different errors vary markedly. In this Chapter, we discuss some of the sampling techniques used for balancing the datasets, and the performance measures more appropriate for mining imbalanced datasets.},
  isbn = {978-0-387-25465-4},
  langid = {english},
  keywords = {classification,cost-sensitive measures,imbalanced datasets,precision and recall,ROC,sampling},
  file = {C:\Users\wenji\Zotero\storage\NI4SQIUP\Chawla_2005_Data Mining for Imbalanced Datasets.pdf}
}

@incollection{wuFairDataGeneration2022,
  title = {Fair {{Data Generation}} and {{Machine Learning Through Generative Adversarial Networks}}},
  booktitle = {Generative {{Adversarial Learning}}: {{Architectures}} and {{Applications}}},
  author = {Wu, Xintao and Xu, Depeng and Yuan, Shuhan and Zhang, Lu},
  editor = {Razavi-Far, Roozbeh and Ruiz-Garcia, Ariel and Palade, Vasile and Schmidhuber, Juergen},
  date = {2022},
  pages = {31--55},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-91390-8_3},
  url = {https://doi.org/10.1007/978-3-030-91390-8_3},
  urldate = {2024-08-13},
  abstract = {In this chapter, we present a fair generative adversarial networks framework (named FairGAN) for fair data generation and fair predictive learning. The FairGAN framework can accommodate various fairness notions by changing the network architecture and objective functions of generators and discriminators. Under the FairGAN framework, we present three previously published model designs, Simplified-FairGAN [1], Causal-FairGAN [2], and FairGAN\$\$\textasciicircum +\$\$+[3], discuss their designs, fairness, utility, convergence, and implementation. We then present a short literature review of closely related works of using GANs for structural data generation and achieving differential privacy in GAN. At the end, we introduce several future research directions including architecture design, achieving long-term fairness in dynamic decision making, achieving fairness in regression and recommendation, and open source development. This paper expects to advance understanding and applicability of GAN from image data generation to fair data generation and fair predictive learning.},
  isbn = {978-3-030-91390-8},
  langid = {english},
  file = {C:\Users\wenji\Zotero\storage\5IH4QN27\Wu et al_2022_Fair Data Generation and Machine Learning Through Generative Adversarial.pdf}
}

@inproceedings{vanbreugelDECAFGeneratingFair2021,
  title = {{{DECAF}}: {{Generating Fair Synthetic Data Using Causally-Aware Generative Networks}}},
  shorttitle = {{{DECAF}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {family=Breugel, given=Boris, prefix=van, useprefix=true and Kyono, Trent and Berrevoets, Jeroen and family=Schaar, given=Mihaela, prefix=van der, useprefix=true},
  date = {2021},
  volume = {34},
  pages = {22221--22233},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2021/hash/ba9fab001f67381e56e410575874d967-Abstract.html},
  urldate = {2024-08-13},
  abstract = {Machine learning models have been criticized for reflecting unfair biases in the training data.  Instead of solving for this by introducing fair learning algorithms directly, we focus on generating fair synthetic data, such that any downstream learner is fair. Generating fair synthetic data from unfair data - while remaining truthful to the underlying data-generating process (DGP) - is non-trivial. In this paper, we introduce DECAF: a GAN-based fair synthetic data generator for tabular data.  With DECAF we embed the DGP explicitly as a structural causal model in the input layers of the generator, allowing each variable to be reconstructed conditioned on its causal parents.  This procedure enables inference time debiasing, where biased edges can be strategically removed for satisfying user-defined fairness requirements. The DECAF framework is versatile and compatible with several popular definitions of fairness. In our experiments, we show that DECAF successfully removes undesired bias and - in contrast to existing methods - is capable of generating high-quality synthetic data. Furthermore, we provide theoretical guarantees on the generator's convergence and the fairness of downstream models.},
  file = {C:\Users\wenji\Zotero\storage\TVHURPYQ\van Breugel et al_2021_DECAF.pdf}
}

@article{huangEnhancedBalancingGAN2023a,
  title = {Enhanced Balancing {{GAN}}: Minority-Class Image Generation},
  shorttitle = {Enhanced Balancing {{GAN}}},
  author = {Huang, Gaofeng and Jafari, Amir Hossein},
  date = {2023-03-01},
  journaltitle = {Neural Computing and Applications},
  shortjournal = {Neural Comput \& Applic},
  volume = {35},
  number = {7},
  pages = {5145--5154},
  issn = {1433-3058},
  doi = {10.1007/s00521-021-06163-8},
  url = {https://doi.org/10.1007/s00521-021-06163-8},
  urldate = {2024-08-14},
  abstract = {Generative adversarial networks (GANs) are one of the most powerful generative models, but always require a large and balanced dataset to train. Traditional GANs are not applicable to generate minority-class images in a highly imbalanced dataset. Balancing GAN (BAGAN) is proposed to mitigate this problem, but it is unstable when images in different classes look similar, e.g., flowers and cells. In this work, we propose a supervised autoencoder with an intermediate embedding model to disperse the labeled latent vectors. With the enhanced autoencoder initialization, we also build an architecture of BAGAN with gradient penalty (BAGAN-GP). Our proposed model overcomes the unstable issue in original BAGAN and converges faster to high-quality generations. Our model achieves high performance on the imbalanced scale-down version of MNIST Fashion, CIFAR-10, and one small-scale medical image dataset. https://github.com/GH920/improved-bagan-gp.},
  langid = {english},
  keywords = {Artificial Intelligence,Data augmentation,GAN,Image generation,Imbalanced data,Medical image},
  file = {C:\Users\wenji\Zotero\storage\XEAE76RI\Huang_Jafari_2023_Enhanced balancing GAN.pdf}
}

@inproceedings{rangwaniClassBalancingGAN2021,
  title = {Class Balancing {{GAN}} with a Classifier in the Loop},
  booktitle = {Proceedings of the {{Thirty-Seventh Conference}} on {{Uncertainty}} in {{Artificial Intelligence}}},
  author = {Rangwani, Harsh and Mopuri, Konda Reddy and Babu, R. Venkatesh},
  date = {2021-12-01},
  pages = {1618--1627},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v161/rangwani21a.html},
  urldate = {2024-08-14},
  abstract = {Generative Adversarial Networks (GANs) have swiftly evolved to imitate increasingly complex image distributions. However, majority of the developments focus on performance of GANs on balanced datasets. We find that the existing GANs and their training regimes which work well on balanced datasets fail to be effective in case of imbalanced (i.e. long-tailed) datasets. In this work we introduce a novel theoretically motivated Class Balancing regularizer for training GANs. Our regularizer makes use of the knowledge from a pre-trained classifier to ensure balanced learning of all the classes in the dataset. This is achieved via modelling the effective class frequency based on the exponential forgetting observed in neural networks and encouraging the GAN to focus on underrepresented classes. We demonstrate the utility of our regularizer in learning representations for long-tailed distributions via achieving better performance than existing approaches over multiple datasets. Specifically, when applied to an unconditional GAN, it improves the FID from 13.0313.0313.03 to 9.019.019.01 on the long-tailed iNaturalist-201920192019 dataset.},
  eventtitle = {Uncertainty in {{Artificial Intelligence}}},
  langid = {english},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\7Q6QXUCC\\Rangwani et al_2021_Class balancing GAN with a classifier in the loop.pdf;C\:\\Users\\wenji\\Zotero\\storage\\LSDDJKE6\\Rangwani 等 - 2021 - Class balancing GAN with a classifier in the loop.pdf}
}

@inproceedings{yangIDAGANNovelImbalanced2021,
  title = {{{IDA-GAN}}: {{A Novel Imbalanced Data Augmentation GAN}}},
  shorttitle = {{{IDA-GAN}}},
  booktitle = {2020 25th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  author = {Yang, Hao and Zhou, Yun},
  date = {2021-01},
  pages = {8299--8305},
  issn = {1051-4651},
  doi = {10.1109/ICPR48806.2021.9411996},
  url = {https://ieeexplore.ieee.org/abstract/document/9411996?casa_token=HrQ9LnvgqK4AAAAA:yQWYy8f8lgYKHHSALHluupp9Pr0wemPHaK6cvNW11N9tBS7VQzAJqPlTRQeJcZP5_c23buozSXg},
  urldate = {2024-08-14},
  abstract = {Class imbalance is a widely existed and challenging problem in real-world applications such as disease diagnosis, fraud detection, network intrusion detection and so on. Due to the scarce of data, it could significantly deteriorate the accuracy of classification. To address this challenge, we propose a novel Imbalanced Data Augmentation Generative Adversarial Networks (GAN) named IDA-GAN as an augmentation tool to deal with the imbalanced dataset. This is a great challenge because it is hard to train a GAN model under this situation. We address this issue by coupling variational autoencoder along with GAN training. In this paper, specifically, we introduce the variational autoencoder to learn the majority and minority class distributions in the latent space, and use the generative model to utilize each class distribution for the subsequent GAN training. The generative model learns useful features to generate target minority-class samples. Compared with the state-of-the-art GAN model, the experimental results demonstrate that our proposed IDA-GAN could generate more diverse minority samples with better qualities, and it could benefits the imbalanced classification task in terms of several widely-used evaluation metrics on five benchmark datasets: MNIST, Fashion-MNIST, SVHN, CIFAR-10 and GTSRB.},
  eventtitle = {2020 25th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  keywords = {Benchmark testing,Data augmentation,GAN,Generative adversarial networks,Imbalanced learning,Measurement,Network intrusion detection,Performance analysis,Tools,Training,Variational autoencoder},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\3TZWLNA3\\Yang_Zhou_2021_IDA-GAN.pdf;C\:\\Users\\wenji\\Zotero\\storage\\EV72GKBR\\9411996.html}
}

@inproceedings{mikolajczykBiasingEffectGANBased2022,
  title = {The (de)Biasing {{Effect}} of~{{GAN-Based Augmentation Methods}} on~{{Skin Lesion Images}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} – {{MICCAI}} 2022},
  author = {Mikołajczyk, Agnieszka and Majchrowska, Sylwia and Carrasco Limeros, Sandra},
  editor = {Wang, Linwei and Dou, Qi and Fletcher, P. Thomas and Speidel, Stefanie and Li, Shuo},
  date = {2022},
  pages = {437--447},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-16452-1_42},
  abstract = {New medical datasets are now more open to the public, allowing for better and more extensive research. Although prepared with the utmost care, new datasets might still be a source of spurious correlations that affect the learning process. Moreover, data collections are usually not large enough and are often unbalanced. One approach to alleviate the data imbalance is using data augmentation with Generative Adversarial Networks (GANs) to extend the dataset with high-quality images. GANs are usually trained on the same biased datasets as the target data, resulting in more biased instances. This work explored unconditional and conditional GANs to compare their bias inheritance and how the synthetic data influenced the models. We provided extensive manual data annotation of possibly biasing artifacts on the well-known ISIC dataset with skin lesions. In addition, we examined classification models trained on both real and synthetic data with counterfactual bias explanations. Our experiments showed that GANs inherited biases and sometimes even amplified them, leading to even stronger spurious correlations. Manual data annotation and synthetic images are publicly available for reproducible scientific research.},
  isbn = {978-3-031-16452-1},
  langid = {english},
  keywords = {Bias,Explainable AI,Generative adversarial networks,Skin lesion classification},
  file = {C:\Users\wenji\Zotero\storage\WV7IFSXX\Mikołajczyk et al_2022_The (de)biasing Effect of GAN-Based Augmentation Methods on Skin Lesion Images.pdf}
}

@article{lucasUnderstandingPosteriorCollapse2019a,
  title = {Understanding {{Posterior Collapse}} in {{Generative Latent Variable Models}}},
  author = {Lucas, James and Tucker, George and Grosse, Roger and Norouzi, Mohammad},
  date = {2019-04-19},
  url = {https://openreview.net/forum?id=r1xaVLUYuE},
  urldate = {2024-08-14},
  abstract = {Posterior collapse in Variational Autoencoders (VAEs) arises when the variational distribution closely matches the uninformative prior for a subset of latent variables. This paper presents a simple and intuitive explanation for posterior collapse through the analysis of linear VAEs and their direct correspondence with Probabilistic PCA (pPCA). We identify how local maxima can emerge from the marginal log-likelihood of pPCA, which yields similar local maxima for the evidence lower bound (ELBO). We show that training a linear VAE with variational inference recovers a uniquely identifiable global maximum corresponding to the principal component directions. We provide empirical evidence that the presence of local maxima causes posterior collapse in deep non-linear VAEs. Our findings help to explain a wide range of heuristic approaches in the literature that attempt to diminish the effect of the KL term in the ELBO to reduce posterior collapse.},
  langid = {english},
  file = {C:\Users\wenji\Zotero\storage\CIENAVDH\Lucas et al_2019_Understanding Posterior Collapse in Generative Latent Variable Models.pdf}
}

@inproceedings{lucasDonBlameELBO2019,
  title = {Don' t {{Blame}} the {{ELBO}}! {{A Linear VAE Perspective}} on {{Posterior Collapse}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Lucas, James and Tucker, George and Grosse, Roger B and Norouzi, Mohammad},
  date = {2019},
  volume = {32},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2019/hash/7e3315fe390974fcf25e44a9445bd821-Abstract.html},
  urldate = {2024-08-14},
  abstract = {Posterior collapse in Variational Autoencoders (VAEs) with uninformative priors arises when the variational posterior distribution closely matches the prior for a subset of latent variables. This paper presents a simple and intuitive explanation for posterior collapse through the analysis of linear VAEs and their direct correspondence with Probabilistic PCA (pPCA). We explain how posterior collapse may occur in pPCA due to local maxima in the log marginal likelihood. Unexpectedly, we prove that the ELBO objective for the linear VAE does not introduce additional spurious local maxima relative to log marginal likelihood. We show further that training a linear VAE with exact variational inference recovers a uniquely identifiable global maximum corresponding to the principal component directions. Empirically, we find that our linear analysis is predictive even for high-capacity, non-linear VAEs and helps explain the relationship between the observation noise, local maxima, and posterior collapse in deep Gaussian VAEs.},
  file = {C:\Users\wenji\Zotero\storage\I8UHQW76\Lucas et al_2019_Don' t Blame the ELBO.pdf}
}

@online{ReviewEnsembleLearning,
  title = {A Review of Ensemble Learning and Data Augmentation Models for Class Imbalanced Problems: {{Combination}}, Implementation and Evaluation - {{ScienceDirect}}},
  url = {https://www-sciencedirect-com.kuleuven.e-bronnen.be/science/article/pii/S0957417423032803},
  urldate = {2024-08-18},
  file = {C:\Users\wenji\Zotero\storage\BXZEPKAS\S0957417423032803.html}
}

@article{khanReviewEnsembleLearning2024,
  title = {A Review of Ensemble Learning and Data Augmentation Models for Class Imbalanced Problems: {{Combination}}, Implementation and Evaluation},
  shorttitle = {A Review of Ensemble Learning and Data Augmentation Models for Class Imbalanced Problems},
  author = {Khan, Azal Ahmad and Chaudhari, Omkar and Chandra, Rohitash},
  date = {2024-06-15},
  journaltitle = {Expert Systems with Applications},
  shortjournal = {Expert Systems with Applications},
  volume = {244},
  pages = {122778},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2023.122778},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417423032803},
  urldate = {2024-08-18},
  abstract = {Class imbalance (CI) in classification problems arises when the number of observations belonging to one class is lower than the other. Ensemble learning combines multiple models to obtain a robust model and has been prominently used with data augmentation methods to address class imbalance problems. In the last decade, a number of strategies have been added to enhance ensemble learning and data augmentation methods, along with new methods such as generative adversarial networks (GANs). A combination of these has been applied in many studies, and the evaluation of different combinations would enable a better understanding and guidance for different application domains. In this paper, we present a computational study to evaluate data augmentation and ensemble learning methods used to address prominent benchmark CI problems. We present a general framework that evaluates 9 data augmentation and 9 ensemble learning methods for CI problems. Our objective is to identify the most effective combination for improving classification performance on imbalanced datasets. The results indicate that combinations of data augmentation methods with ensemble learning can significantly improve classification performance on imbalanced datasets. We find that traditional data augmentation methods such as the synthetic minority oversampling technique (SMOTE) and random oversampling (ROS) are not only better in performance for selected CI problems, but also computationally less expensive than GANs. Our study is vital for the development of novel models for handling imbalanced datasets.},
  keywords = {Class imbalance,Data augmentation,Ensemble learning,Machine learning},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\D9RXJQCY\\Khan et al_2024_A review of ensemble learning and data augmentation models for class imbalanced.pdf;C\:\\Users\\wenji\\Zotero\\storage\\RTCQ46JV\\S0957417423032803.html}
}

@article{lecunGradientbasedLearningApplied1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  date = {1998-11},
  journaltitle = {Proceedings of the IEEE},
  volume = {86},
  number = {11},
  pages = {2278--2324},
  issn = {1558-2256},
  doi = {10.1109/5.726791},
  url = {https://ieeexplore.ieee.org/document/726791},
  urldate = {2024-08-18},
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
  eventtitle = {Proceedings of the {{IEEE}}},
  keywords = {Character recognition,Feature extraction,Hidden Markov models,Machine learning,Multi-layer neural network,Neural networks,Optical character recognition software,Optical computing,Pattern recognition,Principal component analysis},
  file = {C\:\\Users\\wenji\\Zotero\\storage\\MK54W9EW\\Lecun et al_1998_Gradient-based learning applied to document recognition.pdf;C\:\\Users\\wenji\\Zotero\\storage\\MP84BM6Q\\726791.html}
}
